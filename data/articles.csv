	link	article
0	http://www.vertuoconseil.com/portfolio/square-idea-1/	
1	http://www.vertuoconseil.com/portfolio/chute-de-la-bourse-chinoise-quels-enjeux-economiques-a-moyen-terme/	 Chute de la bourse chinoise : Quels enjeux économiques à moyen terme ? VERTUO 2017-01-12T11:42:09+00:00 Project Description Depuis le début 2016, les bourses chinoises inquiètent les acteurs économiques mondiaux car les pertes liées à ces indices sont assez impressionnants (-10% lors de la première semaine de 2016) ; les bourses atteignent même leur plus bas depuis Septembre 2015. Les autorités chinoises essayent de circoncire ce mouvement qui pourrait avoir des conséquences néfastes et provoquer une véritable guerre des monnaies à moyen terme. Illustration faite, le jeudi 07 janvier 2016, de la fermeture prématurée des différentes places chinoises après avoir chutées de 7,32% ; les autorités veulent à tout prix éviter un krach boursier qui pourrait être fort dommageable pour son économie. Ces nouveaux troubles venant de l’Empire du milieu a des répercutions au niveau mondial, avec l’affectation des bourses Européennes (notamment le CAC 40, les Indices boursiers Norvégiens, Suédois, Danois, Espagnol) et Asiatiques (indices Japonais et Hong Kongais). La Commission de régulation des marchés financiers (CSRC) en Chine a adoptée en Juillet 2015 des mesures ayant pour but de limiter les mouvements de capitaux et interdire aux détenteurs de plus de 5% des parts d’une entreprise cotée de céder leurs titres. Cette mesure devait prendre initialement fin le 08 janvier 2016 mais, avec les événements récents, elle a été reconduite pour une durée indéterminée. Elle prévoit également l’impossibilité de gros porteurs de se séparer de plus de 1% de l’entreprise tous les trois mois, avec le prérequis d’annoncer publiquement, quinze jours à l’avance, leur intention. Les indicateurs économiques chinois sont assez décevants, illustration autour du PPI (prix de vente en sortie d’usine) qui plonge sur le mois de décembre 2015 et ainsi continue la série de 46 mois consécutifs de baisse. En effet, la Chine doit consentir à des baisses de prix pour rester de plus en plus compétitif dans une situation de ralentissement des exportations. Les prévisions des économistes tablent sur une croissance dans les 5 prochaines années autour de 6.5% par an consécutivement à une demande mondiale freinée et un coût du travail à la hausse ; du jamais vu depuis 25 ans. Les investisseurs sont sceptiques sur la capacité des autorités de Pékin à soutenir leur économie malgré le relevé du cours pivot du yuan. Le yuan est au plus bas depuis 2011 afin de tenter de maintenir un certain niveau de compétitivité. Le responsable de recherche « devises des marchés émergents » chez HSBC illustre parfaitement cette situation de trouble où les différents signaux de politique de change ont pris à contre-pied les investisseurs mais reste prudent sur un retour au calme rapidement. Le président du Centre de recherche sur le développement du Conseil des Affaires d’Etat, Li Wei, indique que les années de croissance chinoise à deux chiffres sont maintenant résolues et vont laisser place à un PIB proche de 5%. Ce rythme pourrait même se dégrader avec le temps si la compétitivité chinoise venant à décroitre. Le milliardaire Georges Soros, financier américain reconnu pour ses activités de spéculation sur les devises et actions, pense que les problèmes d’ajustement de la monnaie chinoise pourraient nous conduire à la prochaine grande crise financière mondiale. Le prochain objectif chinois est de convaincre les investisseurs qu’elle a toute les cartes en main pour gérer cette situation, à l’image d’un ancien responsable des paiements internationaux au sein de l’administration chinoise des changes qui affirme que le yuan reste stable face à la plupart des autres devises hors dollar. La chine a même proposé récemment, en Décembre 2015, de mettre en place un indice du yuan mesurant l’évolution du taux de change face à un panel d’autres devises afin de détendre le lien entre le yuan et dollar. L’année 2016, réservera bien des surprises et les acteurs du monde de la bourse ont les yeux rivés sur les décisions économiques chinoises. Benjamin Fradet, Consultant Senior du cabinet VERTUO Conseil
2	http://www.vertuoconseil.com/portfolio/soiree-gatsby-magnifique/	
3	http://www.vertuoconseil.com/portfolio/defi-chinois-marche-europeen/	 Le défi chinois au marché européen Mathilde. Taillez 2017-02-12T12:41:50+00:00 Project Description Les Echos – 23 janvier 2017 : Quinze ans après son adhésion à l’OMC, la Chine apparaît comme la puissance commerciale-clé de la mondialisation. Admise à reculons dans le libre échange mondial, son statut particulier d’économie non marchande devait expirer une fois les réformes attendues réalisées. Entre temps, l’Union européenne (UE) a subi les crises économiques et la désindustrialisation, laissant la Chine développer une puissance commerciale qui la place aujourd’hui en position de force. Or, sa conformité aux engagements de l’OMC laisse à désirer et défavorise les entreprises européennes. L’UE est au pied du mur. Une approche commune impossible La mise en oeuvre de moyens de défense commerciaux pour l’UE est discutée dans les institutions européennes depuis plusieurs années : l’échéance du changement de statut chinois est un risque pour de nombreux secteurs de l’économie européenne. Le Conseil européen du 21 octobre 2016 a rappelé dans ses conclusions la nécessité de trouver un consensus entre les États membres sur ce sujet. C’est là toute la question, car le blocage européen est patent. Doter l’UE d’instruments de lutte antidumping plus efficaces n’est pas nécessaire pour toutes les capitales du continent. Certains pays du nord, dont le Royaume-Uni, assument une tradition libre-échangiste et se méfient de signaux négatifs, qui pourraient conduire à des rétorsions commerciales. La coutume du « droit moindre » cristallise les oppositions entre Européens. Cette règle consiste à instaurer un droit qui corrige le prix du produit importé, comme si les conditions de concurrence étaient loyales. La menace est donc très faible. Cette approche accommodante des Européens ne produit aucun effet dissuasif : la priorité de l’UE c’est la continuité commerciale avec Pékin. Vers une approche pro active de la Commission européenne ? Jusqu’à présent, l’UE pouvait arguer de l’interventionnisme étatique pour justifier de barrières douanières face aux produits chinois. Le prix, faussé par l’État, offrait donc la justification nécessaire à une mesure protectionniste. Cette approche, qui fait l’actualité de la sidérurgie européenne, va devoir évoluer. Mais, faute de consensus entre ses membres, l’UE ne pourra pas développer de législation plus protectrice. L’un des axes de réformes possibles, tel que porté par la Commission européenne, serait d’inverser la charge de la preuve en matière de dumping : l’UE devrait alors prouver le dumping et non plus Pékin. Cette nouvelle approche va pousser la Commission européenne à de profondes études sur les marchés et les entreprises avec pour objectif de documenter les anomalies commerciales pour peser sur la Chine. Cette méthode de lutte antidumping reste suspendue à l’accord des institutions européennes et devra faire ses preuves en pratique. Néanmoins, il s’agit d’une approche constructive, qui pallie la division des États membres. Cela ne suffira pas à régler le clivage idéologique sous-jacent : la relation au libre échange est différente selon les États membres. Expansionnisme chinois en Europe En parallèle des questions douanières, les entreprises chinoises investissent massivement en Europe. Deux problèmes en découlent : la nature des actifs cédés et la réciprocité. Ainsi, l’Allemagne a vu Kuka, un spécialiste de la robotique industrielle, passer sous pavillon chinois. Et les actionnaires allemands semblent séduits par les capitaux chinois : Sanan Optoelectronics serait en discussion pour racheter le Bavarois Osram. Berlin a toutefois posé une limite cet été, avec la tentative de rachat d’Aixtron par Fujian Grand Chip Investments. Au motif d’intérêts stratégiques liés à la défense, la chancellerie a finalement bloqué l’opération pour garder l’entreprise de semi-conducteurs sous capitaux européens. L’appétit chinois pour les firmes européennes de haute technologie tend à s’affirmer, ce qui pose une question de sécurité économique. Le départ de centres de décisions et de production de haut niveau pourrait coûter cher aux économies européennes. La France n’est pas en reste, après la prise de contrôle du Club Med, Fosun visait cet été le capital de la Compagnie des Alpes, suscitant l’intervention du ministre de l’Économie pour garantir l’engagement de la Caisse des Dépôts au capital de l’opérateur de remontées mécaniques et de parcs d’attractions. La réciprocité avec l’économie chinoise laisse également à désirer. Régulièrement dénoncée par l’UE, l’intervention de l’État chinois dans son économie est loin des standards de l’OMC et alimente une concurrence déloyale, dont pâtissent les entreprises européennes sur le marché chinois. Le libre échange en est donc faussé. À cela s’ajoutent une protection des droits de propriété intellectuels particulièrement faible, une position dominante des entreprises publiques et un accès limité aux marchés publics chinois. Le chemin est encore long pour atteindre la réciprocité dans les échanges sino-européens. L’idéal libre-échangiste d’économies réciproquement ouvertes s’apparente d’abord à une utopie : la Chine a tout intérêt à préserver son marché intérieur et s’approprier les technologies qui lui manquent pour asseoir sa puissance économique. L’UE devrait s’en inspirer : ce préalable permettrait d’envisager une relation commerciale plus équilibrée. Réguler pour ne pas subir : c’est l’unique choix pour les Européens. Mais ici, tout est dans la proportion et en la matière, l’UE est contrainte au moins-disant. Le droit européen ne protège pas suffisamment le marché commun et Pékin en profite. Plus que jamais d’actualité, la gestion du libre échange sino-européen exige une posture ferme de la part des vingt-huit. Par Pierre Theobald, Consultant Senior chez Vertuo Conseil
4	http://www.vertuoconseil.com/portfolio/vertuo-une-des-40-societes-a-suivre-en-2016-selon-le-magasine-challenges/	
5	http://www.vertuoconseil.com/portfolio/les-investissements-socialement-responsables-reflet-dune-prise-de-conscience-durable-ou-simple-artifice-marketing/	 Les Investissements Socialement Responsables : reflet d’une prise de conscience durable ou simple artifice marketing ? VERTUO 2017-02-12T13:13:54+00:00 Project Description Les Echos – 27 mai 2016 : Les évènements historiques et géopolitiques, catalyseurs des ISR… Historiquement, les Investissements Socialement Responsables trouvent leur origine dès le XIIIe siècle aux États-Unis au travers d’un mouvement religieux protestant (les Quakers) qui interdit à ses membres d’investir et de tirer profit dans des opérations liées à l’industrie de l’armement et à l’esclavage. Historiquement, les Investissements Socialement Responsables trouvent leur origine dès le XIIIe siècle aux États-Unis au travers d’un mouvement religieux protestant (les Quakers) qui interdit à ses membres d’investir et de tirer profit dans des opérations liées à l’industrie de l’armement et à l’esclavage. Plus tard, dans les années 20, d’autres congrégations religieuses, notamment les anabaptistes, refusent toute forme d’investissement dans ce qu’ils appellent les sin stocks (les actions du péché) : alcool, jeux, tabac. L’investissement socialement responsable n’est que très embryonnaire et progresse alors très lentement jusqu’à la fin des années 60. La guerre du Vietnam et la prise de conscience politique du régime de l’Apartheid en Afrique du Sud sont des déclencheurs qui accélèreront le développement des ISR dans une dimension plus laïque et militante : en effet, de nombreuses firmes américaines génèrent de confortables revenus et défendent leurs intérêts dans ces contextes où l’éthique n’a qu’une place secondaire. General Motors dont l’actionnariat compte alors des universités, des églises et des fonds de retraite défend ouvertement le régime de Prétoria et se retrouve sous les feux des projecteurs (déjà un premier exemple dans l’industrie automobile quant au respect des valeurs fondatrices des ISR…). Dans les années 80, des réflexions nouvelles sur la valorisation des entreprises introduisent les notions de pratiques environnementales, sociales, ou encore de gouvernance d’entreprises (les critères ESG) qui auraient un impact positif sur les résultats des entreprises à long terme ; des agences de notation « extra financières » apparaissent ainsi que les premières sociétés de gestion exclusivement orientées sur l’investissement socialement responsable. Les Investissements Socialement Responsables, aujourd’hui… Selon le Forum pour l’Investissement Responsable (FIR), créé en 2001 pour le promouvoir, l’investissement socialement responsable est « un placement qui vise à concilier performance économique et impacts social et environnemental en finançant les entreprises et les entités publiques qui contribuent au développement durable, quel que soit leur secteur d’activité. En influençant la gouvernance et le comportement des acteurs, l’ISR favorise une économie responsable« . Est donc qualifié d’ISR l’ensemble des outils d’épargne, produits d’investissements ou instruments financiers qui vise à appliquer de manière concrète dans la sphère financière les notions de développement durable, de respect de l’environnement et de l’être humain, et de bonne gouvernance d’entreprise selon les critères ESG. Pour évaluer « l’ISRisation » d’une entreprise, plusieurs méthodes non standardisées existent : – La méthode dite « exclusive » qui vise à exclure les entreprises ne respectant pas certaines normes, conventions, pratiques, ou convictions. – La méthode « inclusive » par laquelle les investissements se concentrent sur des secteurs d’activité précis (traitement des déchets, énergie renouvelable, etc.) ou des problématiques ponctuelles (déforestation, pollution, etc.) – La méthode « proactive » qui implique de la part de l’investisseur une démarche pour réorienter les pratiques d’une entreprise ou une obligation de partage des bénéfices auprès d’organisations caritatives. – Last but not least, et nous devinons aisément pourquoi, la méthode « douce » qui intègre quelques critères ISR à minima dans une démarche d’investissement traditionnel. Un marché en pleine expansion… Des facteurs clés marquent dans les années 2000 l’expansion des ISR. L’apparition d’institutions propres aux marchés des ISR (en France : ORSE, FIR, Eurosif, label Novethic, agence de notations extra-financière) associées aux concepts et aux politiques de RSE, de finance responsable, ou encore plus globalement de développement durable permettent d’envisager l’essor et une meilleure régulation de ce marché. Dans le monde, le vieillissement de la population et la succession de crises financières spéculatives (crise japonaise, bulle internet, subprimes, dette souveraine) encouragent les fonds de pension et les investisseurs à définir de nouvelles stratégies d’investissement et sur des horizons de plus long terme. En 2011, alors que les encours mondiaux des ISR s’élèvent à plus de15 000 milliards de dollars, l’Europe s’octroie 53 % du marché devant les États-Unis avec 39 % : si les montants d’investissements sont encore relativement peu importants (entre 0,5 % et 2 % des marchés actions selon les pays), les ISR suscitent un intérêt grandissant de la part de la sphère financière. Mais un marché immature… Mais le marché des ISR est à ce jour immature et encore trop artisanal : un large éventail d’offres, diverses méthodes de valorisation et une multitude de sensibilités se confondent et empêchent la lisibilité et la clarté pourtant indispensables et que tout investisseur est en droit d’attendre d’un produit financier. Selon le périmètre retenu par les différentes études et analyses existantes, les encours ISR européens représentent de 3 % à 18 % de l’industrie européenne de la gestion d’actifs. Pourquoi un tel écart ? La réponse tient au fait que ce marché est aujourd’hui non régulé : si nous évaluons les ISR au sens le plus strict tel que nous l’avons défini plus haut, nous atterrissons dans le bas de la fourchette (3 %) alors que si nous tenons compte des investissements « traditionnels » intégrant quelques critères ISR, nous nous retrouvons dans le haut de la fourchette (18 %). C’est en ce sens, et ces éléments chiffrés en sont la preuve, que l’essence des d’ISR a été détournée pour être marketée et répondre aux besoins inconscients ou conscients d’une population marquée d’une part par le développement durable et toujours concernée, voire concentrée d’autre part par l’appât du gain. L’exemple le plus marquant est à mettre à l’actif du secteur automobile qui décidément fait toujours parler de lui : en septembre 2015, l’indice DJSI, crée en 1999 et qui récompense les entreprises les plus performantes en matière de critères économiques, sociaux et environnementaux, révélait son palmarès 2014. Dans la catégorie « automobile », le gagnant était… Volkswagen ! ce que l’indice n’a pas manqué de corriger le 6 octobre 2015 dès les premières révélations de l’affaire des logiciels truqueurs, mais au fond le mal était fait : quelle crédibilité pouvons-nous accorder à ces institutions ? Autre illustration : en 2010, certaines agences de notation extra-financières, dont le rôle est d’évaluer les politiques environnementales, sociales et de gouvernance des entreprises (nos fameux critères ESG), notaient au sein de la British Petroleum la volonté de réduire son impact environnemental et l’existence d’une démarche pro active en matière de sécurité. Quelques mois plus tard, la plateforme pétrolière Deepwater Horizon explosait au large des côtes américaines générant un désastre écologique sans précédent… Nous avons là une partie de notre réponse : pour certains les ISR sont bien un artifice marketing qui n’a d’autres ambitions que de surfer sur la vague du développement durable au détriment de son esprit originel. Mais, au-delà de ce constat, la démarche des investissements socialement responsables doit exister et continuer de se répandre durablement afin que nous acceptions cette idée que l’on puisse gagner de l’argent sans détruire le tissu social et notre environnement. Pour autant les ISR sont-ils performants et rentables ? Un rapport public de la Direction Générale du Trésor révélait que « d’un point de vue théorique, la question n’est pas tranchée, mais les études empiriques permettent de conclure à une absence de différence significative entre ISR et investissement classique ». Nombreuses sont les études comme celle publiée en 2013 par Morningstar qui attestent de la performance des fonds ISR et talonnent même celle des fonds traditionnels. Toujours est-il que la définition même d’un ISR n’étant pas standardisée au niveau mondial, il est délicat, voire impossible, de comparer objectivement investissement classique et ISR et d’en tirer des conclusions dans un sens ou dans l’autre. Une labellisation du marché pour une meilleure régulation… Le 28 septembre 2015, Michel Sapin officialisait lors de la « Semaine de l’ISR » la création d’un label public ISR permettant aux investisseurs d’identifier simplement les fonds pratiquant l’investissement responsable. Notre ministre des Finances précisait d’ailleurs qu’il ne s’agirait pas « de donner une bonne image à un placement », mais bien de défendre « un résultat concret et mesurable pour orienter les investissements publics et privés vers l’ISR« . Attendons donc pour voir : si le principe est louable et nécessaire, son cahier des charges et les éléments pertinents le recouvrant en feront un atout, un frein ou… un nouvel artifice. Le monde l’ISR n’en est qu’à ses débuts et ne représente qu’une part très limitée des investissements mondiaux. Il doit se structurer dans toutes ses composantes tout en conservant son originalité et son essence même pour éviter qu’ils ne soient utilisés à d’autres fins que celles pour lesquelles ils ont été conçus. Pour accélérer son essor et en écho au récent Accord de Paris suite à la COP 21 nos politiques peuvent accompagner son développement : les fondations des ISR doivent dans un premier temps être consolidées pour rendre ce marché régulé et lisible (s’il est bien conçu le label public servira au marché français), mais le levier le plus significatif restera fiscal. Si, dans un second temps, nos gouvernants français, européens, voire internationaux, adoptent une fiscalité avantageuse à l’égard des ISR, son essor n’en sera que plus marquant et plus probant dans la prise de conscience responsable et salutaire pour notre monde d’un nécessaire nouveau modèle de développement économique et social. Par Patrick Lebihan, senior Manager du cabinet VERTUO Conseil
6	http://www.vertuoconseil.com/portfolio/bientot-bale-iv/	
7	http://www.vertuoconseil.com/portfolio/le-chief-data-officer-va-t-il-prendre-le-pouvoir-dans-les-banques/	 Le Chief Data Officer va-t-il prendre le pouvoir dans les banques ? VERTUO 2017-02-12T12:53:59+00:00 Project Description Les Echos – 13 octobre 2016 : Les enjeux et investissements autour de la gouvernance et de la gestion des données au sein des établissements bancaires prennent aujourd’hui une importance croissance. De l’exploitation de leur qualité dépend l’accroissement de l’efficacité opérationnelle. Le régulateur ne s’y est pas trompé en multipliant les textes encadrant leur utilisation. Le volume exponentiel des données est un vrai défi auquel les banques doivent répondre, mais comment ? Comment doivent-elles se transformer et s’adapter afin d’en optimiser le traitement ? Des contraintes réglementaires structurantes autour de la gestion des données… Depuis la crise financière de 2008, les établissements bancaires doivent faire face à une véritable inflation réglementaire avec des besoins accrus en termes de reportings, mais aussi de fiabilité des informations présentées. Avec un volume croissant de données disponibles, les réglementations sur leur gestion et leur traçabilité fleurissent : « FATCA », « AEOI », et bientôt « GDPR » sont des acronymes qui régissent le quotidien des banques. Aujourd’hui, c’est la réglementation BCBS 239 qui est à l’honneur. Le comité de Bâle a publié en janvier 2013 un ensemble de principes visant essentiellement à renforcer la qualité et la capacité de production des reportings réglementaires. Entrée en vigueur depuis le 1er janvier 2016. Inutile d’être exhaustif, on assiste bel et bien à la multiplication de textes contraignants synonyme de surcoûts. Mais dans le domaine de la donnée, la réglementation peut également devenir une opportunité de gains. Ainsi, l’exigence de mettre en place une solide gouvernance autour de la gestion des données se transforme en levier pour créer de la valeur et des revenus supplémentaires à travers l’exploitation de ces informations. … à capitaliser en opportunités pour accroître efficacité opérationnelle des banques Les établissements financiers doivent donc considérer leurs investissements dans la gouvernance des données comme un moyen et une chance de pérenniser leurs activités en repensant leurs processus et leurs systèmes d’informations. Pour une banque, la qualité des données est essentielle, non seulement en externe pour son image et la satisfaction de ses clients qui attendent précision et exactitude, mais aussi en interne pour le pilotage financier et stratégique de la banque. La donnée devient un actif stratégique et un atout concurrentiel. Pourquoi un atout concurrentiel ? Prenons un exemple simple : à travers une gestion efficiente et en temps réel des données de son client, la banque va acquérir une meilleure connaissance des membres de sa famille, de ses comportements d’achat, de son évolution de vie et va ainsi pouvoir anticiper ses besoins et lui proposer le bon produit, au bon moment et par le bon canal : une offre pour la rentrée scolaire de ses enfants par SMS, un livret d’épargne pour le nouveau-né par notification Facebook… autrement dit une stratégie marketing plus ciblée et donc plus efficace. La banque créée de la valeur via l’analyse et la gestion pertinentes de ses données. Des compétences nouvelles dans le data management, historiquement non disponible dans la banque, deviennent ainsi indispensables. Une fonction clé pour faire face à ces nouveaux enjeux : le Chief Data Officer Si la fonction de CDO (avec un D comme Data) n’est pas nouvelle, ce n’est que récemment qu’elle a commencé à gagner du terrain. Son ascension ne fait que débuter, elle devrait continuer à progresser de façon exponentielle dans les prochaines années, non seulement dans les établissements bancaires, mais aussi gagner l’ensemble du secteur économique. Si le rôle premier d’un CDO est de gérer les données, les contours et le positionnement de la fonction restent encore flous à ce jour et il existe des différences significatives suivant les exigences des entreprises. Rôle tenu par la DSI hier, le CDO d’aujourd’hui est surtout concentré sur le management des données d’un point de vue strictement réglementaire : définition des principes de gouvernance des données, contrôle de leur implémentation, mise en place d’outils de contrôle de qualité des données. Mais quid du CDO de demain ? Il devra disposer d’une vision data transverse à l’entreprise, devra favoriser l’émergence d’une culture « data-driven », et ainsi accompagner à la définition de la stratégie et des priorités de développement business d’une entreprise. À ce titre, il devra être à la pointe du digital, mais sans perdre de vue bien entendu les principes liés à l’éthique et à la réglementation en vigueur autour de l’utilisation des données. Dans cet objectif, il sera nécessairement rattaché à la Direction Générale de l’entreprise, avec sa propre structure et des relais dans les entités métiers de la banque. En synthèse, en plus d’un bon relationnel pour interagir avec l’ensemble des fonctions de l’entreprise, le CDO doit avoir des compétences à la fois métiers et techniques : il doit maîtriser l’art de la gestion des données, mais aussi les supports digitaux de type Big data pour exploiter ces données. Finalement, l’ascension du CDO ne fait que commencer, mais engager des CDO ou autres spécialistes de la data n’est pas suffisant et ne suffira pas à enclencher une stratégie data-driven pertinente. C’est maintenant aux cadres dirigeants que revient la mission d’insuffler le changement dans l’entreprise. Ils doivent être au fait des opportunités que l’exploitation des données offre pour mettre en place des projets et les suivre sur le long terme. Par Maïa Grangier, Project Manager du cabinet VERTUO Conseil
8	http://www.vertuoconseil.com/portfolio/gestion-de-la-liquidite-et-si-la-prochaine-crise-bancaire-etait-reglementaire/	 Gestion de la liquidité : et si la prochaine crise bancaire était règlementaire ? VERTUO 2017-01-24T19:49:19+00:00 Project Description Après les turbulences observées au cours de la crise des subprimes en 2007 et la crise souveraine de 2011, le monde bancaire traverse aujourd’hui un nouveau type de crise : une réglementation stricte, un environnement économique et social sous tension, un business model en péril… Les banques tentent, tant bien que mal, de suivre la cadence imposée tout en essayant de transformer leur activité : un pari difficile, exigeant, et le plus souvent sans récompense à l’arrivée car signifiant une perte de rentabilité. Et si l’on en croit l’ACPR et le comité de Bâle, la mise en place des normes réglementaires risque encore de s’accélérer dans les prochaines années. Et si nous étions tout simplement au bord d’une crise « réglementaire » ? Une exigence réglementaire qui met déjà à mal les établissements financiers sur le plan organisationnel. Si l’ensemble du secteur s’accorde à dire qu’il est nécessaire d’encadrer la gestion de la liquidité, la mise en œuvre opérationnelle des ratios de liquidité LCR et NSFR, elle, est loin d’être aisée. En effet, plusieurs obstacles s’offrent aux établissements : La quantité de demandes réglementaires : entre le nombre d’actes délégués publiés par l’EBA, les demandes de l’ACPR, et celles du comité de Bâle, les banques ne savent plus où donner de la tête ! La granularité de l’information : un exemple édifiant est l’audit AQR mené en 2014, qui a nécessité dans de nombreux établissements, la mise en place d’équipes entières afin de renforcer la force de travail et être en mesure de restituer l’imposant volume de données demandé par le régulateur à un niveau très fin. La fréquence des reportings, avec notamment un ratio, initialement en J+30, devant être produit aujourd’hui à J+8, et très bientôt à la demande ou de façon hebdomadaire. Le périmètre d’application difficile à identifier : les banques ont du mal à déterminer ce qui rentre ou non dans le cadre du reporting réglementaire. La gestion « intragroupe » ou inter-contrat, en est un exemple : elle consiste à identifier au sein de la structure bancaire les opérations qui sont effectuées entre deux entités internes, et ce, afin qu’elle ne soit pas comptabilisées dans le bilan. Or, sur un groupe aussi significatif que BNP Paribas, certaines entités font partie du bilan de la banque sur un mois et disparaissent le mois suivant, rachetées ou fusionnées à d’autres entités. Le ratio LCR est également demandé sur des sous-niveaux de consolidation significatif afin d’éviter la formation de poche de liquidité : cela complique d’autant plus la tâche de découpage des entités. Dans ce contexte de tension et de reporting permanent, l’ensemble de la chaîne opérationnelle de la banque est constamment mobilisée : comptabilité, ALM, direction des risques, équipes de reporting, et IT. La gestion de la production est privilégiée à la demande projet, par peur de fournir de « mauvais chiffres » au régulateur ; ainsi, très peu de projets d’investissements peuvent voir le jour, limitant les réformes structurelles nécessaires aux banques pour s’adapter. Par ailleurs, pour fournir des chiffres acceptables dans les délais demandés et avec des moyens limités, les banques se voient contraintes de jouer sur l’interprétation de la norme actuelle. Et cette interprétation, notamment sur le ratio LCR donne lieu à des situations pour le moins déconcertantes ! Un ratio de liquidité à court terme, rempli d’incohérences et de contradictions vis à vis des autres réglementations Le LCR se propose d’observer la liquidité d’une banque sur une période de 30 jours dans un contexte de tension systémique (contagion du marché) et idiosyncratique (facteurs propres à la banque). Son principe est le suivant : les réserves de liquidité doivent être supérieures aux fuites de liquidité générées par la perte des possibilités de refinancement sur le marché, et par une série d’autres facteurs qui peuvent advenir lors d’une crise de liquidité (tirages de lignes hors-bilan, fuite de liquidité liée aux collatéraux…). Par cette définition même, on observe qu’en plaçant en réserve de liquidité le montant d’un emprunt à plus d’un mois, le ratio LCR s’en trouve artificiellement gonflé. Cet artifice est limité par le ratio de levier mais illustre bien le paradoxe du LCR : plus on emprunte, plus on améliore son ratio de liquidité ….et plus on dégrade sa rentabilité ! L’interprétation de la régulation locale vis-à-vis de la régulation européenne fait également l’objet de dérives : certains actifs détenus par une banque ne sont pas éligibles à la BCE mais le sont pour une entité étrangère détenue par la banque ou un autre établissement bancaire. Rien n’empêche alors les établissements d’utiliser massivement des swaps afin d’échanger entre elles ces actifs et gonfler artificiellement leur réserve. Elément majeur dans la constitution du ratio LCR, la classification du type de clientèle soulève également certaines incohérences : d’une situation mensuelle à une autre, un client peut être classé PME ou Corporate s’il dépasse un seuil de passif de 1 M€, et donc pondéré différemment alors que son profil de risque n’a pas évolué. Des variations anormales sur le ratio peuvent donc se produire subitement d’un mois sur l’autre, sans que cela ne soit contraire à la règlementation, mais rendant délicat le travail d’analyse et de justification de l’écart. D’ailleurs, l’utilisation du chiffre d’affaire comme indicateur de classification permettant d’estimer la santé financière d’une entreprise est-il le plus approprié? Le bénéfice, couplé au profil de risque, serait un indicateur beaucoup plus fiable : de nombreuses entreprises peuvent dégager un chiffre d’affaire conséquent sans faire de bénéfices pour autant alors que certaines PME de taille plus restreinte dégagent un bénéfice important. Concernant les titres, leur classification dépend de la notation externe de l’émetteur, redonnant un pouvoir important aux agences de notations, critiquées pour leur rôle dans la crise de 2008 et également remis en cause dans la dernière consultation du comité de Bâle sur la révision de la méthode standard. Des effets indésirables, à court-terme, sur le financement de l’économie… Premier effet pervers de l’application du LCR, les banques n’ont plus d’intérêt à commercialiser des assurances vie ou des OPCVM puisque ces opérations ne rentrent pas dans leur bilan et de ce fait, dans la réserve de liquidité. Or, ces produits servent grandement le financement de l’économie, en proposant des taux bien plus intéressants que les livrets d’épargnes classiques, comme le LDD ou le Livret A. On risque alors de voir ces produits disparaitre des offres proposées aux particuliers, surtout en période de quantitative easing (facilitation de la banque centrale visant à racheter des bons du trésor aux états pour la bonne circulation de l’argent dans l’économie). Le constat n’est pas plus rassurant vis-à-vis des entreprises : au final, peu d’actifs émis par celles-ci sont considérés comme éligibles à la réserve de liquidité et classés de surcroit en actif de qualité moyenne. Or, les règles de solvabilité de Bâle III vont réduire la capacité de financement des banques. Si dans le même temps, le ratio de liquidité limite aussi leur capacité à investir en obligations corporate, les entreprises vont avoir de plus en plus de mal à trouver des sources de financement pour soutenir leur développement ! La définition même d’actif liquide fournie par l’EBA reste d’ailleurs très paradoxale : les éléments rentrant dans la définition des actifs de « haute qualité » concernent des actifs mobilisés pour la banque centrale, des obligations d’état, d’organismes bancaires ou assimilés. Si l’effet recherché est bien de limiter la part des particuliers et des entreprises, cela lie d’autant plus le sort des banques à celui des états ou d’autres banques, entretenant le phénomène de contagion systémique que l’on cherche à éviter, et qui fut particulièrement présent lors de la crise de liquidité souveraine de 2011. …mais également à plus long terme. Le métier même de la finance se trouve transformé par le business model imposé par le superviseur et le régulateur, obligeant un nécessaire allongement du passif de la banque afin de répondre aux exigences imposées par le LCR. Cet allongement du passif s‘observe par le biais de l’augmentation massive des programmes d’émission, profitant des conditions excellentes offertes par le marché en ce moment, mais qui ne dureront pas éternellement. Cette tendance peut s’avérer dangereuse à la longue : les besoins en liquidité long terme sont tels qu’il est nécessaire de recourir à des solutions de financements alternatives, renforçant les opérations de titrisation que parallèlement les régulateurs souhaitent éviter pour retomber dans une nouvelle crise ! Car, rappelons-le, la titrisation a été l’un des acteurs principaux de la crise des sub-primes en 2007, participant au phénomène de contagion en transformant certains crédits immobiliers américains en actifs toxiques pour les banques. Pour ce qui est de l’immobilier, le marché de l’habitat en France est également à surveiller. Une crise financière est très souvent suivie d’une crise de l’immobilier et la France en présente actuellement tous les prémisses : coût de la vie élevé, décalage entre le prix de l’immobilier et le revenu des ménages, allongement de la durée des prêts et baisse des taux… Au-delà du préjudice important pour la population, cela ne serait évidemment pas une bonne nouvelle pour les banques françaises : la part des créances clientèles, constituées en grande partie par les crédits immobiliers, est très importante dans leur bilan (25% de l’actif BNP Paribas, 30 % chez la Société Générale, 23% pour Crédit Agricole SA en 2014). A ce titre, les représentants des banques françaises, allemandes, belges et japonaises ont adressé en 2014 un courrier au Comité de Bâle pour défendre le crédit immobilier à taux fixe car le Comité envisage de modifier la manière dont les banques traitent le «?risque de taux?», c’est-à-dire l’impact que peut avoir sur le bilan d’une banque une hausse ou une baisse importante des taux d’intérêts. Les crédits immobiliers sont actuellement classés en «?banking book?», une partie du bilan contenant des actifs destinés à être conservés par la banque. Ils sont, de ce fait, une composante utilisable dans le cadre du ratio LCR. Leur passage en «?trading book?», comme le prévoit le comité de Bâle, remettrait fortement en cause leur utilisation comme source de résilience face aux tensions de liquidité, comme cela a été le cas lors de la crise des sub-primes. En définitive, trois possibilités pour les prêteurs : des réserves accrues face aux crédits octroyés (donc des taux moins intéressants pour l’emprunteur), accentuer la titrisation (encore), ou relancer le crédit immobilier à taux variable (une route dangereuse vu le niveau très faible des taux actuels). La dernière hypothèse est d’ailleurs peu probable : cela serait une remise en cause complète du système de financement français par l’immobilier à taux fixe. Nous voilà bien avancés : quelle que soit la solution choisie, elle s’avère pénalisante pour le financement de l’économie ! A l’image du phénomène lié aux assurances vies non-comptabilisées dans le bilan bancaire, ce comportement traduit une volonté de privilégier les ressources de marché vis-à-vis des ressources clientèles. D’un point de vue gestion des risques, recourir aux ressources de marché permet de diversifier les sources de financement car la part des dettes auprès de la clientèle reste gigantesque dans le bilan bancaire français (près de 30% du passif, un chiffre resté stable depuis 2011). Il en est autrement d’un point de vue « social » : cela n’encouragera pas les banques à proposer à ses clients des taux intéressants pour emprunter, bien au contraire. Autre fait marquant et peu abordé, le vieillissement démographique risque d’entraîner une contraction de l’épargne à long-terme qui peut s’avérer destructrice sur des ratios basant une grande partie de leur liquidité sur les « petits épargnants ». En effet, le profil de l’épargnant « vieillissant » actuel se concentre principalement sur des placements sans risque comme le livret A et le LDD, dont la part dans le bilan cherche à être réduite, ou des assurances vies qui ne sont pas comptabilisées actuellement. Dans le contexte démographique actuel, cet élément ne doit pas être négligé, car il sera très difficile d’orienter cette clientèle vers d’autres placements. Peut-on finalement miser sur la constitution d’un ratio LCR par les crédits immobiliers, le marché de la titrisation, et l’épargne règlementée ? C’est assurément la direction qui est prise actuellement. Un ratio de liquidité à contre-emploi ? En principe destiné à améliorer la solidité des banques, le ratio de liquidité pourrait donc aboutir au résultat inverse ! En pénalisant le financement de l’économie par les particuliers (assurance vie, crédit immobilier à taux fixe), celui des entreprises (la limitation des obligations corporates), et des banques (allongement du passif, réserves de liquidité toujours plus importantes, transformation trop rapide du business model), le ratio de liquidité devient l’antithèse parfaite du fonctionnement qu’il devrait avoir : un garant du financement économique. L’arrivée du NSFR risque également d’accentuer ce phénomène de contraction et d’entraîner le monde bancaire dans une nouvelle spirale conduisant à une inévitable conclusion : la prochaine crise pourrait bien être réglementaire. Aujourd’hui, les banques se trouvent face à un vrai problème de sur-réglementation et il s’agit de trouver un vrai équilibre entre la rentabilité et la règlementation. Car après tout, si les banques se limitent à la publication de ratios, si le client particulier n’est plus qu’un acteur marginal du rouage bancaire, si les entreprises ne disposent d’aucune capacité d’investissement et si les Etats sont déjà sur-endettés, qui va financer l’économie mondiale ? Julien Delrieu, Consultant Senior du cabinet VERTUO Conseil
9	http://www.vertuoconseil.com/portfolio/xerfi-canal-tv-3-grands-defis-groupes-de-protection-sociale/	
10	http://www.vertuoconseil.com/portfolio/election-de-donald-trump-brexit-vers-deglobalisation/	 Élection de Donald Trump, Brexit… vers une déglobalisation ? Mathilde. Taillez 2017-02-12T12:46:56+00:00 Project Description Les Echos – 16 décembre 2016 : 58 millions électeurs US ont voté pour Donald Trump… 17 millions d’électeurs anglais ont voté le Brexit. 2 décisions motivées par la crainte d’une ingérence extérieure invasive et stimulées par des discours populistes à la limite du rationnel. Néanmoins, le peuple a voté. Il a voté contre les préconisations des élites et des médias. Cet américain moyen que le monde entier méprise depuis quelques heures ou quelques mois. Mais pourquoi cette défiance voire cette animosité ? N’est-ce pas la moindre des choses qu’un peuple puisse disposer de sa destinée et à ce titre choisir son leader ? Oui, le Royaume-Uni est sorti de la zone euro et oui, Donald Trump et son cortège de mesures populistes débarquent à la Maison-Blanche, mais le monde ne s’est pas écroulé. Au final quels sont les motifs de ces craintes ? Oui, les politiques migratoires américaine et anglaise vont se durcir. Mais le mur au sud des États-Unis et la gestion de la jungle de Calais sont des phénomènes très localisés, et bien qu’en terme humanitaire les enjeux soient réels, ils ont un impact très « local » au final. Et oui ils vont renforcer leur protectionnisme économique… Et enfin, on décèle un malaise ! En fait, que vont devenir les conditions d’investissement dans ces pays ? Les capitaux européens resteront-ils au Royaume-Uni, même s’ils ne font plus partie de l’UE ? ? A priori, la stabilité connue jusqu’à lors vacille. Cette activité va-t-elle s’envoler vers d’autres cieux ou rester là où l’on maîtrise leur fonctionnement ? Si tant est que la fiscalité et les compétences soient toujours attractives. Ces interrogations irritantes sont la pire crainte des investisseurs et acteurs politiques. L’instabilité met à mal les stratégies et arbitrages établis de longue date. Pire, c’est la tendance sous-jacente à ces 2 résultats. Un phénomène de repli des états et des économies. En bref, une amorce de dé-globalisation. Depuis des années, elle nous est présentée comme inéluctable et implacable. Il ne sert à rien de s’opposer à l’intégration des économies mondiales à ce schéma globalisé. Forcément, il ne sert à rien d’aller contre le progrès ou le sens de l’histoire. Tout contestataire passerait pour un hérétique des temps modernes, c’est logique. Ce conditionnement fait passer les pro-Trump ou les pro-Brexit pour des hérétiques, qui foulent au pied cette croyance. Qui sont les gourous de cette nouvelle religion ? Un oligopole politique et de magnats de la finance qui entretiennent leurs marges financières et statut social ? En tout cas, c’est ce que semblent croire les populations britannique et américaine, qui l’ont exprimé à travers leurs votes. Ces populations choisissent de rompre avec ces administrations et l’immobilisme proposé par ses élites. Tout le monde voit que M. Trump est anticonformiste (pour ne pas dire un OVNI), que le Royaume uni hors de l’Union européenne se met en danger. Mais le ralentissement des économies les plus « fortes » au profit de quelques-uns et de l’émergence d’autres pays n’est pas satisfaisant pour des populations qui souffrent. Ces « souffrants » comme touchés par une infection préfèrent prendre le risque d’être amputés d’une jambe et de guérir que de mourir à petit feu. La perspective est similaire. Mieux vaut une décision radicale qui rompt avec tous les modèles existants qu’un statu quo avec des mesures faibles qui n’auront qu’un effet placébo. Cette tendance qui se dessine est un message fort pour ces élites qui ne doivent pas mépriser l’électeur moyen qui n’est pas nécessairement antisémite ou misogyne, mais surtout exaspéré du marasme économique et social auquel il est confronté. Par Lionel Lafontaine, consutant Senior du cabinet VERTUO conseil
11	http://www.vertuoconseil.com/portfolio/favoriser-prise-dinitiative-sein-grands-groupes/	
12	http://www.vertuoconseil.com/portfolio/un-entretien-avec-aurelien-lachaud-responsable-du-pole-innovation-et-nouveaux-usages-la-banque-postale/	 Un entretien avec Aurélien Lachaud, Responsable du Pôle Innovation et Nouveaux Usages (La Banque Postale) VERTUO 2017-01-12T11:55:48+00:00 Project Description Qu’est ce qu’un wallet ? Le wallet inspire de nombreuses définitions dont certaines le limitent à une solution de paiement sur Internet. Cette définition réductrice peut être largement étayée par celle de la Banque de France : « Une solution permettant à un utilisateur de confier à un tiers, jugé de confiance, des données de paiement et des données personnelles, stockées en vue d’effectuer ultérieurement notamment des ordres de paiement. ». Le wallet ne se limite donc pas à un simple moyen de paiement sur Internet. Celui-ci a une vocation multicanale (paiement en ligne et paiement de proximité), multi-devices (ordinateur, mobile, tablette), multi-services et multi-usages (intégration des cartes de fidélité, cartes de paiement, coupons, bons plans, etc.) permettant de répondre à tous les nouveaux besoins des clients particuliers et commerçants. Il existe deux grands types de wallet : le wallet commerçant tels que Fivory (Crédit Mutuel-CIC) et Flash’N Pay (Oney Banque Accord) le wallet émetteur tels que Paylib, V.me by Visa et MasterPass par MasterCard. Comment définir la solution de paiement Paylib en quelques mots ? Paylib est un service innovant lancé en septembre 2013 par BNP Paribas, la Société Générale et La Banque Postale. Celui-ci sera également proposé en 2015 par le Crédit Mutuel Arkéa et le Crédit Agricole qui rejoindront le consortium Paylib. Ce nouveau service de paiement permet aux clients d’effectuer des achats en ligne sans avoir à saisir leurs coordonnées bancaires quel que soit le device utilisé : ordinateur, mobile ou tablette. Le sous-jacent de la transaction est une carte bancaire. Paylib est un système de paiement, dit « 4 coins », permettant d’apporter aux clients les avantages de la carte (assurance voyage, prolongation garantie constructeur, etc.) et aux commerçants de simplifier leur transaction sur leur site Internet (gestion des litiges, back office, etc.). Quel est le bénéfice client de souscrire à une telle offre ? Le paiement via la solution Paylib est simple, accessible et sécurisé. Le client ne saisit pas ses données de cartes bancaires à l’enrôlement et lors de chaque transaction. La banque est le tiers de confiance auquel le client confie ses données de cartes bancaires. De plus, la banque est l’unique interlocuteur du client assurant la gestion des cas de litiges/fraudes. La banque du client détenant le service Paylib est donc le garant, et reste le gestionnaire exclusif au même titre que dans le cadre des moyens de paiement traditionnels. A ce jour, cette solution de portefeuille numérique satisfait déjà près de 300 000 clients particuliers et 700 e-commerçants partenaires (dont vente-privee.com, voyages-sncf.com, billetreduc.com, priceminister.com, showroomprive.com, etc). Comment se différencie le portefeuille numérique Paylib comparativement à d’autres wallets comme Paypal ? Paylib est un moyen de paiement français, commercialisé par plusieurs banques en France, et créé pour leurs clients afin de couvrir leurs besoins et leurs usages bancaires connus. Face à l’augmentation croissante de la fraude sur les paiements en ligne, la banque endosse son rôle de tiers de confiance en garantissant les transactions effectuées par les clients depuis la solution Paylib. La promesse marketing principale est donc d’assurer la sécurité des moyens de paiement du client. La sécurité et la confiance, que se doit d’inspirer une banque, constituent un avantage concurrentiel important par rapport à certains concurrents. Ainsi, chaque banque est en charge de la relation commerciale avec ses clients. Paylib a pour objectif de renforcer la relation bancaire entre les clients et leur banque en se constituant comme un vrai outil de fidélisation. Comment se démarque la solution Paylib afin de capter/conquérir des grandes enseignes du e-commerce ? Quels sont les nouveaux services associés à Paylib ? Paylib offre un parcours client optimal et sécurisé. Pour chaque transaction réalisée, le wallet offre la garantie de paiement et donc la sécurité financière aux commerçants. La solution a été conçue pour répondre aux nouveaux besoins des commerçants et pour faciliter les achats en ligne multi-devices. Ainsi, Paylib étend son acceptation à l’international grâce à un accord avec MasterPass (solution de paiement numérique de MasterCard). Courant 2015, les utilisateurs de Paylib pourront donc payer chez plus de 60 000 commerçants MasterPass participants dans 13 pays dans le monde. L’intégration de Paylib dans les applications smartphone est également à l’étude. Quel est l’avenir du wallet ? L’objectif d’un wallet est de simplifier la vie quotidienne d’un client. Il est difficile d’imaginer des solutions de paiement indépendantes les unes des autres. Le succès du wallet passe forcément par une convergence des différents services de paiement. A terme, les banques devront proposer un produit qui permettra de réunir tous les services de paiement actuellement proposés. Il est cependant trop tôt pour savoir quelle forme prendra cette convergence. Aurélien Lachaud – Responsable du Pôle Innovation et Nouveaux – La Banque Postale Propos recueillis par Sandie Baureilles et Marion Chabrier, consultantes VERTUO Conseil
13	http://www.vertuoconseil.com/portfolio/rd-retour-sur-la-premiere-conference-du-lab-vertuo/	
14	http://www.vertuoconseil.com/portfolio/vertuo-de-nouveau-jt-de-20h-tf1-dimanche-5-mars/	
15	http://www.vertuoconseil.com/portfolio/dici-priips-pouvoir-comparer-produits-assurantiels-et-bancaires/	
16	http://www.vertuoconseil.com/portfolio/enjeux-perspectives-activites-de-marche-bfi/	
17	http://www.vertuoconseil.com/portfolio/michelin-ou-lexemple-dune-diversification-moderne/	 Michelin ou l’exemple d’une diversification moderne VERTUO 2017-02-12T13:18:13+00:00 Project Description L’argus – 21 mars 2016 : Abandon progressif de l’activité poids lourd, déploiement massif sur la Toile… Adrien Aubert, senior manager chez Vertuo Conseil, décrypte la stratégie adoptée par Michelin au cours des derniers mois. Le fabricant français de pneumatiques Michelin a annoncé une réorganisation de ses activités dans son berceau historique de Clermont-Ferrand, qui va provoquer la fermeture de l’atelier de rechapage poids lourds d’ici fin 2017. Au total, 494 postes (sans départs contraints) vont être supprimés. Cette restructuration survient quelques mois après avoir l’annonce de la fermeture des usines de Fossano (Semi finis) et de Pneu Laurent (Rechapage Poids Lourd), filiale basée à Oranienburg en Allemagne, en fin d’année 2016, tandis que les activités du site de Ballymena (poids lourd), au Royaume-Uni, cesseront en 2018. Ces fermetures d’usine illustrent davantage une réaction à un marché plus qu’une stratégie d’investissement orientée vers une logique d’optimisation des coûts de production. En effet, le marché du pneumatique poids lourd subit les assauts de la concurrence asiatique qui pousse des produits neufs moins chers que les pneus rechapés de Michelin. Les entreprises de transports, structurellement fragilisées depuis 2008 en Europe et aux Etats-Unis, se sont naturellement engagées dans une logique de réduction des coûts, qui a eu pour effet de booster l’achat de pneumatiques de marques plus compétitives. Il faut savoir que le transport est devenu l’un des secteurs les plus risqués aux yeux des banques, devant le bâtiment, la restauration ou encore l’hôtellerie, qui étaient historiquement les secteurs les plus sensibles. Des développements tous azimuts sur la Toile Ainsi, et sans pour autant se retirer du marché, la direction générale du manufacturier a logiquement choisi de se focaliser sur son autre activité : la production de pneus de véhicules de tourisme, des produits à fort contenu technologique avec lesquels la marque est en progression sur ses marchés stratégiques, comme l’illustrent ses résultats records 2015. Des performances bien aidées, il est vrai, par les cours pétroliers, mais aussi une image en béton armé tirée par les nombreux succès du manufacturier en compétitions automobiles (nouvelle Formule F, WRC, etc.). Une orientation qui prend sa pleine mesure sur Internet. En début d’année dernière, Michelin a fait main basse sur le site revisersavoiture.com, avant d’entrer dans le capital du site Allopneus.com, à hauteur de 40 %. Quelques semaines plus tard, le manufacturier annonçait l’acquisition de 100 % de Blackcircles.com, numéro 1 de la vente de pneumatiques sur Internet au Royaume-Uni, pour un montant de 50 millions de livres sterling. Dernier exemple en date : le rachat de Bookatable, début janvier 2016, afin de renforcer sa position sur le secteur restauration en croisant ce nouveau système de réservation avec sa sélection historique des meilleures tables. Enfin, depuis quelques semaines, le géant français du pneumatique commercialise directement aux Internautes son dernier pneu hautes performances, le Pilot Sport 4, via un pop-up store éphémère disponible sur son site Internet http://www.michelin.fr. L’exemple de l’industrie financière Ainsi, le contraste est saisissant entre d’un côté le retrait progressif d’un marché industriel traditionnel, le pneumatique poids lourd, et le renforcement de l’autre des développements sur Internet, qui illustre le virage « digital » pris par l’ensemble des acteurs économiques et, parmi eux-mêmes, les industries les plus lourdes et traditionnelles. Un virage digital pour tenter de répondre à une évolution du besoin et surtout du mode de consommation, à l’image des transformations opérées dans d’autres secteurs, comme l’industrie financière. Sous fortes contraintes réglementaires de liquidité et de solvabilité, elle ajuste d’un côté sa présence sur des marchés historiques, tels que le crédit ; et d’un autre côté, elle investit massivement dans les partenariats avec les « FinTechs » (technologies financières) dans l’optique non seulement d’apporter un service différenciant mais surtout de répondre à un usage du client final qui a évolué. Par Adrien Aubert, Senior Manager du cabinet VERTUO conseil
18	http://www.vertuoconseil.com/portfolio/le-marche-du-credit-a-la-consommation-entrevoit-il-le-bout-du-tunnel/	 Le marché du crédit à la consommation entrevoit-il le bout du tunnel ? VERTUO 2017-02-12T13:36:49+00:00 Project Description Les Echos – 28 juin 2015 : Alors que notre modèle économique est profondément basé sur la consommation des ménages, indicateur clé scruté de près par tous les observateurs économiques et décideurs politiques, le crédit à la consommation en est un déterminant majeur. Ses encours représentent près de 13% de la consommation des ménages et 7 points de PIB, proportion parmi l’une des plus élevées de la zone euro. Or depuis 2008 ce marché a vécu l’une des pires périodes de son histoire. Pourquoi tant de turbulences ? Dans un environnement conjoncturel dégradé depuis plusieurs années déjà et un enchaînement sans précédent de crises en tout genre, la situation du marché du travail n’a fait que se dégrader en France, et reste à ce jour très fragile : le taux de chômage dépasse désormais le seuil des 10 % au niveau national et le revenu disponible des ménages (revenus d’activités, prestations sociales, et revenus du patrimoine) évolue faiblement, trop faiblement pour soutenir la consommation. Et ce d’autant plus que les enjeux de redressement des comptes publics et les politiques de hausses des recettes fiscales ont touché l’ensemble de la population qui s’est vue fortement solliciter ces deux dernières années : impôts, taxes, et autres contributions directes ou indirectes. Jouissant d’une mauvaise image sociale, accusé d’être à l’origine des excès d’endettement, stigmatisé par la crise financière et le surendettement de certains ménages, le crédit à la consommation possède néanmoins deux vertus : il est un levier à la croissance économique surtout dans nos sociétés occidentales, mais il a également une utilité sociale manifeste permettant ainsi à un certain nombre d’entre nous d’accéder à des biens ou des services et participe donc à notre épanouissement… Oscar Wilde : « Il y a deux tragédies dans la vie : l’une est de ne pas satisfaire son désir et l’autre de le satisfaire ». De 2000 à 2007, soit les 7 dernières années précédant la fameuse crise de 2008, le taux de croissance annuelle moyen des encours du crédit à la consommation au sens large atteignait les 6 % ; sur les 7 années suivantes, il est tombé à 1,2%… Et encore ce taux doit être interprété en tenant compte des diverses mesures politico-économiques visant à soutenir la consommation : à titre d’exemple, la prime à la casse instaurée de 2009 à 2010, couplée à la baisse des taux directeurs par la BCE qui a favorisé des taux de crédit attractifs et historiquement bas pour l’époque, ont très largement contribué à la croissance des ventes automobiles sur la même période et donc au marché du crédit à la consommation. Rappelons qu’en France, plus d’un véhicule neuf sur deux est financé par un crédit à la consommation. Outre des éléments très mesurables qui influent sur le marché du crédit à la consommation tels que le revenu disponible, le taux de chômage, les taux d’intérêt, le niveau d’inflation ou plus généralement la conjoncture économique (notamment l’évolution des achats de biens durables), des éléments plus subtils entrent en ligne de compte pour en expliquer ses tendances : la confiance (ou l’inquiétude) des ménages en l’avenir qui en temps de crise préfèrent épargner ou se désendetter et repoussent leurs achats de biens durables (automobiles, appareils électroménagers, etc.), mais aussi les évolutions règlementaires qui ont notamment mis fin à l’âge d’or du crédit renouvelable. Quel impact de la réglementation sur le crédit renouvelable ? Transposant en droit français la directive européenne « Consumer Credit Directive » (CCD), la batterie de mesures issues de la loi Lagarde (ou LCC : Loi sur le Crédit à la Consommation) et entrées en vigueur entre septembre 2010 et octobre 2011 a directement contribué à un net recul de la production des crédits à la consommation (et plus particulièrement des crédits renouvelables) et une baisse significative des agios payés par les consommateurs. Si ces derniers ne peuvent que se féliciter des effets positifs en termes de protection du consommateur et de coût d’emprunt, c’est la triple peine pour les établissements de crédit spécialisés : une décroissance de leur PNB, une diminution de leur marge historiquement tirée par des taux élevés et par la durée pratiquée sur les crédits renouvelables, et une complexification de leur processus d’acquisition avec à la clé un coût financier réel d’ajustement de ces processus. Un rapport du Ministère de l’Économie et des Finances de septembre 2012 révélait que « selon les estimations des établissements de crédit, la LCC amputerait de 1,5 milliard d’euros leur produit net bancaire (PNB) sur une période couvrant les exercices 2011 à 2014 », dont « 74 % de ce montant seraient dus à la réforme des taux d’usure, 15 % à la réduction de la durée de remboursement des comptes de crédit renouvelable et 9 % à la mise en place de l’option comptant par défaut ». Ainsi, depuis fin 2010, les encours des seuls crédits renouvelables ont chuté de plus de 20% alors que les encours sur les crédits personnels amortissables ne diminuaient que de 1%. Sur la même période, la production nouvelle de crédits renouvelables passait de 14,4 milliards d’euros à moins de 10,9 milliards d’euros, soit une baisse de 32%, le prêt personnel amortissable étant dorénavant le premier produit de financement des établissements spécialisés. Et maintenant ? Le cadre règlementaire vient de nouveau de s’alourdir pour les établissements de crédit spécialisés qui de par leur positionnement ont naturellement plus pâti des effets de la LCC que les banques. En effet, la loi Hamon vient, entre autres, renforcer significativement l’alternative au crédit renouvelable par un crédit amortissable, réduire à un an la maturité des crédits renouvelables non utilisés, ou encore mettre en place un registre national des crédits aux particuliers, le fameux « fichier positif », dernière mesure censurée par le Conseil Constitutionnel en mars 2014. La fin de l’âge d’or du crédit renouvelable évoqué plus haut n’en sera que confirmée. Dans cette période trouble, les établissements de crédit ne sont pas restés inactifs ; après avoir utilisé les remèdes classiques de réduction des coûts, de réorganisations internes, de sélectivité des implantations régionales, de positionnement produits voire de plans de sauvegarde de l’emploi, des mutations plus profondes sont engagées ou en passe de l’être : – La transition numérique : face à l’émergence du e-commerce et alors que le financement sur lieu de vente reste un canal d’acquisition significatif, les établissements de crédit se « digitalisent » en proposant de nouvelles solutions et de nouvelles approches « client » et s’adaptent au monde du numérique. En 2012, Franfinance fut pionnier en lançant le 1er service entièrement dématérialisé de souscription de crédits à la consommation en points de vente divisant au passage par deux la durée de montage d’un contrat de financement. – La relation client : les établissements spécialisés repensent leurs modèles pour gagner en efficacité opérationnelle dans un contexte règlementaire contraint, et pour encore améliorer la proximité, la satisfaction et la fidélité de leurs clients mieux informés et toujours plus exigeants. Qui n’a pas vu que Cofidis, qui en a fait son cheval de bataille, avait été élu « Service Client de l’Année » pour la troisième fois consécutive ? – La dédiabolisation de leur activité auprès du grand public : pour lutter contre une image source de faillite financière, matérielle, voire morale, les spécialistes du crédit deviennent des partenaires de projets ou des gestionnaires de budget… douce et subtile reconversion à l’enjeu marketing important. Depuis mai 2014, la signature de Cetelem n’est plus « le crédit responsable », mais devient « plus responsables, ensemble » ; en mars 2013 PSA Banque s’affichait à l’occasion de son entrée sur le marché de l’épargne comme « la banque de l’économie réelle ». – Le développement de nouvelles solutions venant enrichir le catalogue produit des établissements spécialisés : l’épargne intégrée dans l’offre Cetelem depuis 2011, les crédits immobiliers, les solutions de facilités de paiement pour les e-commerçants, ou la téléphonie mobile chez Cofidis depuis 2011. Sur ce tableau noir, force est de constater une légère embellie depuis l’été 2014 et une inversion de cette tendance négative du marché du crédit à la consommation, tendance qui va de pair avec les premiers frémissements d’un retour à la croissance qui se font (légèrement) sentir, et avec l’évolution positive du moral des ménages et donc de leur confiance. Dans une enquête publiée le 28 avril, l’INSEE relevait que la confiance des ménages atteignait son plus haut depuis janvier 2010 et que cet indicateur progressait positivement pour le 3e mois consécutif. Après plusieurs années de décroissance, les établissements spécialisés retrouvent donc timidement en 2014 un niveau de production en hausse de 1,2%, mais qui demeure en valeur absolue inférieur de plus de 20% au pic historique de 2007. Cette tendance reste à confirmer et, au-delà des efforts entrepris par ses différents acteurs, le marché du crédit à la consommation restera sous influence de la situation économique du pays, des arbitrages et des comportements des consommateurs en matière d’achat, et des effets de la réglementation et de la fiscalité. Par Patrick Lebihan, senior Manager du cabinet VERTUO Conseil
19	http://www.vertuoconseil.com/portfolio/rftb-retour-a-lequilibre-entre-confiance-et-controle/	 RFTB : Retour à l’équilibre entre confiance et contrôle Mathilde. Taillez 2017-06-29T08:28:12+00:00 Project Description Lors de la crise financière de 2007 à 2009, les établissements bancaires ont subi de grosses pertes liées aux activités de marché. Les causes de ces pertes furent assez clairement identifiées. Notamment un manque de maitrise, voire une non-prise en compte de l’ensemble des facteurs de risques. Plus précisément du risque de crédit sur opérations de marchés. Cette incapacité à suivre ces risques dans les indicateurs officiels (en particulier la fameuse Value at Risk – VaR), pour des produits complexes, a mis en évidence un angle mort désastreux. Cet accident a eu comme principale victime la collectivité (les Etats), avec la mise en place d’un fond de résolution (garanties des Etats pour le refinancement des banques) et des actions de recapitalisation substantielles des établissements de crédit. Les banques ont ainsi vu leur image écornée, et ont été fortement contraintes sur leur coût du risque. Quelques années plus tard, à travers la proposition de Revue Fondamentale du Trading Book (RFTB), le comité de Bâle semble basculer son mode de pilotage, initialement fondé sur la confiance en la capacité de résilience des établissements, vers un contrôle renforcé grâce à des méthodes homogènes et plus transparentes. L’histoire récente tend à démontrer qu’une zone de non droit, du moins non-réglementée, représente une opportunité de profit pour les entreprises. Sans penser à mal (du moins en pensant à court terme), cet opportunisme perçut par les acteurs crée un déséquilibre ou une bulle. Au début des années 2000, la complexification rapide des instruments traités, et la mise en avant des performances des opérateurs et actionnaires, motivés par de très fortes rémunérations, n’ont pas été accompagnées à travers les dispositifs de contrôle. Le constat post-mortem (post crise 2009) de brèches dans le dispositif de contrôle pousse le régulateur à compléter son dispositif et à reprendre la main sur le pilotage des risques, au risque de devenir une entrave à l’activité économique des banques. Le comité de Bâle a réagi assez vite suite à cette faillite du système. Une des premières mesures fût d’imposer le suivi de ces risques avec des coûts additionnels en capitaux propres, pour s’assurer d’un minimum de provisionnement de ces derniers, à travers de nouveaux indicateurs tels que l’Incremental Risk Charge (IRC) qui permettent de compléter l’arsenal de mesures dédiées au suivi du risque de marché. En outre, l’aspect systémique du risque bancaire nécessite une réponse globale et coordonnée : la RFTB vise à uniformiser et refondre les méthodologies de suivi des risques des activités de marché. Cette proposition et les concertations menées doivent aboutir à un dispositif de supervision globale des risques pris par les banques. Etant donné la forte hétérogénéité des activités des établissements (et de leur prise de risques sur les marchés de capitaux), la normalisation des méthodes d’évaluation, doit permettre à l’ensemble de la place de produire les indicateurs représentant des visions harmonisées de ces risques. Une nouvelle ligne de conduite Dans cette logique, RFTB propose une révision des approches en modèle interne, qui doit permettre de s’appuyer sur des bases méthodologiques communes, avec l’assurance que chaque typologie de risque soit bien classifiée. Les challenges méthodologiques (ex : maitrise des horizons de liquidités) liés à ces évolutions sont nombreux et laissent ouvert certains débats sur l’indépendance des banques sur leur maitrise des risques et leur autonomie stratégique (ex : forte augmentation du coût d’investissement sur les marchés émergents). Ces échanges sont fondamentaux et permettent de remettre à plat ces notions de risques négligées (corrélation et liquidités) dans les modèles existants. Néanmoins, cela n’adresse pas forcement la question d’une gestion harmonisée des expositions. Car, comme c’est le cas aujourd’hui, le modèle interne sera quand même une interprétation de la règlementation propre à chaque établissement. Afin de capitaliser sur cette refonte et de pouvoir observer et contenir des comportements grégaires précurseurs de crises, en plus de cadrer les risques identifiés, il apparait nécessaire pour les instances de contrôle d’avoir un tableau de bord efficace. Indépendamment de leur capacité à suivre leur risque en modèle avancé, les banques devront systématiquement rapporter au superviseur bancaire les résultats de leur consommation de fonds propres liées aux activités de marché, en méthode standard. Cette contrainte permettra au régulateur de suivre de manière consolidée les expositions et les risques pris par les établissements bancaires. Même si ces chiffres ne représentent pas le coût du risque porté dans le bilan des banques en modèle avancé, le modèle standard devient l’échelle commune de suivi des risques et de communication vers les autorités de contrôle. Ces expositions permettront d’alimenter de possibles tableaux de bord. La maîtrise de ces informations peut, par exemple, permettre de challenger les scénarios de stress des banques d’une manière plus transverse (la BCE pourrait lancer des scénarios de stress, à sa main, sur l’ensemble des positions collectées). Mais aussi de définir selon les tendances actuelles, quels acteurs se mettent en danger ou quelles classes d’actifs représentent une opportunité à cadrer (pour prévenir les bulles). Il y a un équilibre à trouver après la mise en place de ce suivi et cette régulation accrue. L’enjeu est de ne pas asphyxier les banques. De ce point de vue, le comité de Bâle tend vers le compromis et compte toujours sur la responsabilisation. Le plan d’action est de laisser à la main des banques, l’évaluation de leurs risques, en imposant toutefois la production d’une consommation réglementaire théorique (standard). Liberté conditionnelle Ce modèle standard, devenant par ailleurs une épée de Damoclès pour les banques. Car en cas d’exceptions trop fréquentes de leur modèle interne, elles auront pour obligation de basculer en modèle standard, beaucoup plus consommateur en capitaux propres. Dorénavant les banques ont une vision claire de l’arbitrage qui s’offre à elles, elles évalueront un manque de maitrise des risques à travers la production de la consommation standard de capitaux propres. On peut continuer à se demander si cette proposition sera suffisante, et en quoi cela peut prévenir d’une politique d’investissement hasardeuse sur des instruments mal maitrisés offrant une forte rentabilité. Cela pourrait impacter de manière très négative l’ensemble des positions détenues dans le trading book car pour un type d’actif mal maitrisé, l’ensemble du portefeuille serait dégradé avec la bascule du calcul prudentiel de la méthode avancée à la méthode standard bien plus coûteuse. Avec cette nouvelle règlementation, le comité de Bâle semble avoir rattrapé son retard dans son appréhension des risques liés aux opérations de marché ; attention toutefois à ne pas sous-estimer l’inventivité des banques car, comme l’a rappelé le passé récent, toute zone non réglementée n’est qu’une prochaine opportunité. Par Lionel Lafontaine, consultant Senior du cabinet VERTUO conseil
20	http://www.vertuoconseil.com/portfolio/pourquoi-les-banques-ferment-de-plus-en-plus-leurs-agences/	 Pourquoi les banques ferment de plus en plus leurs agences ? Mathilde. Taillez 2017-03-24T09:41:54+00:00 Project Description Challenges – 22 mars 2017 : Le jour même de l annonce par BNP Paribas de son plan numérique à l horizon 2020, ce 20 mars, Le Monde révélait que la banque de la rue d Antin préparait la fermeture de 200 agences d ici 2020, avec réduction d effectifs de 2% à 4% par an à la clé. Pour Maïa Grangier, senior Manager de Vertuo Conseil (groupe Square), en charge de l innovation, les banques n ont d autres choix que d accélérer leurs restructurations.   Du Crédit Agricole aux Caisses d’Epargne, la puissance des banques de détail a longtemps reposé sur la taille du réseau d’agences. Il semble que l’on assiste à une rupture… La révolution numérique est belle et bien en marche et le secteur bancaire n’en sortira pas indemne. Les plans de transformation à horizon 2020 s’enchaînent dans les principales banques françaises et les mesures sont radicales: partant du constat que les clients se rendent moins en agence, les banques «optimisent» leur réseau par la suppression de leurs points de ventes. Fin 2015, c’est la Société Générale qui démarrait le mouvement en annonçant son intention de supprimer environ 400 de ses 2.221 agences d’ici à 2020. Suivi de près par le LCL, qui a annoncé en 2016, un programme de fermeture d’environ 250 agences, sur les 1 900 que compte son réseau. L’année 2017 a continué sur le même rythme avec BPCE qui a dévoilé le 21 février dernier, la suppression de 400 points de vente aujourd’hui au nombre de 8.000. C’est maintenant à BNP Paribas de dévoiler son plan stratégique à 2020 avec la fermeture de 50 agences par an sur toute la durée du plan.   Pourquoi cette accélération dans les annonces? Le secteur bancaire est en pleine mutation: l’arrivée d’Orange Bank sur le marché au printemps prochain qui intègre le digital et l’intelligence artificielle au cœur de ses processus et de son offre bouscule les établissements traditionnels. Associée à la loi Macron, qui facilite les démarches des particuliers pour changer de banque, la concurrence s’annonce acharnée. En conséquence, les banques traditionnelles misent sur le digital avec de nouveaux formats d’agence, la généralisation des ventes à distances, le développement des robots (chatbots), mais aussi sur l’optimisation du traitement des données encore peu exploité jusqu’à présent. Autant que l’argent des déposants, vous estimez que les données des clients constituent la richesse des banques? La donnée devient un actif stratégique et un atout concurrentiel. La banque créée de la valeur via une gestion efficiente et en temps réel des données de son client. Elle va en effet pouvoir acquérir une meilleure connaissance de son client, des membres de sa famille, de ses comportements d’achat, de son évolution de vie et va ainsi pouvoir anticiper ses besoins et lui proposer le bon produit, au bon moment et par le bon canal: une offre pour la rentrée scolaire de ses enfants par SMS, un livret d’épargne pour le nouveau-né par notification Facebook… autrement dit une stratégie marketing plus ciblée et donc plus efficace. La mutation numérique du secteur bancaire est en marche et les banques traditionnelles n’ont pas dit leur dernier mot !   Par Maïa GRANGIER, Senior Manager du cabinet VERTUO Conseil
21	http://www.vertuoconseil.com/portfolio/3-bienfaits-de-musique-travail/	 Les 3 bienfaits de la musique au travail Mathilde. Taillez 2017-09-06T06:07:11+00:00 Project Description Huffingtonpost – 31 août 2017 : La musique au travail n’est plus un tabou, même dans des environnements qui y étaient plutôt réfractaires au départ, celle-ci est entrée dans l’usage courant.   Business calls me Trouble down the waterfront You know, I tried to But now i m running outta lies     Run, Run, Run  , cette musique de Phoenix, vantant la liberté de fuir la réalité du boulot quotidien, semble rattrapée par la réalité, tant l environnement professionnel est maintenant conditionné par l écoute musicale. Au rythme du beat des productions de Pharell Williams ou de Will I am (  Bring the Action!  ), les mails fusent, la créativité s&#8217;emballe, les gestes et les idées se synchronisent pour ne faire qu un avec la musique. Car oui, la musique au travail n est plus un tabou, même dans des environnements qui y étaient plutôt réfractaires au départ, comme le secteur bancaire, celle-ci est entrée dans l usage courant avec la démocratisation des ipod, iphone et lecteurs mp3.   Un phénomène qui relève de la sociologie Cette prise d assaut de la musique dans les bureaux et les open-space se transforme en véritable phénomène de société, jusqu à devenir la normalité. Doit-on y voir pour autant un effet de mode ou une installation durable ? Les faits penchent plutôt pour la seconde option car ce mouvement s inscrit bel et bien dans une logique d individualisation de la société observée depuis plusieurs années, jusqu à interpeller les sociologues. La musique, mise à disposition massivement par l accès à des plateformes de téléchargement ou de streaming, comme Napster ou Deezer, devient un vecteur de consommation, comme n importe quel bien ou service. Dans ce contexte, le besoin d exporter sa passion au travail est inévitable, où l individu moyen passe 80% de son temps. Tel les envolées lyriques d un concerto de Rachmaninov, le phénomène va crescendo, jusqu à croiser des bureaux entiers dont le seul bruit provient des hochements de tête et des claquements de main au son de leur musique préférée ! De prime abord, la musique tendrait donc à éloigner les gens au travail. La réalité est bien entendue plus complexe que cela.   Une thérapie au sens médical La musique, au-delà de l aspect individualiste, a de sérieux atouts sur le plan médical. Tout d abord, toute mélodie n est pas adaptée à une ambiance professionnelle: c est ce qui transparait de certaines études scientifiques sur la productivité et la qualité du travail. Il ressort notamment que les morceaux instrumentaux ou encore la musique classique permet une meilleure concentration sur les tâches intellectuelles, là ou pour les tâches répétitives, rien de mieux qu une musique rythmée ! Tout est également une question d affinités: un baby boomer sera plus influencé et productif sur un morceau de Springsteen ou des Wings qu une personne des années 90, qui sera nourrie au son de la dance, de nirvana ou de rage against the machine (du moment que la révolte anticapitaliste ne choque pas votre responsable). Au-delà de trouver le bon morceau pour la bonne personne, la musique sert de conducteur pour le fonctionnement des liaisons neurologiques. Un chercheur en neuropsychologie, Henri Platel, travaillant notamment sur la maladie d Alzheimer, indique que les malades se rappellent mieux des mélodies que des textes car la musique fait appel à différentes zone cognitive et logées dans les deux hémisphères du cerveau. Au travail, ceci prend tout son sens dans la mesure où nous sommes constamment en train de d avancer simultanément sur des tâches multiples et variées: mailing, réflexion, présentation sous PowerPoint, infographie sous Visio, expression orale et écrit&#8230; nous sollicitons toutes les zones de notre cortex cérébral et la musique nous aide à fluidifier nos pensées, nos prises de décisions en ayant un contexte familier, un cocon qui nous permet de nous abstraire de toute influence externe. C est ce rituel familier qui semble prédisposer les patients à se souvenir des airs de musique même au dernier stade de la maladie. Une influence positive de la musique donc, au travail comme en institut de santé, et qui, qui sait, pourrait même atténuer ou retarder l apparition de la maladie d Alzheimer ?   Un plaisir avec modération pour être efficace Toutefois, avoir une personne constamment isolée dans son environnement, les écouteurs en permanence sur les oreilles, peut avoir des effets néfastes sur une équipe, ses collègues peuvent avoir la désagréable impression de le déranger à chaque fois pour lui poser une question. Rater la dernière blague tendance dans l open-space ou une information capitale sur le projet peut s avérer problématique pour les addicts musicaux que nous sommes ! Mais avouez, qui n a jamais été pris de la soudaine envie de mettre ses écouteurs pour éviter le collègue trop bruyant et à la voix stressante, jusqu à vous provoquer des migraines ? Là encore, médicalement, cela est recevable car les blocs opératoires proposent systématiquement des lecteurs mp3 pour aider les patients à gérer la douleur. Nous y revoilà: la musique a des vertus thérapeutiques ! Savoir doser son écoute et choisir son ambiance en fonction des tâches et des situations semble être la clé entre la performance, la qualité de travail, et le bien-être. A chacun de trouver le compromis idéal pour délivrer le maximum, tout en prenant garde de veiller au respect de ses collègues de travail. Musicalement vôtre, quelques morceaux dans différents registres, de la tracklist qui a servi à la rédaction de cet article : Moves like Jagger &#8211; Maroon 5 Fuel – Metallica Depeche Mode – Personal Jesus Air, Orchestral Suite n°3 en ré majeur – Johann Sébastian Bach Bonne écoute. Par Julien DELRIEU, Project Manager du Cabinet Vertuo Conseil
22	http://www.vertuoconseil.com/portfolio/changement-climatique-risque-financier-a-ne-ecarter/	
23	http://www.vertuoconseil.com/portfolio/faillites-bancaireset-a-la-fin-les-particuliers-payent/	 Faillites bancaires…et à la fin, les particuliers payent VERTUO 2017-02-12T13:25:55+00:00 Project Description La Tribune – 28 Janvier 2016 : La cacophonie actuelle autour des frais de tenue de comptes nous fait oublier l’entrée en vigueur du mécanisme de résolution unique au 1er janvier 2016, s’appliquant aux faillites bancaires. Simple coïncidence ou orchestration parfaite ? Au menu, la possible ponction des comptes des particuliers, qui sale l’addition autrement que la facturation des comptes courants, laquelle n’était finalement qu’une légère mise en bouche ! Est-il encore besoin de rappeler l’ampleur des politiques d’endettement public qui devaient tenter de sauver l’activité économique post crise des subprimes ? Ces tactiques court-termistes se sont traduites dans certains pays européens par une explosion des dépenses partiellement financée par le levier fiscal. Il n’est pas totalement infondé de considérer que les contribuables ont alors indirectement assumé le coût (et aujourd’hui le coup) de la crise pour le compte de certaines banques. Ils sont désormais mis plus directement en première ligne avec l’entrée en vigueur de la « Banking Recovery and Resolution Directive » (BRRD), complétée du règlement « Single Resolution Mechanism », dernières briques de l’Union Bancaire, après le mécanisme de supervision bancaire en place depuis fin 2014, voulue en réponse politique aux déficiences de l’encadrement des activités financières des années 2000. Plusieurs contributeurs pour renflouer les banques Que sont ces textes ? Pourquoi première ligne ? En réalité, les observateurs diront 3ème ligne car le dispositif de résolution prévoit plusieurs lignes de défense contre la faillite bancaire. En effet, le renflouement des caisses appelle plusieurs contributeurs. En premier lieu, lorsque la capitalisation boursière d’une banque s’effondre en période de crise, les actionnaires (qui peuvent tout à faire être des particuliers à travers des produits d’épargne classique type PEA ou assurance vie…) sont priés de bien vouloir prendre leur courage à deux mains pour réinvestir quelques pièces au prétexte de sauver le monde d’une nouvelle crise financière. Alors, qui se lance le premier ? En second lieu, les créanciers Au second rang figurent les créanciers, autrement dit ceux à qui la banque doit de l’argent. En quoi le renflouement les sollicite ? C’est simple, on efface l’ardoise, on passe en pertes leurs détentions. A l’heure de la lutte contre le risque systémique, à l’heure des participations croisées, des produits dérivés et de la dispersion des obligations dans les multitudes de portefeuilles que compte la sphère financière, le phénomène de contagion laisse perplexe sur la portée de la mesure. … et les particuliers Au troisième rang apparaissent enfin… nous, les particuliers. Avec désormais la possibilité pour la banque de ponctionner les comptes des particuliers pour lesquels le cumul des avoirs au sein d’un même établissement dépasse 100 000€, incluant les contrats d’assurance vie. Notons que, en cas de faillite imminente d’un acteur majeur, il est probable que l’ensemble des places soit dans le rouge et que les performances des fonds investis à travers les assurances vie soient très dégradées, ce qui réduit d’emblée la portée de la mesure. Quant à la question de savoir s’il est pertinent de dépouiller en temps de crise les seules personnes à même de pouvoir investir et injecter de l’argent dans l’économie, pour soit disant une cause plus grande, mieux vaut en fait ne pas se la poser… La question du droit à la propriété privée Les dépôts inférieurs à ce montant sont eux supposés garantis par le Fonds de Garantie des dépôts bancaires. Supposés car ses réserves s’élevait fin 2014 à un peu plus de 3 milliards d’euros, moins de 1% de l’encours des dépôts à vue en France… Pas de quoi effrayer la majorité des contribuables donc. Mais alors, si la mesure ne concerne qu’une infime minorité de la population, en quoi peut-elle réellement permettre de sauver les meubles ? Après le très agressif discours du Bourget puis les tentatives de fiscalisation jusqu’à 75% des revenus, aurait-elle, elle aussi, uniquement une valeur symbolique de solidarité unilatérale des « riches » envers les « pauvres » ? A tout le moins se pose la question du droit constitutionnel à la propriété privée de l’article 17 de notre Constitution : « La propriété étant un droit inviolable et sacré, nul ne peut en être privé, si ce n’est lorsque la nécessité publique, légalement constatée, l’exige évidemment, et sous la condition d’une juste et préalable indemnité. » Mais quelle indemnité ? Une médaille, une poignée de main et merci pour ce moment (jusqu’au prochain, bien entendu) ? La BCE a une curieuse approche de l’économie du partage. La BCE a déformé les marchés Avec sa politique de « Quantitative Easing », elle avait déjà en 2015 ouvertement déformé les marchés, en injectant de la monnaie à ne plus savoir qu’en faire, à part perturber le cycle économique avec des taux négatifs et biaiser le cours des actions en les réhaussant de façon spectaculaire et décorrélée de la valeur intrinsèque d’une entreprise. Certains verront désormais dans la BRRD une nouvelle opportunité pour se diriger vers des valeurs refuges exemptes du périmètre de la ponction : l’or en premier lieu, mais aussi les œuvres d’art qui disposent de certains privilèges fiscaux en France, tout comme les automobiles de collection (on assiste déjà depuis fin 2014 à une vraie bulle sur la cotation de modèles anciens et parfois mêmes nouveaux). Bref, certains marchés doivent s’apprêter à voir leurs cours flamber. « Tout bien que tu détiens est un souci qui te retient » Et pour nous ôter tous nos soucis, il fallait bien entendu compter sur notre grand gourou bureaucratique, qui fait de cette citation parodique un leitmotiv. Entre mesurette d’avance inefficace et déformation majeure des marchés, la BCE frappe fort pour démarrer une année 2016 déjà bourrée d’incertitudes politiques   économiques, incertitudes toujours peu propices à l’investissement à un moment où les économies européennes sont à un tournant technologique, social et environnemental. La seule certitude, c’est qu’entre innovations fiscales ou ponctions de leurs comptes, à la fin ce sont toujours les mêmes qui payent. Par Adrien Aubert, Senior Manager du cabinet VERTUO conseil
24	http://www.vertuoconseil.com/portfolio/rd-retour-video-conference-lab-vertuo-revue-banque/	
25	http://www.vertuoconseil.com/portfolio/vertuo-sengage-toute-lannee-pour-les-droits-des-femmes/	
26	http://www.vertuoconseil.com/portfolio/nivellement-par-le-bas-ou-par-le-haut-les-risques-des-reglementations/	 Nivellement par le bas ou par le haut : les risques des réglementations VERTUO 2017-02-12T13:31:35+00:00 Project Description Les Echos – 23 novembre 2015 : A l’heure du très attendu choc de simplification du cadre professionnel et législatif français, les projets de lois s’accumulent, se suivent… et ne se remplacent pas assez souvent. La simplification, un vœu pieu en opposition frontale avec le sacro-saint principe de précaution qui soulève le débat quasi-philosophique du besoin de définir les limites de la légalité et de la liberté économique. La simplification, oui mais pas pour tout le monde. Dire que les entités financières ont vu leur cadre réglementaire évoluer et se renforcer est un doux euphémisme qui ravive en permanence la question de la place de l’Etat dans l’économie et plus particulièrement dans le financement de l’économie et la sécurisation des acteurs financiers. Dans ce jeu politique qui oppose un lobby bancaire à un superviseur, symbole de la bureaucratie européenne, et à des politiques totalement étrangers à la gestion opérationnelle d’une entreprise privée, symbole de la vraie fracture sociale, légiférer est un jeu dangereux qui pénalise tout le monde, rassure certains, et parfois coûte cher. Le nivellement par le bas ou la politique du « plus jamais ça ». N’importe quelle structure politique à peu près censée ne pouvait que condamner les excès du shadow banking et de la sur-titrisation des années 2000, avec pour seul levier réel le renforcement des exigences prudentielles et organisationnelles (citons pêle-mêle CRD II, CRD III, CRD IV, CRR, Loi Bancaire Française / Volcker, etc.) en guise de « réprimande ». L’établissement de seuils minimaux, en particulier en solvabilité et en liquidité des actifs détenus, et l’établissement d’exigences minimales sur le plan des pratiques opérationnelles conduit à une transformation profonde du modèle économique. Nette réduction de la voilure sur les activités pour compte propre, nombreuses fermetures de desks de trading, augmentation des coûts et notamment ceux des fonctions support liées aux activités de contrôle et de reporting, interminables réorganisations en pagaille (au sein des DSI pour une « agilisation » des projets et entre les directions Risque   Comptabilité pour un rapprochement forcé par la mise en cohérence des reportings réglementaires), réorientation de la communication financière, etc. Tout est fait pour ne plus reproduire les erreurs du passé. Mais la difficile harmonisation des règles sur le plan international et le caractère peu anticipatif de ces règles ne permet en rien de s’assurer de la sécurité du système financier à l’heure de l’émergence de nouveaux acteurs non régulés (fintechs, GAFATs) et de la généralisation de politiques monétaires non conventionnelles. Bien que ces règlementations aient finies par être acceptées, le sacro-saint principe de précaution, souvent perçu comme une forme d’ingérence du public dans le privé, est-il encore légitime en 2015 dans un pays confronté à plus de 3 millions de chômeurs et plus de 10% de la population vivant sous le seuil de pauvreté ? Le nivellement par le haut… ou la stratégie de communication du coup d’épée dans l’eau. Si l’égalitarisme constitue l’un des piliers fondateurs de notre nation, bien plus qu’une finalité, il est à concevoir comme un moyen de tirer vers le haut. L’un des principaux moyens utilisés par un gouvernement pour mener des actions reste l’acte législatif. Il est légitime de se demander en quoi l’actuelle approche restrictive et punitive est une méthode pédagogique pour s’inciter, se pousser à l’amélioration. Les exigences de solidité des banques n’ont jamais été aussi fortes pour renforcer la stabilité financière, pourtant en 2014, les banques ont détruit de l’emploi et l’investissement ne s’est pas relancé. A quoi sert une banque solide si elle ne finance pas le développement économique, social et environnemental ? Le Code du Travail compte plus de 1500 pages, pourtant le nombre de chômeurs poursuit sa progression. A quoi bon verrouiller l’emploi salarié quand l’embauche est elle-même verrouillée ? Le ratio du nombre de filières Bac + 5 par étudiant figure parmi les plus élevé au monde pourtant près de 40% de cette population reste au chômage un an après l’obtention d’un diplôme. A quoi bon persévérer dans un cursus théorique quand des dizaines de milliers d’emploi demeurent non pourvus dans les secteurs primaires et secondaires ? Le nombre de radars au bord de nos si belles routes n’a jamais été aussi grand, pourtant le nombre de décès liés aux accidents de la circulation augmente. A quoi bon rouler lentement si on ne sait pas conduire et gérer une situation d’urgence ? Le modèle répressif est à bout de souffle et il est désormais grand temps de se pencher sur les objectifs fondamentaux, les moyens et les modalités de l’encadrement réglementaire. Vers de nouvelles approches réglementaires moins manichéennes ? En 2015, la notion de conformité ne peut plus être restreinte à une vision tout noir ou tout blanc et doit donc être repensée. La définition du cadre prudentiel, et plus globalement du volet législatif dans son ensemble, doit être réorientée vers du conseil économique, social, civique et environnemental a priori plutôt que comme un package restrictif et punitif incapable de la souplesse et de la flexibilité pourtant demandée par tous les partenaires sociaux. La conformité ne peut plus être perçue comme un concept manichéen, mais doit être l’enjeu d’un accompagnement continu vers l’excellence : pour cela, il faut une approche totalement nouvelle en matière de réglementation, mais quel gouvernement prendrait le risque de changer de posture vis-à-vis de ses propres fonctions régaliennes ? Par Adrien Aubert, Senior Manager du cabinet VERTUO conseil
27	http://www.vertuoconseil.com/portfolio/anacredit-lueur-despoir-banques/	 AnaCredit, une lueur d’espoir pour les banques ? Mathilde. Taillez 2017-04-19T18:43:57+00:00 Project Description Revue Banque &#8211; 24 février 2017 : L’Analytical Credit Dataset, ou AnaCredit, est un projet européen, piloté par la BCE en appui sur le Système Européen des Banques Centrales, qui vise à établir une base de données détaillées sur la distribution de crédit au sein de l’Union Européenne. Après la publication du règlement le 18 mai dernier cadrant les grands principes de la collecte, les institutions financières, principaux vecteurs d’alimentation de la base, ont reçu début novembre les premiers éléments méthodologiques du Manuel AnaCredit, comportant notamment les détails fonctionnels sur la détermination du périmètre éligible (typologies de clientèles et d’opérations). Encore une contrainte règlementaire ? Oui mais certains préfèrent souligner l’autonomie potentielle que ces données vont accorder à la BCE, donc une fin possible à l’escalade prudentielle entamée en 2007. Il ne faut pas s’y tromper. AnaCredit est en premier lieu un rapport règlementaire obligatoire pour les banques de taille systémique et établit de nouveaux standards. En termes de périmètre, le règlement se veut exhaustif des lignes de crédits octroyées, à l’exception des encours inférieurs à 25000€ et des prêts aux personnes physiques qui sont, pour le moment, exclus de la livraison. Si la centaine d’informations granulaires demandées ne dépaysera pas les équipes rodées à leur manipulation (notamment depuis les AQR 2014, un gigantesque audit mené par la BCE pour marquer sa prise de fonction par une évaluation des actifs bancaires), les modalités de livraison sont, elles, plus surprenantes. En effet, le règlement définit autant de jeux de données que de thématiques métier (par exemple, un jeu de données comptables ou un jeu de données relatives aux garanties reçues), et chaque jeu de données, en plus d’une remise trimestrielle, peut faire l’objet d’une demande ad hoc de la part du superviseur national (en France, il s’agit de l’Autorité de Contrôle Prudentiel   Résolution, émanation de la Banque de France). Une demande ad hoc consisterait par exemple à demander la fourniture des jeux de données 4 et 9 le 23 du mois sur une filiale d’affacturage, et non plus à un niveau consolidant le plus élevé sur un arrêté mensuel ou trimestriel. Ce fonctionnement fixe donc de nouvelles exigences en termes de flexibilité, tant du point de vue organisationnel que technique. Car de nombreuses équipes fournissent aujourd’hui un effort très conséquent une fois par trimestre sur une courte période pour produire une importante batterie de reportings réglementaires, avec des outils alimentés encore souvent alimentés une fois par mois ou même une seule fois par trimestre. Or demain, ces équipes pourront être sollicitées au jour le jour, comme lors des exercices Fire Drill menés par la BCE à l’automne dernier, et devront disposer dans leurs outils habituels de la vision Risque la plus fraîche possible, c’est-à-dire d’une photographie du portefeuille datant de la veille voire du jour même pour les opérations de marché. Encore plus de données, encore plus de flux, encore plus de volume d’informations, encore plus de contraintes sur les délais (le superviseur allemand cible une remise à J+6 en situation courante), il s’agit bien d’un nouveau défi technique à relever pour des banques en pleine mutation technologique, avec à la clef un nouvel enjeu de conformité, la production AnaCredit étant obligatoire dès le quatrième trimestre 2017. Pour ne laisser planer aucun doute sur le sujet, le règlement, à travers l’article 18, prévoit d’ores et déjà la possibilité de sanctionner, à la fois par la BCE mais également par le superviseur local, les établissements qui seraient dans l’incapacité de justifier le non-respect de leurs obligations de déclarations statistiques. Avec des amendes allant de 10 000 à 200 000€ selon la typologie d’anomalie (du simple retard de livraison jusqu’à l’obstruction dans l’audit des résultats par la BCE) relevée à chaque sollicitation, la BCE place la barre haute. Pas vraiment d’inquiétude cependant, car au-delà de la menace, les banques systémiques ont tout intérêt à jouer le jeu. Car pour elles, produire l’AnaCredit ne sera que le fruit de la profonde refonte de leur infrastructure entamée depuis quelques années, poussée par l’évolution des besoins de clients, les innovations technologiques ainsi que le renforcement des exigences réglementaires comme celles liées à la pratique des reportings risques (parmi lesquelles la circulaire BCBS239 figure en tête). Cette adaptation a conduit les banques à se doter de gigantesques hubs de données très hétérogènes, avec des définitions normalisées à l’échelle de leur groupe et donc partagées avec leurs filiales   succursales, des référentiels enfin administrés de façon pro-active, des plans de contrôles plus souvent pertinents. Mieux, le panel de données collectées, qui se chiffre par centaines, a été conçu de manière anticipative, de façon à adresser l’ensemble des futurs besoins réglementaires, tels que les normes IFRS9 ou l’AnaCredit aux échéances similaires. De plus, à travers l’enrichissement d’une super base de données relatives aux crédits et au risque de crédit, AnaCredit a vocation à faciliter le travail de la BCE en termes de production de statistiques économiques, d’analyses prudentielles et d’arbitrages sur les impacts des politiques monétaires notamment en termes de coût du crédit et de défaillances d’entreprises. Cette base de données, alimentée de façon industrielle et automatisée, offre surtout la possibilité à la BCE et aux Banques Centrales Nationales de mener leurs propres analyses à distance, sans systématiquement passer par de coûteuses missions d’inspection locales ou de nouveaux reportings complexes à exploiter. Cela n’enlève rien aux discussions quasi permanentes entre les autorités et les établissements supervisés, mais cela permet a minima de soulager ces derniers d’une charge de production chronophage et à faible valeur ajoutée. Et si 2017 sera l’année de l’AnaCredit « v0 », il se conçoit aisément, à la manière d’un logiciel ou d’applications pour smartphone, des versions ultérieures de l’AnaCredit. Une v1 puis une v2 et ainsi de suite pour étoffer le périmètre, le nombre et la typologie de données collectées : ce nouveau mode de fonctionnement permet aux autorités de glaner des informations très granulaires en lieu et place des reportings réglementaires qui fourmillent d’indicateurs très agrégés. De là à rêver de la fin de l’escalade prudentielle, il n’y a qu’un pas… et sans doute encore beaucoup de marge ! Par Adrien AUBERT, Senior Manager du cabinet VERTUO conseil
28	http://www.vertuoconseil.com/portfolio/les-normes-prudentielles-sont-elles-du-protectionnisme/	
29	http://www.vertuoconseil.com/portfolio/rd-retour-seminaire-de-restitution-12-juillet-2017/	
30	http://www.vertuoconseil.com/portfolio/blockchain-technologie-revolutionnaire-parle-t-on-impacts-a-long-terme/	 La blockchain, technologie révolutionnaire, mais parle-t-on des impacts à long terme ? Mathilde. Taillez 2017-06-14T05:35:35+00:00 Project Description Les Echos &#8211; 09 juin 2017 : La crypto-monnaie bitcoin a longuement été critiquée par les banques et les leaders d’entreprise lors de son apparition en 2009 avec la blockchain. Retour de situation, car aujourd’hui le protocole du bitcoin   chaîne de Bloc   intéresse… Effectivement même si la technologie blockchain est née avec la monnaie numérique, son utilisation va bien au-delà et ses champs d’exploitation sont gigantesques. Enjeux pour les banques ? Nombreuses sont les banques qui souhaitent implémenter cette technologie révolutionnaire. Selon une étude réalisée auprès de 200 banques mondiales par IBM, 65 % d entre elles prévoient de débuter un projet de mise en place de la plateforme commune avant 2020 et 15 % pour fin 2017. Depuis des années les banques d Investissements aussi bien que les banques de détail accentuent leurs plans d économie par des réductions de coûts qui s effectuent à travers la mutualisation, la baisse des effectifs, et les délocalisations visant notamment à compenser la hausse des coûts règlementaires et de transformation digitale. La mise en place d une plateforme blockchain pourrait donc être un moyen de répondre à ces objectifs et faire réaliser aux banques encore plus d économie ? Le secteur bancaire investit et n a pas fini d investir dans la technologie blockchain. La plateforme pourrait les aider à réduire leurs coûts d infrastructure et à réaliser des économies potentielles à hauteur de 10 milliards d euros par an. Nombreuses questions tournent autour de son intégration au sein des processus métiers : une automatisation des fonctions supports des banques à travers la plateforme avec pour but principal la diminution des cycles de décision et processus administratifs, l automatisation et l optimisation du traitement des données, un contrôle plus efficient, des règlements plus rapides, et l éventuelle suppression des tiers de confiance. La décentralisation d activités vers la blockchain pourrait donc retourner le secteur financier et les métiers supports en occuperaient le premier rang. En effet, les métiers opérationnels, métiers de la compliance, onboarding, chargés de reporting, Business Opération pourraient assister à une réduction majeure de leurs coûts d au moins 50 %, selon une étude menée par IBM. Ce qui laisse place à la question du risque de diminution des effectifs dû à une automatisation des tâches. Même si aujourd hui un grand nombre d acteurs affirment que celle-ci pourrait conduire à la création de nouveaux emplois avec un fort besoin de   spécialistes blockchain  , notamment dans le domaine de la sécurité, les métiers supports sont-ils à risque ? Le secteur va devoir s adapter à ce nouveau paradigme comme il l a toujours fait. Il y aura des restructurations, mais ce sera une opportunité offerte aux acteurs du secteur pour rester plus proche de leurs clients banque ; comme vient de l annoncer la Société Générale dans son plan de transformation en enrichissant la relation qu ils entretiennent avec leurs partenaires et collaborateurs pour apporter davantage de valeur aux clients dans l ensemble des métiers du groupe. Par Stéphanie GEHIN, Consultante Senior du Cabinet Vertuo Conseil
31	http://www.vertuoconseil.com/portfolio/2017-cru-de-maturite-prudentielle/	 2017, cru de la maturité prudentielle ? Mathilde. Taillez 2017-02-12T12:44:41+00:00 Project Description Les Echos – 4 janvier 2017 : La BCE avait dévoilé fin décembre son programme au titre de la supervision bancaire, un programme sans surprise dans la lignée de l’année écoulée, signe d’une certaine maturité dans son approche. Mais ce programme demeure d’autant plus chargé que l’actualité financière maintient l’état d’urgence pour au moins deux banques européennes majeures. Car si le coup de chaud de Deutsche Bank semble apaisé après la signature d’un accord avec les autorités américaines pour le rôle tenu dans la propagation de titres RMBS (des actifs immobiliers impliqués dans la crise des subprimes), d’autres dossiers semblent perdurer (en Russie et en Chine) et font planer une menace sur de nouvelles provisions exceptionnelles, alors même que demeure la question épineuse de la solvabilité fondamentale d’une banque qui n’a plus connu les profits depuis 2014. Cette situation tend à rendre la mobilisation de la BCE pour accompagner une sortie de crise d’autant plus remarquable que le jeu politique qui l’accompagne est complexe. Complexe, mais sans doute moins que le plan de quasi-résolution préventive mis en oeuvre pour l’établissement italien Monte dei Paschi di Siena. Avec pourtant un besoin en fonds propres évalué à près de 9Md EUR et des réserves de liquidité de court terme très amoindries ces dernières semaines, il semblait acquis que les mécanismes de résolution établis dans la directive BRRD allaient s’appliquer. Or pour l’heure, c’est principalement l’État italien qui va finalement s’engager en rachetant l’essentiel des obligations subordonnées, qui seront converties en actions puis à nouveau transformées en titres séniors non éligibles à la résolution (bail-in). L’État italien a beau jeu de contester sur la forme les sommes annoncées par la BCE, il n’en reste pas moins que sur le fond ce n’est que maintenant qu’il commence à passer à l’action pour redresser sa 4e banque. Il faut en effet garder en tête que le problème est plus large que la seule situation de MPS : c’est en réalité l’ensemble du secteur bancaire italien qui est en péril avec plus de 300Md EUR de créances dites non performantes. Et cet aspect, la BCE l’a bien en tête comme en témoigne le communiqué de presse annonçant ses priorités 2017 et son plan de supervision. Lucide sur les facteurs de risque qui fragilisent les économies et les organisations financières, au premier rang desquels figurent désormais un très haut niveau d’incertitude géopolitique combiné à un important risque de faille dans la cybersécurité, la BCE procèdera à 3 grandes revues des institutions. La première fait écho à la croissance atone des pays européens malgré une stratégie de taux bas et à la remise en cause de la croissance de certains pays émergents (Chine, Amérique du Sud, etc.) habituellement porteurs de débouchés pour les économies européennes. Les Joint Supervision Teams (JST) de la BCE auditeront le profil de rentabilité intrinsèque de chaque structure systémique et benchmarkeront le niveau d’exposition à la concurrence de nouveaux acteurs issus de l’économie digitale (services de paiement, finance participative, etc.) ou du shadow banking. Avec à la clef des préconisations très attendues sur la réorientation des business models. La seconde, comme l’an passé, consistera à analyser en détail la composition des portefeuilles bancaires et en particulier les risques de concentration sectorielle comme sur l’immobilier ou le naval, en difficulté. Ces études sont à coupler avec la prochaine entrée en vigueur des normes IFRS9 qui vont intensifier le provisionnement, particulièrement sensible au moment où le poids des créances douteuses grève les bilans bancaires. La pérennité de certaines activités pourrait entrer en jeu. La troisième enfin reste consacrée aux pratiques de gestion des risques et notamment en termes de gestion des données (conformité à la directive BCBS239) ou de maintenance des modèles internes à travers la Target Review of Internal Models. 3 orientations qui demeurent donc conformes à l’esprit des orientations 2016. Cette stabilité, si elle ne permet sans doute pas aux banques de souffler, mais a minima de peut-être ne pas mobiliser de ressources supplémentaires, sonne comme une reconnaissance de la pertinence du modèle actuel. Un modèle extrêmement intrusif, critiquable, coûteux, mais qui commence à faire ses preuves. Tel un bon vin bonifiant avec les années, la gouvernance exercée par la BCE semble parvenir à une certaine forme de maturité et le nouveau paradigme de la supervision est cette fois bien en place : la dimension répressive de l’actuelle réglementation, qui n’aura de cesse d’évoluer et de se renforcer, se complète désormais d’un véritable accompagnement sur le terrain par un superviseur mieux à même de concilier micro et macroéconomie. Alors comme on dit à Francfort en ce début d’année, bonne santé… financière ! Par Adrien Aubert, Senior Manager du cabinet VERTUO conseil
32	http://www.vertuoconseil.com/portfolio/marche-du-paiement-loffensive-des-nouveaux-acteurs/	
33	http://www.vertuoconseil.com/portfolio/trading-haute-frequence-cadence-decadence/	 Trading Haute Fréquence : cadence et décadence Mathilde. Taillez 2017-05-29T06:04:13+00:00 Project Description Eric Hunsader est un justicier des temps modernes : il passe son temps à traquer les cas de malversations sur les marchés financiers. Son ennemi préféré? Le trading haute fréquence (THF). En mars 2016, il s’est vu récompensé à auteur de 750 000$ par la SEC pour avoir mis en lumière un cas de fraude sur le New York Stock Exchange.   Comment en est-on arrivé à une situation où les autorités réglementaires ont besoins de lanceurs d’alerte comme Eric Hunsader ? Depuis les années 2000, la multiplication des bourses et des plateformes de trading ainsi que le passage à un système de cotation décimalisée ont rendus les marchés financiers rapides et efficients. Revers de la médaille, les transactions sont devenues plus opaques et de nouveaux prédateurs attirés par l’appât du gain sont alors apparus: les firmes de trading haute fréquence. Derrière ce terme se cache une technique où l’informatique et les algorithmes sont rois. Des serveurs surpuissants passent des milliers d’ordres en quelques millisecondes dans le but de profiter de la moindre anomalie de marché en un temps record. L’achat et la revente des titres se font en une fraction de seconde pour un gain réalisé souvent minime, mais qui, multiplié par le nombre d’opérations, devient très rentable. L’arrivée de ces nouveaux acteurs a bouleversé les marchés. Lorsqu’un trader classique détient un portefeuille basé sur ses propres analyses; le THF lui, ne se préoccupe pas de savoir dans quoi il investit. Le problème est, qu’aujourd’hui, cette technique se répand puisqu’elle représente plus de deux tiers du volume de transactions des marchés actions US et un tiers du volume de transactions échangées en France. Alors lorsqu’un de ces robots vient à dysfonctionner il peut vite semer la panique. L’exemple le plus marquant est celui du 6 mai 2010 où le Dow Jones plongea de 1 000 points &#8211; soit près de 10% &#8211; en moins de 20 minutes, avant de revenir à son cour normal. Un évènement qui pose la question de la gestion des risques engendrée par ces nouvelles technologies.   Sont-ils aujourd’hui bien encadrés par les autorités compétentes ? Alors que les sociétés de THF parlent aujourd’hui de breveter des algorithmes, que d’autres estiment qu’un ordre d’achat/vente passé en 13 millisecondes n’est pas assez rapide (rappelons qu’un clignement d’oeil dure 150 millisecondes), ou bien encore que l’on entreprenne déjà le passage de l’information par faisceaux lasers, les régulateurs eux restent passifs. On voit alors clairement deux mondes s’opposer : la lenteur législative face à des marchés financiers en constante évolution. Il semblerait que les instances directives aient du mal à opérer une distinction entre régulation (fait de contrôler un système existant stable) et règlementation (fait d’encadrer un système par des lois), et ne souhaitent pas aller réellement à l’encontre du Trading Haute Fréquence. L’exemple du projet du Loi de Finance 2017 rediscuté en chambre par les députés le 21 octobre dernier est un exemple flagrant : peut-on réellement penser qu’un régulateur souhaite réprimander le THF, alors que ce même régulateur élargit l’assiette de la Taxe sur les Transactions Financières aux opérations de trading haute fréquence et intra-journalières, afin d’affecter ces ressources nouvelles au budget 2017…   Et si les régulateurs ne prenaient pas le problème à sa source ? La régulation de ces nouveaux acteurs de marché passe donc par une bonne compréhension de cet environnement complexe et peu accessible. Les instances régulatrices se doivent par conséquent de mettre en place de nouvelles méthodes afin de contrôler et réguler cette activité si sophistiquée. D’autres solutions pourraient être envisagées par les régulateurs, comme: Un encadrement des sociétés de THF en imposant aux développeurs d’effectuer des tests sur leurs outils. En effet, eux seuls sont à même de comprendre et de détecter les potentielles anomalies du programme créé. A titre d’exemple, un rapport détaillé pourrait être demandé auprès de la société de THF mentionnant les risques et les corrections d’anomalies effectuées sur un algorithme durant une période donnée. Des contrôles périodiques sur la chaine de traitement de l’algorithme THF (création, évolution et surveillance) permettant ainsi d’éviter les situations d’autogestion, notamment dans les banques. Dans cette volonté de trouver des solutions pour encadrer le THF, la directive européenne MIFID 2 et ses évolutions sont sur le point d’être mises en oeuvre pour 2017. Le texte prévoit notamment des dispositions concrètes comme le gel instantané des marchés électroniques en cas d’emballement d’un algorithme de trading ou la divulgation systématique des informations liées aux codes source de celui-ci. Malgré cela, les évolutions de cette directive demeurent lentes et complexes à implémenter. Dans le cas de MIFID 2, la France a élargi la définition du Trading à Haute Fréquence, qui repousse l’échéance de mise en application de ces nouvelles dispositions. De plus, MIFID 2 ne règle pas le problème du lobbying financier orchestré par les grands utilisateurs du THF tels que les banques, qui feront sans nul doute pression sur les instances régulatrices pour divulguer le moins d’informations possibles. Entre projets de loi et conflits d’intérêts, le Trading Haute Fréquence est, et restera, un sujet sensible pour le monde de la finance et difficile à encadrer par les régulateurs. Apparaissent alors des solutions alternatives comme le ralentissement des ordres, déjà mis en place en août dernier lors de l’inauguration de la plateforme boursière IEX (Investors Exchange) qui bannit le THF ; ou encore des solutions palliatives comme le trading en temps discret, technique étudiée de près par les acteurs du marché … Par Marie GUIGOUT, Alexandre GEHIN et Steven COUSIN, Consultants du cabinet VERTUO Conseil
34	http://www.vertuoconseil.com/portfolio/gestion-risques-coeur-de-transformation-bancaire/	 La gestion des risques au cœur de la transformation bancaire Mathilde. Taillez 2017-01-24T20:18:14+00:00 Project Description Depuis quelques années, on observe un étrange phénomène dans l’univers bancaire, une dualité entre une règlementation structurante (IFRS, normes liquidités, EMIR, ….) qui vise à orienter le business model bancaire et un marché des moyens de paiement digitaux en pleine expansion, qui commence à lorgner sur le monde de la finance. Au cœur de ce combat de David contre Goliath, la gestion des risques, devant composer avec une approche cartésienne de la régulation et innovante de la part de nouveaux acteurs comme les Fintech ou les GAFA qui révolutionnent le secteur par une approche « client-driven ».   David, l’outsider aux grandes ambitions ? Que dire des fintech ? Si l’on en croit la définition de Wikipédia, les Fintech, ou technologie financière sont généralement « des startups qui maitrisent bien les technologies de l’information et de la communication, qui tentent de voler des parts de marché à des grosses entreprises en place qui sont souvent peu innovantes et qui sont en retard sans l’adoption des nouvelles technologies ». Cette définition colle finalement bien avec celle d’une banque : pour ceux qui ont l’opportunité de travailler dans une institution financière, on se retrouve souvent étonné du niveau d’obsolescence des systèmes d’informations, peinant à traiter une dizaine de millions de flux par jour dans un monde dicté par le big-data et l’afflux de données. A l’heure des tablettes et des casques de réalité virtuelle, nous exploitons encore l’information de manière sédentaire, vissés sur une chaise de bureau, dépassant difficilement les 20 giga de stockage sur notre poste de travail ! L’avènement des GAFA est également un grand chamboulement sur l’échiquier bancaire. Google ou Apple proposent des outils permettant de concurrencer les banques sur un des aspects dont elles sont les garantes depuis la nuit des temps : le financement et la gestion dépositaire. Et quelles sont les caractéristiques de ces structures ? Des capitaux énormes pour commencer : les 4 acteurs américains totalisent 123 milliards de dollars de réserves financières, de quoi racheter la quasi-totalité des fintechs du marché ! Elles disposent d’une hégémonie sur tous les secteurs clefs de la transformation numérique, ce qui leur permet d’adresser tout type de clientèle. Difficile pour les banques de rivaliser : qu’est ce qui empêche Google de se mettre au financement structuré et de proposer des prêts via un financement participatif à disposition de ses clients en matière d’énergie ou de télécom ? Plus important encore, ces acteurs connaissent leur clientèle via une énorme base de données contenant les habitudes de leurs utilisateurs, ciblant de manière chirurgicale les produits proposés. Les banques, elles, disposent d’informations très fragmentaires sur leurs clients et ne vont mettre à disposition que des produits « standards» faisant partie de leur business plan. Or, l’information sur la clientèle est cruciale pour la gestion des risques afin d’adresser de grands chantiers réglementaires comme le respect des ratios financiers (liquidité, solvabilité), dont les indicateurs de performance au passif s’attachent à la relation que peut avoir le client avec la banque pour catégoriser les dépôts. Les GAFA partent avec un train d’avance : elles définissent les besoins de leurs utilisateurs grâce à des profils très personnalisés, alors que nous cherchons encore une source de données « Golden » sur les contreparties et les titres dans les banques avec le chantier BCBS239. Pour les GAFA comme pour les banques avec l’octroi grandissant de services en ligne, le centre de création de valeur s’est déplacé de l’institution vers le consommateur et transforme les clients en autoentrepreneurs, pour le meilleur et le pire : Le meilleur c’est la possibilité de proposer des services à distance et sur mesure, 24h/24h, avec des frais de gestion limités ou inexistants. Le pire, c’est le transfert du risque vers le consommateur et une externalisation du métier bancaire Et c’est cette notion de transfert de risque qui sera l’un des grands challenges des institutions financières pour les prochaines années.   Goliath, le mastodonte règlementaire financier L’avènement de ces acteurs pose donc des questions d’ordre sociétal, avec la place du consommateur dans le rouage financier, mais également sur la gestion des risques. Le transfert d’argent par paiement sur mobile ainsi que le financement participatif accentuent la désintermédiation bancaire. Or, les banques ont besoin de ce système pour survivre : le passif de bilan est déjà suffisamment mis à mal par la nécessité de conserver un certain nombre d’actifs liquides en fonds propres. Cette logique de passif long devrait s’accentuer dans les prochaines années avec la venue du NSFR et son modèle de stabilité du profil de financement des banques, allant à l’encontre du modèle bancaire actuel qui consiste à collecter des dépôts clientèles à court-terme afin de proposer des financements à long-terme aux entreprises. D’autres réglementations, comme IFRS 9, sont également impactantes pour une institution financière, avec la revue de tous les états financiers présents au bilan bancaire et la comptabilisation des éléments en juste valeur de marché. Cette revue du système de provisionnement, jugée trop tardive et trop faible, augmentera encore le passif des banques, obligeant celles-ci à garder plus de ressources au bilan et de réduire d’autant plus leurs marges malmenées par un contexte prolongé de taux bas permis la politique de « Quantitative Easing » de la BCE (programme massif d’émission de dettes). Cette pression sur le passif bancaire, les banques y font pour l’instant face grâce à la présence encore forte des dépôts clientèle au passif et l’activité de prêt et de financement à l’actif. On entend souvent parler dans les institutions financières de business model « drivé » par le régulateur, car le cadre réglementaire dicte dernièrement les produits qui entrent ou non dans un bilan bancaire mais nous nous trompons peut-être de débat : et si au final c’était le consommateur qui avait le dernier mot dans cette histoire ? Cela serait une belle revanche sur certaines dérives spéculatives et règlementaires du monde bancaire qui ont jalonné ces dernières années pour lesquelles il n’a été qu’un spectateur impuissant. Par Julien Delrieu, Consultant Senior du cabinet VERTUO Conseil
35	http://www.vertuoconseil.com/portfolio/marche-de-cyber-assurance-risque-assureurs/	 Le marché de la cyber-assurance : risqué pour les assureurs ? Mathilde. Taillez 2017-04-10T21:31:31+00:00 Project Description Les Echos &#8211; 28 mars 2017 : Le cyber-risque est devenu central pour nos économies, nos industries et nos services. En 2015, le nombre de cyberattaques recensées a progressé de 38 % dans le monde, et de 51 % en France. En parallèle de ce phénomène, les budgets de cyber-sécurité des entreprises ont significativement augmenté en 2015 : le marché de la cyber sécurité représente aujourd hui 77 milliards de dollars et devrait atteindre entre 130 et 170 milliards de dollars d ici 2020. Néanmoins, les résultats de la dernière étude du Lloyds sont édifiants : 50 % des dirigeants européens d entreprises de plus de 250 M$ CA ne connaissant pas les solutions proposées par l assurance contre les cyberattaques et 92 % des entreprises européennes étudiées mentionnent avoir été victimes d une cyber-intrusion. Pour autant, seulement 42 % des entreprises estiment qu elles pourraient de nouveau être victimes de telles attaques. Cette étude révèle donc que la demande en cyber assurance reste limitée, malgré son haut potentiel de croissance. Ce faible de taux de souscription aux produits de cyber assurance est induit par 2 causes : d une part à la sous-évaluation par les entreprises de leurs cyber risques, et d autre part, par une offre de la part des assureurs qui n est pas attrayante ni même innovante. Historiquement, les industries sont celles qui sont les plus sensibilisées, notamment à l espionnage industriel et au risque d intrusion. Mais le constat actuel montre que tous les secteurs sont exposés : industrie &#8211; une aciérie en Allemagne a vu ses systèmes de contrôle piratés informatiquement via un simple mail, ses fourneaux n ont pas pu être arrêtés sociétés de service &#8211; piratage de la page Twitter et Facebook de TV5 Monde secteur de la santé &#8211; le groupe pharmaceutique américain Johnson   Johnson a mis en garde, mardi 4 octobre, les utilisateurs canadiens et américains d une de ses pompes à insuline contre d éventuels piratages informatiques transports &#8211; une équipe de chercheurs chinois a réussi à prendre le contrôle d une berline Tesla à 19 km de distance par une simple intrusion informatique La sous-évaluation des cyber-risques par les dirigeants des entreprises relaie ces attaques au rang de   potentielles  , voire   rares   en termes de probabilité d occurrence. Pourtant, la survie des entreprises peut être mise en cause en cas d attaque. Faut-il attendre qu une attaque survienne pour mettre en place le dispositif adéquat ? Afin de mieux appréhender ces risques, les assureurs devraient s engager de façon plus formelle dans cette lutte. Force est de constater que l offre en produits de cyber assurance n a pas atteint une maturité optimale, et ne bénéficient que de très peu de notoriété. De ce fait, les assureurs doivent appréhender le cyber risque comme un risque systémique de grande envergure. À la différence du risque terroriste ou catastrophe naturelle (inondations par exemple), qui sont maitrisés géographiquement, le risque cyber n a pas de limites géographiques. Les produits proposés par les assureurs ne répondent pas, à l heure actuelle, aux besoins des clients, et ne couvrent pas leurs risques. Cela s explique notamment par le fait que ce risque dépasse très largement les montants généralement couverts par les grosses entreprises &#8211; 500 millions de $- et il est trop important pour que les bilans des assureurs puissent l absorber. La difficulté de tarifer ces offres de cyber assurance est due à plusieurs facteurs : pas d historique ni de recul suffisant sur les sinistres et les impacts financiers, frilosité des entreprises à communiquer sur l impact des cyberattaques subies, et évolution constante et rapide des NTIC. Une veille permanente est donc nécessaire afin d appréhender au mieux ces risques. Par ailleurs, les conséquences d un cyber risque peut varier d une entreprise à l autre, d un système à l autre. Ainsi, la modélisation du risque ne peut bénéficier d un modèle unique. Pour tenter de contourner ces obstacles, deux solutions pourraient être envisagées : développer le marché de la réassurance et se rapprocher des pouvoirs publics. Le fait de transférer le risque aux réassureurs permettra aux assureurs principaux de mieux maitriser l impact d une cyber attaque et de partager les coûts éventuels. À l instar de Swiss Re (réassureur allemand), qui a annoncé le 1er septembre la mise en service sur le marché allemand de solutions adaptées au cyber risque, les principaux réassureurs de la place devraient se positionner sur des solutions préventives, avec des solutions de formations spécifiques, l accompagnement à des stress tests réguliers et des offres mieux adaptées aux entreprises. Par ailleurs, un rapprochement avec les pouvoirs publics pourrait permettre d aboutir à une solution. Un soutien technique de la part des gouvernements pourrait permettre aux assureurs d assurer une couverture plus importante. À ce titre, le dernier rapport d activité de l ANSSI fait état d une relation solide et stratégique avec les pouvoirs publics, pour répondre aux enjeux des cyber-risques. Ainsi, la stratégie conjointe pour la sécurité du numérique offre un cadre au développement de la cyber sécurité. Le cadre règlementaire doit être renforcé, au gré des évolutions technologiques, et les assureurs doivent être rassurés à leur tour quant à leurs engagements financiers. Il s agit donc d anticiper ces changements numériques, afin d offrir un environnement sécurisé, aux différents acteurs économiques. Ce renforcement du cadre règlementaire doit notamment prendre en compte les contraintes propres à chaque secteur d activité. La réglementation joue un rôle central dans la prévention et la connaissance du cyber-risque. Les nouvelles exigences en termes de protection, les règles renforcées pour l administration des systèmes, l extension du pouvoir de l État en matière de contrôle et de sanctions, et les obligations de notification d incidents, devraient également contribuer au développement de l assurance. Actuellement, les opérateurs d importance vitale (OIV) sont les principaux concernés par ces mesures. L extension de l application de ces règles au niveau national ou international pourrait aider à étendre l offre de cyber assurance. A cet effet, la directive Network   Information Security (NIS), adoptée le 6 juillet 2016 par le Parlement Européen, permettra aux différents acteurs locaux de lutte contre le cyber risque d étendre leurs actions de prévention, de sensibilisation et d anticipation. La cyber assurance représente aujourd hui un marché stratégique pour les assureurs. Pour sensibiliser les dirigeants sur la nécessité de souscrire à un produit de cyber assurance, le rapprochement entre assureurs et cyber sécurité est préconisé, comme cela a été fait par Axa et Airbus, ou encore Allianz et Thales. Cela permet de proposer deux volets aux assurés : un contrat d assurance couvrant les dommages subis, et un accompagnement sur mesure en ingénierie. Les produits   clés en main   ou   all in one   pourraient donc être la clé de voute pour les assureurs, et représenter un levier de croissance des produits de cyber assurance. Par Hind AQALLAL, Project Manager du cabinet VERTUO Conseil
36	http://www.vertuoconseil.com/portfolio/lapproche-agile-au-service-des-projets-complexes/	 L’approche « Agile » au service des projets complexes Mathilde. Taillez 2017-01-24T20:21:15+00:00 Project Description Comment définir un projet complexe ? Demander à un chef de projet si son projet est complexe ; il vous répondra « Oui » car les difficultés rencontrées pour mener à bien un projet sont nombreuses et l’équilibre temps/coût/qualité tant convoité est rarement réalisable. Pour autant, il convient de différencier le projet compliqué du projet complexe. Un projet « compliqué » fait référence à une situation où plusieurs éléments simples sont combinés. Il est nécessaire de pouvoir identifier les composants du projet pour comprendre la situation et ainsi être en mesure de lever ou surmonter les obstacles. Un projet « complexe » désigne en revanche un ensemble d’éléments dont un ou plusieurs ne sont pas maîtrisés. La situation est alors difficile à appréhender, à analyser et donc à restituer. La perception du projet est alors subjective et le partage de l’analyse n’en est que plus difficile. Par exemple, un projet impliquant de multiples corrections informatiques nécessite une attention particulière dans le suivi des évolutions avant la livraison de la nouvelle version du logiciel. En présence d’un planning strict et avec des contributeurs répartis sur plusieurs sites, ce projet est certainement compliqué, mais pas obligatoirement complexe. En effet, par un effort de suivi et de coordination des contributeurs et des équipes informatiques, le projet a de grandes chances d’être mené à bien. Cependant, la réalité du projet peut comporter un caractère plus « humain » avec des managers opposés à ce projet et/ou une équipe informatique qui ne se sent pas concernée par l’urgence du sujet. L’effort d’accompagnement et de suivi va alors être plus « complexe » et difficile à gérer pour le Chef de projet ou tout autre intervenant sur le sujet. Matrice complexité / taille projet | D’après le rapport CHAOS du « Standish Group », la taille et la complexité d’un projet ont des conséquences sur sa réussite. Ce qui définit un projet « complexe » est la combinaison de 2 facteurs. Le facteur humain : il sera plus ou moins impactant en fonction du nombre d’acteurs sur le projet, de leur localisation (dans un même pays ou dans plusieurs), de leur niveau d’intervention mais aussi des intérêts qu’ils portent au projet. Il est facilement concevable qu’il soit plus difficile de coordonner les contributions de chacun, de réaliser les reportings associés, de suivre les charges et surtout d’anticiper les réactions en fonction du nombre de contributeurs sur le projet, des attentes qu’ils ont du projet, etc. Le facteur technique : les technologies utilisées dans le projet ne sont pas figées. Il est alors indispensable de s’adapter et accepter que le projet puisse évoluer au fur et à mesure de son avancée (périmètre et/ou budget et/ou planning) Ce sont ces deux facteurs combinés qui rendent un projet « complexe », instable et donc difficilement prévisible. Le niveau d’incertitude est l’un des deux principaux éléments de différenciation des projets. Selon la matrice de Stacey, le degré de complexité est fonction du niveau d’incertitude et d’accord face à la situation. Quatre états sont identifiés dont un est nommé « complexe ». Il représente la situation où il y a plus d’éléments inconnus que d’éléments connus. Nous pouvons faire le rapprochement avec les projets « complexes » où les besoins et la solution, par exemples, sont plus ou moins identifiés. Matrice de Ralph Stacey La matrice présente aussi l’état dit « simple » où tous les éléments sont connus, l’état « compliqué » où il y a plus d’éléments connus que d’éléments inconnus et l’état « chaotique » où très peu d’éléments sont connus. Les solutions à apporter et la démarche de gestion du projet doivent s’adapter selon ces différents états. On peut d’ailleurs trouver dans les méthodes dites « Agile » de bonnes pistes au management de projets complexes comme la méthode « Scrum ». Elle vise à gérer la complexité à partir de courtes périodes itératives nommées « sprint » qui s’appuient sur une replanification opérationnelle régulière. Les facteurs clé de succès C’est en effet cette part d’imprévisible, d’incertitude, que l’on tente de diminuer lorsque l’on met en place une méthodologie projet ou que l’on adapte son organisation. L’approche « agile » pour accepter les changements La gestion même du projet peut sembler se complexifier au regard des ajustements continuels entre la corde temporelle et celle budgétaire. Les risques à piloter se multiplient, du fait même de cette gestion, et alimentent les difficultés du chef de projet. C’est dans ce cas précis que la philosophie « agile » prend tout son sens. Elle permet d’ajuster au fil de l’eau le périmètre à livrer en s’appuyant sur un équilibre entre valeur métier, qualité et contrainte en lieu et place du triangle traditionnel coût, périmètre et planning. C’est une approche où le changement et les adaptations sont intrinsèques à l’organisation. Ils sont donc acceptés et mis en œuvre plus facilement par les acteurs du projet, qui les considèrent comme une opportunité d’accroître la valeur du livrable Dans le rapport CHAOS, publié en 2015 par le « Standish Group » (rapport réalisé à partir de 50 000 projets de développement logiciel à travers le monde.), celui-ci confirme que le taux de réussite des projets « agiles » atteint 39% (toutes tailles de projets confondues) contre 11% de réussite pour les projets en méthode « classique ». En effet, le travail collaboratif et les itérations réalisées tout au long du projet permettent de rectifier les erreurs au plus tôt dans le projet et plus régulièrement que dans le cadre d’une gestion de projet « classique ». L’expertise comme savoir-faire indispensable L’expertise de chaque intervenant du projet est un des éléments essentiels et nécessaires pour la réussite d’un projet « complexe ». Effectivement, un Chef de projet certifié PMP, des intervenants experts dans chacun de leur domaine (maîtrise d’ouvrage, maîtrise d’œuvre, accompagnement au changement) permet de sécuriser le projet. Les organisations comportant plus d’un tiers de chef de projet certifiés PMP achèvent ainsi plus de projets selon les délais, le budget et les buts définis (Pulse of the Profession® study, PMI, 2015). Simplifier l’organisation : une nécessité En réduisant les niveaux hiérarchiques et le nombre d’équipes, les échanges entre les intervenants sont facilités. Ils ont ainsi les moyens de faire changer les choses plus facilement. Mais cela ne suffit pas, il faut également éliminer les barrières qui séparent les acteurs d’un projet. Les collaborateurs ont ainsi une meilleure visibilité des tâches des autres; ce qui permet de comprendre les contraintes de chacun. Dans ce contexte, il est aussi plus facile de comprendre sa propre valeur ajoutée. En se sentant utiles et intégrés au projet, la coordination en est renforcée et l’implication de tous devient un facteur clé pour l’utilisation optimale des compétences et de l’énergie de chacun. Le cas assez atypique de la Morning Star, entreprise californienne de transformation de tomate décline la réduction de la hiérarchie à l’extrême. Elle ne comporte aucun manager ! Chaque « associé » (salarié) gère son projet, est responsable de son périmètre et de ses décisions. La clé : des périmètres bien définis et des discussions facilitées. Le résultat ? Morning star détient 40% du marché de la première transformation de tomates aux Etats Unis. Faciliter les interactions entre les individus Un projet peut être comparé à un orchestre c’est le chef d’orchestre qui créé des connexions et permet de créer une harmonie entre les musiciens. Quelle est l’utilité d’un violon s’il ne joue pas à l’unisson avec les autres instruments? Il en va de même pour un chef de projet isolé qui perdrait en efficacité, se désengagerait et ne serait plus impliqué dans la réussite du projet. En revanche en créant des synergies entre les protagonistes, les ressources à disposition peuvent être exploitées dans leur intégralité. Nous pouvons faire le lien avec l’approche « agile » d’un projet qui valorise en effet les interactions plutôt que les processus et les outils. La collaboration est un principe clé et facilité si la coopération entre les différents contributeurs est partagée. L’implication de toute l’équipe projet Au-delà du rôle du chef de projet, il est indispensable que les managers se sentent concernés et soient impliqués dans le projet. En effet, la position centrale du manager dans l’organisation et sa connaissance des problématiques du terrain en font un interlocuteur privilégié, un ambassadeur, sur lequel l’équipe projet peut s’appuyer afin de mobiliser et faire adhérer les populations impactées. Il faut donc responsabiliser les managers afin qu’ils prennent la mesure de leur rôle dans la réussite du projet ; au travers des actions de communication et d’information. En plus des managers, tous les contributeurs du projet doivent se mobiliser car c’est l’ensemble des forces de chacun qui permettra l’atteinte de l’objectif escompté. Cette mobilisation passe par la récompense des bonnes actions, des coopérations et collaborations constructives. C’est grâce à ces interventions que les projets peuvent être menés à bien. Les contributeurs allant à l’encontre de ce principe, à l’inverse, ne doivent pas être récompensés ; pour les raisons qui en font des freins à la réussite du projet. Seul le manque d’implication doit être souligné et sanctionné, et non pas l’échec des personnes motivées participant au projet. Ainsi, chacun est impliqué et conscient de sa responsabilité dans la réussite ou l’échec du projet. En définitive, c’est le niveau de complexité du projet qui permet d’estimer si la gestion en mode « agile » du projet est adaptée ou non, notamment au travers du cadrage du projet qui demeure essentiel pour en évaluer sa complexité. La complexité d’un projet étant en perpétuelle évolution, la méthode Agile nous apparaît comme une solution pour réduire l’impact de l’incertitude. Cette philosophie valorise en effet le principe de l’ignorance découlant de l’incertitude. L’expérimentation est ainsi une clé dans la création de valeur, l’essai peut entraîner l’échec, mais celui-ci est permis afin de s’améliorer face à la complexité. L’expertise métier de l’équipe projet permet par ailleurs d’anticiper ou de faire face à cette éventuelle complexité. A travers une organisation simplifiée favorisant la communication et les interactions des différents participants du projet, cette expertise peut être pleinement exploitée. Nous estimons par ailleurs que la méthodologie n’est pas suffisante. Bien qu’importante, elle doit être associée à un système de management adapté permettant aux contributeurs projet, aux collaborateurs et aux managers d’être motivés, intéressés et volontaires. Par Emilie Lopez et Thibaut Le Garrec consultants du cabinet VERTUO conseil
37	http://www.vertuoconseil.com/portfolio/royaume-uni-lalea-souverain/	 Royaume-Uni : l’aléa souverain VERTUO 2017-02-12T13:09:02+00:00 Project Description Les Echos – 30 juin 2016 : Tempête boursière, turbulences politiques et incertitudes sont le lot du récent « Brexit ». La Reine et la Patrie sont sauves, mais isolées. Le prix de cette indépendance est incertain et impose au Gouvernement britannique de reconsidérer les tenants et aboutissants de sa souveraineté économique. Albion, l’orgueilleuse L’Europe est un projet global d’association d’États, dont l’objet n’a jamais été d’être une seule zone de libre-échange. Ce n’est ni l’esprit ni la lettre des traités. Le projet européen n’avait pas le même sens de chaque côté de la Manche. Le « Brexit » clarifie les trajectoires. À bien des égards, il apparaît comme un piège épineux pour le Gouvernement britannique. Avant le vote, la souveraineté économique britannique a été discutée à Bruxelles en position de force : le nationalisme persévérant du UKIP a aidé David Cameron à alimenter la peur européenne de perdre une grande économie. Mais, la réalisation du « Brexit » est un renversement stratégique irréversible : l’intérêt politique et économique de l’Union européenne est de proscrire l’effet domino et de démarrer rapidement la phase de transition de deux ans. Londres n’emmène aucun autre pays dans sa sortie. C’est une aventure politique et économique solitaire, qui place les Britanniques en position de faiblesse face à des Européens pressés d’agir avant les échéances électorales de 2017 en France et en Allemagne. Cet isolement politique est le premier coût de ce « Brexit ». Albion, l’esseulée L’Union européenne propose un modèle économique qui se fonde sur des abandons de souveraineté étatiques au bénéfice d’une souveraineté économique collective. Doté d’un marché unique et partiellement d’une monnaie unique, le dispositif assure le fonctionnement de l’économie de ses membres. L’Union européenne est un succès économique, certes contestée dans le contexte des crises actuelles, mais il demeure que structurellement ce modèle assure une sécurité économique aux entreprises, aux consommateurs et aux États membres. Cette sécurité est perdue pour le Royaume-Uni. Dans l’incertitude actuelle les Britanniques seraient bien inspirés de sécuriser leurs échanges, à la fois avec les non-membres de l’Union européenne et avec les États-Unis. Réussir ce double mouvement diplomatique assurerait une position plus forte dans les négociations à venir avec Bruxelles. Mais, ce rééquilibrage stratégique nécessiterait l’aval de Washington et indirectement de Bruxelles. En effet, le grand projet diplomatique du moment est celui du marché transatlantique (Transatlantic Trade and Investment Partnership – TTIP), qui repose sur le marché unique européen. Une exception britannique s’impose de facto dans cette négociation complexe et contestée, dont l’échec n’est pas à exclure. Les options diplomatiques sont floues pour Londres et l’isolement économique menace Sa Majesté. Albion, l’imprudente Les résultats du scrutin sont clairs : Angleterre et Pays de Galles ont choisi le « Brexit », tandis que l’Écosse et l’Irlande du Nord ont préféré le « Bremain ». À peine sortie d’un referendum d’indépendance, les Écossais ont matière à réflexion : choisir l’Union Jack ne signifiait pas rejeter l’Union européenne. La géopolitique a changé et un royaume désuni menace Sa Majesté. L’autre risque qui émerge est celui de la perte d’influence dans les services financiers. Le sujet est sensible pour une économie aussi financiarisée que celle du Royaume-Uni. La City of London ne sera plus la capitale financière de l’Europe. Francfort, Luxembourg et Paris sont en embuscade. Cet enjeu de sécurité économique n’a pas été perçu par les électeurs. Une imprudence. Considéré comme un pays tiers, le Royaume-Uni ne pourra plus bénéficier de l’accès aux marchés financiers comme les autres États membres. Les relocalisations sur le continent s’annoncent inévitables. Le malheur des uns fera le bonheur des autres… Par Pierre Theobald, Consultant Senior chez Vertuo Conseil
38	http://www.vertuoconseil.com/portfolio/trim-2016-les-modeles-bancaires-a-nouveau-sous-les-projecteurs/	 Trim 2016 : les modèles bancaires à nouveau sous les projecteurs VERTUO 2017-02-12T13:01:57+00:00 Project Description Les Echos : 23 août 2016 : Et c’est parti pour une nouvelle consultation de place ! Hasard des acronymes anglais, cette nouvelle campagne s’appelle Trim. Comme une prémonition de la charge supplémentaire de travail pesant sur les équipes des institutions financières, cette Target Review of Internal Models doit permettre à la BCE de comprendre en détail les rouages techniques et organisationnels des modèles internes, ces fameux outils statistiques censés évaluer le profil de risque des contreparties et à l’origine de nombreuses critiques lors de l’explosion de la bulle de 2008. Dans la foulée des Asset Quality Reviews de 2013-2014, un audit géant des portefeuilles des principaux établissements bancaires européens, la BCE poursuit son long chemin de montée en compétence sur son périmètre de supervision. Cette fois, elle porte son attention sur le coeur des méthodes de calcul du capital règlementaire, capital qui constitue le socle de la solvabilité d’une banque. Un exercice pas totalement innocent puisqu’une autre consultation vient juste de prendre fin… elle portait précisément sur une proposition de règlement relatif à l’interdiction des modèles internes sur de nombreux périmètres, ou à l’ajout de nombreuses contraintes sur leur mise en place et leur utilisation ! En réalité, cet exercice de place n’est pas une surprise : la BCE, lors de ses voeux de début d’année, avait d’ores et déjà annoncé, dans son programme annuel, son intention de se pencher sur ces problématiques. Et c’est au moyen d’un double questionnaire qu’elle compte procéder. Le premier est qualitatif : il doit expliciter les modalités organisationnelles instaurées par les banques pour régir l’utilisation desdits modèles. Gouvernance, principes méthodologiques et normatifs, audit interne, typologie des utilisations, évaluation de la qualité des données, gestion du défaut règlementaire, tout y passe ! Le second est quantitatif : sans doute redondant avec les reportings règlementaires actuels (type COREP) et avec la dernière campagne de benchmark des actifs pondérés (la mesure règlementaire du risque de crédit) du second trimestre 2016, il doit apporter une décomposition des portefeuilles avec une granularité fine sous l’angle de la notation. Sur la forme, il faut à nouveau saluer la démarche de la BCE qui, bien qu’elle ne soit pas optionnelle et fasse porter une charge significative de travail supplémentaire sur des équipes fragilisées par les congés d’été, a instauré une approche pédagogique. Le niveau de communication avec les entités supervisées n’a jamais été aussi élevé ; le niveau d’exigence est devenu tel qu’il serait sans doute inatteignable sans une bonne circulation de l’information et la disponibilité des équipes de la BCE pour répondre aux questions pratiques des équipes mobilisées. Sur le fond, il reste, aujourd’hui encore, des questions en suspens autour de l’exploitation réelle par la BCE de ces questionnaires et des conclusions qu’elle pourrait en tirer. Des esprits chagrins pourraient même qualifier l’exercice de superficiel, car, quelles que soient les informations remontées dans les questionnaires, il semble d’ores et déjà compromis de réussir à faire dévier les autorités du chemin sur lequel elles se sont engagées, un chemin qui mène vers une limitation assez drastique de l’utilisation des modèles internes, remettant ainsi en cause des années de pratique bancaire et d’investissements (en compétences, en technologie informatique, etc.). Les observateurs ne manquent pas de souligner que cette évolution ne va pas dans le sens de l’histoire, qui s’est déjà engagée sur la voie de la robotisation du conseil financier, de l’exploitation massive de données hétérogènes (Big data), entre autres exemples ayant notamment permis ou facilité l’émergence de nouveaux acteurs. En l’espace d’un an, les autorités prudentielles ont multiplié les consultations : révision de l’approche standard et de l’approche avancée de mesure du risque de crédit, révision de l’évaluation des risques opérationnels, révision de la mesure du risque de marché et de contrepartie, révision de la définition du défaut, suppression des options nationales, benchmark et études d’impacts en tout genre, etc. Toutes ces composantes convergent pour former la confirmation qu’un nouveau cadre règlementaire s’appliquera aux établissements bancaires d’ici la fin de la décennie. Peu importe qu’on l’appelle Bâle IV ou la fin de la mise en oeuvre de Bâle III, les banques européennes ne connaîtront pas de repos. Entre les incertitudes liées à la mise en oeuvre du Brexit, celles du contexte prudentiel, ou de la politique monétaire, pas d’autres choix que de naviguer à vue ; voilà qui doit être bien inconfortable au moment où les enjeux technologiques accélèrent la transformation des usages bancaires… Par Adrien Aubert, Senior Manager du cabinet VERTUO conseil
39	http://www.vertuoconseil.com/portfolio/teaser-w-e-square-aux-canaries/	
40	http://www.vertuoconseil.com/portfolio/rachat-de-linkedin-quelle-strategie-pour-microsoft/	 Rachat de LinkedIn : quelle stratégie pour Microsoft ? VERTUO 2017-02-12T13:12:06+00:00 Project Description Le Journal du Net – 13 juin 2016 : L’acquisition de LinkedIn, c’est un pas de plus dans la stratégie d’implantation de Microsoft en entreprises. Ce rachat est amené à étoffer l’offre de CRM du géant IT américain, et combler le vide de la dimension sociale et collaborative d’Office. 26,1 milliards de dollars : c’est le montant du rachat surprise LinkedIn par Microsoft. Surprise ? Pas totalement : ce n’est pas la première fois que le géant sort le chéquier. Souvenons-nous de l’acquisition de Skype en 2011 pour 8,5 milliards de dollars, plus récemment en 2014 de celle de la société Mojang (l’éditeur du jeu de briques Minecraft) pour 2,5 milliards de dollars, et enfin le retentissant rachat de la division mobile de Nokia en 2013 pour 5,5 milliards de dollars. LinkedIn est l’une des premières pépites de la Silicon Valley : fondé en 2003, le service se targue, à fin 2015, d’être utilisé par plus de 400 millions de membres dans le monde, dont 125 aux États-Unis, avec un CA a 3 milliards de dollars, ce qui en fait le premier réseau social d’entreprise dans le monde. Seul l’irréductible français Viadeo lui fait de l’ombre, mais soyons honnête, en France seulement. Cette croissance, LinkedIn la doit à ses investissements pour rendre son modèle économique viable, en monétisant l’ensemble de ses services: offres d’emplois (Talent Solutions), contenus sponsorisés (Marketing Solutions) et abonnements LinkedIn Premium. Finalement, tout va bien pour LinkedIn ? Pour les investisseurs, la réponse est « peut mieux faire », à l’image d’un Twitter en perte de vitesse. Les investissements consentis par l’entreprise en R D et rachats ont été lourds (à l’image de la start-up spécialisé en MOOC Lyndia racheté 1,5 milliard en 2015). De même, les actionnaires craignent une stagnation du marché du fait de la surconcentration du réseau social sur le sol américain. C’est à ce moment que Microsoft frappe. “Cloud first and mobile first”, et “company first” aussi En 2014, Satya Nadella devient le nouveau CEO de l’ogre américain. Sa stratégie est en rupture avec celle de son prédécesseur, Steve Ballmer (désormais l’heureux propriétaire d’une franchise NBA). Après plusieurs échecs de ce dernier (l’incongrue Windows Vista, l’incompris Windows 8, le flop Windows Mobile et le rachat de Nokia), la réorientation de Microsoft est claire : « cloud first and mobile first ». Peu étonnant lorsque l’on sait que Satya Nadella est l’ancien vice-président de la branche Cloud   Entreprise. Cette stratégie se matérialise autour de 3 grands axes : le développement des services de cloud (avec Windows Azure, Dynamics CRM), l’unification des OS (PC / téléphone, et un peu Xbox) et le portage des produits phares du groupe sur les supports concurrents (dont la disponibilité de la suite Office sur iPhone est le meilleur exemple). Pour Microsoft, les leviers de croissance ne sont plus sur l’utilisation du PC familial tournant sous Windows : avec 90% de part de marché, le secteur est saturé. Après avoir raté le tournant des réseaux sociaux (qui a dit Yammer ?) et du mobile grand public, Microsoft a décidé de miser sur les entreprises. Comme évoqué, le développement de la suite Office sur Android et iOS est fait avant tout pour séduire les entreprises et leurs besoins accrus de mobilité. La mobilité et flexibilité en entreprise, deux enjeux majeurs dans les prochaines années ? Peut-être. « Sûrement », vous répondrait cette fameuse et incomprise génération Y. La mobilité, Microsoft souhaite la pousser grâce à la réunification des OS maison sous un seul et même système, Windows 10. Les rumeurs vont bon train depuis quelques temps, mais elles convergent toutes dans un sens : la prochaine gamme de mobile Microsoft serait résolument tournée vers l’entreprise, avec des caractéristiques techniques hors d’atteinte pour le portefeuille de monsieur Toutlemonde. Pourquoi ? Il s’agit simplement de transformer votre téléphone en PC miniature grâce au fameux (ou totalement inconnu) Mode Continuum. En connectant le téléphone à un écran (et un petit boitier), tous vos documents sont alors accessibles, les applications disponibles car conçus pour être utilisées dans les deux environnements. Vos outils de travail avec vous, partout. Et LinkedIn dans tout ça ? Le rachat de LinkedIn, c’est un pas de plus dans la stratégie d’implantation en entreprises. Ce rachat est amené, notamment, à étoffer l’offre CRM du géant américain et son logiciel Microsoft Dynamics CRM et combler le vide de la dimension sociale et collaborative de la suite Office. Rappelons que la prise de risque est limitée pour Microsoft : en investissant massivement pour rendre son modèle économique viable, l’entreprise est désormais rentable (à l’opposé d’un Nokia qui était en total décrochage). Pour LinkedIn et Dynamics CRM, l’utilité est toute trouvée : un enrichissement immédiat du profil des acheteurs et clients, une détection de nouveaux potentiels de vente. Compléter des offres de cloud, l’attaque sera frontale contre Salesforce. LinkedIn et Office, c’est l’assurance pour Microsoft de connecter l’ensemble de l’entreprise, en utilisant votre profil comme identifiant unique sur ses services. En pleine rédaction d’un PowerPoint ? LinkedIn vous proposera du contenu en lien avec votre sujet (grâce notamment au progrès du Machine Learning), et vous suggèrera des collègues experts sur le sujet pouvant vous prêter main forte. En parcourant le profil de votre collègue, vous êtes intéressés : un appel Skype, une réunion Outlook, et le tour est jouer. Ceci, c’est Satya Nadella lui-même qui en parler dans son mail adressé aux salariés Microsoft pour justifier ce rachat. A défaut d’être maintenant, le changement est pour bientôt. Toutefois, ne soyons pas totalement naïfs, Microsoft ne fait pas ça que pour rendre notre vie plus belle et plus simple. Acquérir LinkedIn, c’est mettre la main sur plus de 400 millions de données personnelles. Lier les utilisateurs d’Office à un profil, et le potentiel devient immense. A l’heure où la donnée est reine, où les informations se revendent à prix d’or, la richesse du réseau social combinée à l’omniprésence de Windows en entreprise risque de frapper fort. A quand des publicités ciblées sur son poste de travail ? Encore un peu de patience… Mais promis, vous serez les premiers au courant. Par Michaël Touzé, Consultant Senior du cabinet VERTUO Conseil
41	http://www.vertuoconseil.com/portfolio/square-idea-2/	
42	http://www.vertuoconseil.com/portfolio/le-marche-du-paiement-vers-un-nouvel-eco-systeme/	 Le marché du paiement : vers un nouvel éco-système VERTUO 2017-02-12T13:35:19+00:00 Project Description La Tribune – 16 juillet 2015 : Quel sera l’impact des nouveaux moyens de paiement sur les banques ? Aujourd’hui sur le marché du paiement, un certain nombre de solutions innovantes voient le jour. Ces innovations sont de plus en plus souvent portées par des acteurs non bancaires et laissent par conséquent entrevoir un nouvel éco-système du paiement que nous allons tenter de décrypter. Les acteurs « traditionnels » : banques classiques et… banques en ligne Actuellement en France, les principaux acteurs du marché du paiement sont les banques classiques : BNP Paribas, le Groupe BPCE, le Groupe Crédit Agricole, Crédit Mutuel-CIC, La Banque Postale, La Société Générale, etc. Au début des années 2000, avec ING Direct, les banques en ligne sont apparues dans le paysage des acteurs bancaires. Néanmoins, force est de constater qu’en quinze ans, le marché français n’a pas vu émerger de véritable pure player et que les banques en ligne que nous connaissons tous émanent des banques classiques : BforBank (Groupe Crédit Agricole), Boursorama Banque (Société Générale), Fortuneo Banque (Crédit Mutuel Arkéa), Hello Bank (BNP Paribas), etc. Le marché se partage donc toujours entre les mêmes acteurs… Seul fait notable, les assurbanquiers qui ont lancé leurs sites de banques en ligne avec notamment Axa Banque et Groupama Banque. De nouveaux acteurs , Établissements de Paiement et Établissements de Monnaie Électronique… Une directive Européenne, la DSP (Directive sur les Services de Paiement) a ouvert la voie à un nouveau type d’acteur : les Établissements de Paiement (EP). Ces structures, plus « légères » que les banques car soumises à des contraintes moins fortes notamment en termes de fonds propres et de capital, peuvent néanmoins fournir des services et moyens de paiement. Nous avons ainsi vu ces dernières années apparaître des structures telles que PayTop, Rentabiliweb ou SlimPay. Une autre législation, la DME (Directive sur la Monnaie Électronique) a introduit le statut d’Établissement de Monnaie Électronique (EME). Ces établissements sont donc autorisés à émettre de la monnaie électronique : citons à titre d’exemples S-Money, Ticket Surf International ou W-HA. …qui ne menacent pas les banques Ces nouveaux acteurs, réunis en France au sein de l’AFEPAME (Association Française des Établissements de Paiement et de Monnaie Électronique) ne représentent pas, en tout cas dans l’immédiat, de véritables concurrents pour les banquiers traditionnels et ce pour deux raisons principales. La première étant leur petite taille et leur positionnement sur des produits spécifiques, voire des niches de marché. La deuxième est que, si certains de ces établissements sont indépendants, d’autres appartiennent à des groupes bancaires… (exemple de S-Money, filiale du Groupe BPCE). La véritable menace pour les banques : les GAFA Il va falloir nous habituer à entendre parler des « GAFA ». En effet, cet acronyme a été inventé pour nommer les quatre géants du web et de l’industrie des télécommunications que sont Google, Apple, Facebook et Amazon. Les GAFA apparaissent aujourd’hui comme la véritable menace pour les banques : mais pourquoi finalement ? Tout d’abord, il convient de s’arrêter sur un premier chiffre : la capitalisation boursière de ces quatre acteurs équivaut aujourd’hui à celle de l’intégralité du CAC 40, soit 1.500 milliards de dollars. Autre chiffre, illustrant la « toute-puissance » des GAFA : 55% de la vie numérique d’un utilisateur lambda se concentre sur ces 4 acteurs… Les GAFA disposent donc de deux atouts déterminants qui pourraient leur permettre de bousculer les acteurs déjà en place. Premièrement, des capacités financières impressionnantes pour investir dans le paiement et l’innovation, et deuxièmement les données clients dont ils disposent grâce à un contact permanent avec les utilisateurs. Les GAFA à l’assaut du marché du paiement L’élément déclencheur, le « détonateur » même, ne laissant plus planer le moindre doute sur l’intérêt des GAFA pour le marché du paiement a été la sortie de l’iPhone 6 en septembre dernier aux États-Unis. Apple a enfin décidé de prendre le virage du NFC et cette nouvelle version de l’iPhone permet le paiement mobile sans contact via Apple Pay. Les réactions des concurrents sont très vite arrivées : Samsung, rival d’Apple sur le marché des smartphones, a racheté LoopPay et a annoncé son Samsung Pay. Google, de son côté, a racheté l’ensemble des brevets technologiques de son concurrent Softcard et vient d’annoncer son Android Pay. Bien sûr, ces trois solutions de paiement mobile souhaitent arriver en France au plus tôt pour s’imposer : l’attente ne devrait être que de quelques mois… Côté réseaux sociaux, Facebook et Twitter ont lancé le paiement P2P (Person-to-Person) dans un premier temps et ils ne s’arrêteront vraisemblablement pas à ce type de paiement, portés notamment par un nombre incalculable de comptes utilisateurs. Enfin, les grands e-commerçants et marketplaces se posent comme des acteurs de plus en plus significatifs sur le marché du paiement, notamment à travers leur rôle d’agrégateurs de flux. Par ailleurs, ils développent souvent leur propre wallet du type paiement « one-click » afin de concurrencer les solutions existantes du marché : essayez de trouver le bouton de paiement PayPal sur Amazon… D’autres typologies d’acteurs sur le marché L’arrivée sur le marché des monnaies du type crypto-monnaie, en particulier le bitcoin, a ouvert la voie à un nouveau type d’acteur. En France par exemple, Paymium est un nouvel acteur qui se pose comme une place de marché en ligne où vendeurs et acheteurs de bitcoins concluent des transactions, les vendeurs de bitcoins proposant leurs bitcoins contre des euros. Ces mêmes bitcoins pouvant ensuite être utilisés pour payer sur showroomprive.com par exemple. Un mot enfin sur les banques de la grande distribution, les plus connues étant Carrefour Banque, Oney Banque Accord (Auchan) et Banque Edel (Leclerc). Même si elles existent depuis un certain nombre d’années déjà, soulignons qu’elles sont de plus en plus actives et se positionnent clairement sur le paiement innovant (exemple Oney Banque Accord avec son wallet Flash’N Pay). Trois enjeux majeurs pour les acteurs du marché du paiement L’innovation va inévitablement faire évoluer les business models : le mobile sera l’instrument de paiement de demain, instrument qui n’appartient pas aux banques… Mais au-delà des innovations et des technologies, trois enjeux principaux s’imposent aux acteurs, quelle que soit leur taille. Le premier, ce sont les flux : peu importe la forme que prendra le paiement dans les années à venir, il sera nécessaire de concentrer un certain volume de transactions pour exister sur le marché du paiement. Le deuxième, ce sont les données clients : l’entrée des GAFA sur le marché symbolise le fait que, dans une certaine mesure, le paiement s’émancipe du monde bancaire et les données clients deviennent au moins aussi importantes que l’acte de paiement en lui-même. Nous sommes en train de passer d’un modèle du paiement à un modèle de la donnée. Enfin, troisième et dernier facteur, c’est l’expérience utilisateur. L’ergonomie de l’enrôlement, la sécurité et les services à valeur ajoutée proposés seront de véritables facteurs clés de succès. Les impacts sur l’écosystème et sur l’avenir des banques traditionnelles Les banques vont sans aucun doute garder la relation bancaire globale, surtout en France. Il est peu probable qu’elles décident de se cantonner à des activités de dépôt et de crédit, laissant le paiement aux nouveaux entrants. En revanche, elles seront certainement contraintes de « partager » les commissions liées aux actes de paiement, les acteurs étant de plus en plus nombreux sur la chaîne de valeur. Elles tireront donc moins de bénéfices de leurs activités liées au paiement, surtout dans le contexte actuel de baisse continue des commissions d’interchanges via les décisions successives du législateur européen. Ces éléments vont donc obliger les banques à revoir leur stratégie et une des stratégies prévisible est la constitution d’alliances avec des nouveaux entrants du type GAFA. Ces alliances pourraient d’ailleurs être bénéfiques pour les clients. En effet, elles permettront d’éviter une concurrence à outrance, synonyme d’un marché peu lisible et sans émergence de standards. Car même si beaucoup semblent l’oublier, le consommateur est l’acteur fondamental et c’est lui qui déterminera la réussite ou l’échec d’une solution, voire d’un acteur… Karim Terbeche, Project Manager, VERTUO Conseil
43	http://www.vertuoconseil.com/portfolio/les-lanceurs-dalertes-reconnus-dutilite-publique-et-privee-dans-notre-societe/	 Les lanceurs d’alertes reconnus d’utilité publique et privée, dans notre société Mathilde. Taillez 2017-02-26T19:25:20+00:00 Project Description Les Echos &#8211; 21 février 2017 : Le terme de lanceur d’alerte est apparu dans les années 1990, mais son sens actuel est relativement récent. Lorsque l’on énonce ce terme   lanceur d’alerte  , nous ne pouvons pas passer à côté des révélations qui ont bouleversé les États-Unis en 2013 avec l’affaire Snowden. Ce sujet qui fait couler beaucoup d’encre pose clairement la question de conformité au sein des États et des entreprises. Que ce soient les justices européennes ou américaines, les lanceurs d alertes, en 2016, ont été récompensés suite aux diverses dérives dont ils ont révélé l existence. La dénonciation, notamment dans le secteur bancaire ou des sociétés cotées, peut rapporter gros pour ceux qui s élèvent contre des pratiques illégales. La justice américaine encourage la dénonciation Les récompenses attribuées à ces   gardes fous   sont directement liées à l argent collecté suite aux sanctions prononcées contre les sociétés visées. Ainsi la Securities Exchange Commission (SEC) a attribué la bagatelle de 111 millions d euros en 2016 pour inciter les présents lanceurs d alerte à se muer en des relais de la conformité dans le monde de la finance. La loi Dodd Frank Act a aussi fortement insisté sur l instauration d une prime à la dénonciation sur les usages non conformes aux lois américaines ; ainsi la récompense, si la sanction dépasse le million de dollars, bénéficie à hauteur de 10 % à 30 % au lanceur d alerte qui a remonté l information qui a conduit à condamner la société visée. Inédit en 2016, un lanceur d alerte a refusé la prime accordée par la SEC pour le motif que le régulateur financier n a pas puni les hauts dirigeants de la société, mais les actionnaires par la sanction financière envers cette entreprise. Ainsi, il explique que la société s est vu adresser une lourde amende, mais les dirigeants de celle-ci sont partis en retraite sans ne jamais être inquiétés. En plus de dénoncer les mauvaises pratiques, le lanceur d alerte a également indiqué le fait que des juristes de la SEC (public) ont des positions de séniors au sein de différentes banques (privé), ce qui conduit à de grosses collusions d intérêts. Une banque française fait les frais de la loi Sapin II Plus récemment, la justice française a accordé (en appel), le statut de lanceur d alerte à un collaborateur Natixis, qui a été licencié en 2008 pour avoir dénoncé une manipulation des cours au sein de cette banque d investissement. La loi Sapin II adoptée en décembre 2016 indique clairement qu un salarié qui dénonce des faits pouvant constituer un délit ou crime ne peut pas subir de représailles de la part de son entreprise. Par voie de conséquence, la cour d appel signale à Natixis de réintégrer ce salarié et lui verser une indemnité conséquente. Cette jurisprudence pourrait voir éclore en France, des lanceurs d alerte, qui comme aux États-Unis auraient un rôle prépondérant dans la surveillance des pratiques dans le domaine financier. En France, depuis les années 1990, les employeurs poursuivent leurs salariés suite aux révélations et la loi promulguée en décembre 2016 change la donne et pourrait bien faire inverser la tendance. L Europe étudie un statut de lanceur d alerte au niveau européen Un travail est en cours actuellement pour la création d un   statut de lanceur d alerte au niveau européen  , mais se trouve face au dilemme où   L Union européenne ne peut pas légiférer en matière pénale  , en d autres termes, c est la justice du pays où la société se trouve qui fait foi et non une instance européenne. Une étude juridique est en cours, pour définir la faisabilité (ou non) de ce nouveau statut avec l accord des états membres qui ont mis l accent sur la transparence fiscale notamment.   J insiste sur l importance d avoir une supervision européenne, et de ne pas laisser les États se prononcer eux-mêmes sur la question, car certains mettraient sans doute moins de zèle à offrir la protection   indique l eurodéputée Virginie Rozière, dans une interview réalisée dans l Express. Cette instance européenne trouve pleinement son sens lorsqu il est question de l homogénéité des procédures ainsi que certaines affaires qui ont des répercussions en dehors du pays initialement concerné. Le sujet de l évasion fiscale est un sujet crucial qui continue de faire parler de lui, après les initiatives FATCA puis AEOI (Automatic Exchange Of Information), c est désormais à l administration française de récompenser les indics du Fisc pour traquer les personnes susceptibles de ne pas se conformer à la fiscalité française. Les députés français ont voté mi-novembre, une mesure qui permet pendant 2 années à l administration française de   rémunérer des informations adressées par des personnes étrangères aux administrations publiques afin de révéler un comportement frauduleux  . Bien sûr, des conditions sont nécessairement requises pour profiter de ces forfaits attribués aux informateurs, dans la mesure où l affaire doit être d une taille suffisamment critique. Les salariés toujours mobilisés dans les grandes entreprises malgré un pourcentage en baisse D après un baromètre BVA/Cercle d éthique des affaires/La Poste, lancé en 2012 ; environ 9 salariés sur 10 se déclaraient   probables   ou   certains   à la déclaration de faits graves au sein de leurs entreprises. Ce chiffre diminue de 21 points en 1 an, probablement la conséquence suite aux récents procès tels que les affaires Luxleaks (avec le lanceur d alerte Antoine Deltour) et UBS France (avec la lanceuse d alerte Stéphanie Gibaud) avec des retombées fortement négatives vis-à-vis du lanceur d alerte. La loi Sapin 2 permettant depuis peu, de protéger l informateur, le pourcentage de salariés impliqués devraient normalement repartir à la hausse suite au projet de loi allant dans ce sens. Qui dit dénonciation, dit dérive&#8230; L incitation à la dénonciation, favorisée par un environnement complexe, est un moyen efficace pour les gouvernements de traquer les fraudeurs et ainsi sanctionner les entreprises qui   faussent les règles du jeu  . Néanmoins, de multiples dérives pourraient voir le jour avec une recrudescence d informations erronées dans le simple but spéculatif. Ainsi, les cellules dédiées aux enquêtes financières vont devoir se retrousser les manches et réussir à démêler le vrai du faux, sans quoi, des informations rendues publiques et non avérées pourraient avoir des conséquences désastreuses sur les sociétés visées. Les entreprises vont devoir renforcer leurs contrôles en interne et former les collaborateurs aux bonnes pratiques, les auditeurs ont de beaux jours devant eux&#8230; Par Benjamin Fradet, consultant Senior du cabinet VERTUO conseil
44	http://www.vertuoconseil.com/portfolio/plateformes-evolution-systeme-bancaire/	
45	http://www.vertuoconseil.com/portfolio/la-formation-un-investissement-partage-collectif/	
46	http://www.vertuoconseil.com/portfolio/reforme-europeenne-protection-donnees-va-impacter-banques-assurances/	 Comment la réforme européenne sur la protection des données va impacter les banques et assurances ? Mathilde. Taillez 2017-07-12T07:01:31+00:00 Project Description En mai 2018, la nouvelle réforme européenne sur la protection des données entrera en vigueur. Elle vise à augmenter et faciliter le contrôle des données personnelles des citoyens européens, données manipulées en permanence par les banques et assurances. Le Parlement Européen vote la réforme sur la protection des données Après plus de quatre ans de travaux, le Parlement Européen a finalement voté en avril 2016 une réforme majeure sur la protection des données des citoyens européens (Règlement général sur la protection des données ou RGPD). Cette réforme vient remplacer la directive établie en 1995, date à laquelle internet en était encore à ses balbutiements. La directive sera applicable dans tous les pays de l’UE deux ans après sa publication au Journal officiel, soit en mai 2018. D’ici là, l’ensemble des états membres devront avoir traduit le règlement dans leur législation nationale. La réforme répond à un objectif clair : « avoir un niveau élevé et uniforme de protection des données à travers l’UE ». Elle touche ainsi toutes les facettes de la protection des données : Droit à l’oubli Consentement clair et explicite de la personne concernée quant à l’utilisation de ses données personnelles Droit d’être informé en cas de piratage des données Garantie que les politiques relatives à la vie privée soient expliquées dans un langage clair et compréhensible Création d’un Comité européen de la protection des données Protection des données dès la conception : dès la conception des produits, services et systèmes exploitant des données personnelles, les organisations doivent prendre des mesures visant à contrôler et limiter les données utilisées Protection des données par défaut : toutes les organisations doivent disposer d’un système d’information sécurisé Nomination d’un délégué à la protection des données pour tout organisme public ou privé dont « les activités de base […] consistent en des opérations de traitement qui, du fait de leur nature, de leur portée et/ou de leurs finalités, exigent un suivi régulier et systématique à grande échelle ». A noter qu’ « un groupe d’entreprises peut désigner un seul délégué à la protection des données à condition qu’un délégué à la protection des données soit facilement joignable à partir de chaque lieu d’établissement » Les sanctions en cas de non-respect du règlement ont également été renforcées, pouvant aller jusqu’à 20 millions d’euros d’amende ou, dans le cas d’une entreprise, jusqu’à 4 % du chiffre d’affaires annuel mondial total de l’exercice précédent, le montant le plus élevé étant retenu.Enfin, il est important de noter que le règlement s’applique aux organisations de l’UE ainsi qu’aux entreprises non établies dans l’Union Européenne mais traitant des données à caractère personnel relatives à des personnes sur le territoire de l’UE. Les banques et assurances impactées à tous les niveaux A la lecture du règlement européen, il semble évident que les banques et assurances vont être particulièrement impactées. Tout d’abord, de par leurs activités, elles sont amenées à traiter en permanence des données personnelles, ces dernières étant définies par le Parlement Européen comme toute information « permettant d’identifier, directement ou indirectement, notamment par référence à un identifiant, tel qu’un nom, un numéro d’identification, des données de localisation, un identifiant en ligne, ou à un ou plusieurs éléments spécifiques propres à son identité physique, physiologique, génétique, psychique, économique, culturelle ou sociale; » une personne physique. En l’occurrence, les établissements financiers recensent les noms, adresses, ou numéro de sécurité sociale de leur client, mais également des données relatives à leur santé en cas de demande de prêt. Enfin, ils traitent bien évidemment toutes les données bancaires de leurs clients, de l’IBAN au numéro de carte bleue. Les banques et assurances vont donc devoir réaliser un audit et mettre en place un plan d’actions visant à se mettre en conformité avec le nouveau règlement. Si le règlement ne précise pas le niveau attendu, l’ensemble des systèmes d’information devra être sécurisé. Elles vont devoir nommer un délégué à la protection des données et faire un effort de documentation. En effet, la réforme sur la protection des données oblige les organisations à prouver qu’elles respectent les principes du règlement, et à documenter et auditer les process mis en place (principe d’ « Accountability »). Les audits devront également concerner les sous-traitants des banques, au sein et hors de l’UE, dès lors qu’ils ont accès aux données personnelles des résidents européens. Alors que les projets d’offshoring et de nearshoring sont légion dans les banques, la tâche va être conséquente. La réforme sur la protection des données va également amener le secteur financier à adapter sa communication vis-à-vis de sa clientèle. Viennent ainsi à l’esprit la modification de la documentation à destination de la clientèle afin de répondre aux exigences en matière de consentement, la traduction en langage clair de leur politique de protection des données (Crédit Agricole a par exemple communiqué en 2016 une Charte des données personnelles) ou la formalisation d’un plan de communication de crise en cas de piratage de données. Enfin, le secteur financier va devoir adapter ses outils et méthodes de gestion de projet. D’une part, la notion de protection des données va devoir être prise en compte dès la conception des nouveaux outils (« privacy by design »). Ces derniers devront permettre de paramétrer et gérer un niveau de sécurité adéquat. D’autre part, un aspect indirect de la réforme concerne les jeux de données et les environnements utilisés pour mener à bien les projets des banques et assurances. Lorsqu’un établissement modifie son système d’information ou met en place un nouvel outil, des données réelles sont utilisées pour tester les nouvelles applications. Ces données, pouvant comporter des noms, des IBAN ou toute autre donnée personnelle, sont alors accessibles aux diverses équipes projet et IT. Hors, le règlement dispose dans l’article sur la Protection des données dès la conception que « seules les données à caractère personnel qui sont nécessaires au regard de chaque finalité spécifique du traitement sont traitées. Cela s’applique à la quantité de données à caractère personnel collectées, à l’étendue de leur traitement, à leur durée de conservation et à leur accessibilité. En particulier, ces mesures garantissent que, par défaut, les données à caractère personnel ne sont pas rendues accessibles à un nombre indéterminé de personnes physiques sans l’intervention de la personne physique concernée. ». Plus concrètement, cela signifie que les données utilisées pour les jeux de test devront faire l’objet d’une anonymisation (modification du contenu ou la structure de ces données afin de rendre très difficile ou impossible la « ré-identification » des personnes) ou d’une pseudonymisation (remplacement d’un nom par un pseudonyme). Une attention particulière devra également être apportée aux environnements informatiques utilisés pour réaliser les tests. En effet, afin de ne pas perturber la production, les DSI utilisent des environnements informatiques alternatifs pour réaliser leurs tests. Hors ces environnements ne disposent pas toujours du même degré de sécurité que les environnements de production et devront donc, soit être mis à niveau, soit ne contenir aucun donnée personnelle non anonymisée. Les départements RH avaient déjà commencé à se mettre en conformité avec les règlements précédents. Cependant, la démarche de protection des données doit dorénavant être étendue à l’ensemble de la banque et faire l’objet d’un suivi beaucoup plus systématique. Les banques et assurances étudient donc la réglementation afin de lister les exigences minimales requises. Déjà aux prises avec de nombreuses réglementations européennes, elles souhaitent investir a minima pour répondre aux exigences de la RGPD. Des rencontres avec des éditeurs de logiciel d’anonymisation sont également organisées (Optim d’IBM ou DOT-Anonymizer) et des phases de POC (proof of concept) sont lancées. Par Mathilde DEGREMONT, Senior Manager du Cabinet Vertuo Conseil
47	http://www.vertuoconseil.com/portfolio/rallye-des-princesses/	
48	http://www.vertuoconseil.com/portfolio/square-de-lautre-cote-du-miroir/	
49	http://www.vertuoconseil.com/portfolio/malgre-laccord-le-ceta-est-un-echec-democratique-pour-leurope/	 Malgré l’accord, le Ceta est un échec démocratique pour l’Europe VERTUO 2017-02-12T12:52:22+00:00 Project Description Les Echos – 27 octobre 2016 : La Wallonie a longtemps refusé le Ceta. Cela illustre à quel point l’UE peine à négocier des accords commerciaux correspondant aux attentes de ses populations. C’est presque fait, et ce fut difficile. Une position commune de la Belgique sur le traité de libre-échange entre l’Union européenne et le Canada (Ceta) a été arrêtée, ce qui pourrait permettre sa signature prochaine , a affirmé ce jeudi 27 octobre le Premier ministre belge, Charles Michel. La Wallonie, une région belge, avait opposé son veto à un accord sur le Ceta. Ces péripéties illustrent à quel point la politique commerciale de l’Union européenne peut être contestée, même par une région de 3,5 millions d’habitants. Compétente pour négocier le libre-échange de ses États membres, l’UE est dorénavant discréditée dans sa capacité à mener un tel accord jusqu’au bout. Contestable à plusieurs titres, sa politique commerciale ne pourra plus éviter une remise en cause. Devant le fait accompli Exemples d’opacité, les négociations du traité de libre-échange enter l’UE et les Etats-Unis (le TTIP) n’offrent que des fuites d’informations invérifiables. Seule une poignée de parlementaires européens disposeraient d’un accès aux documents, sans pouvoir les communiquer. Secret du mandat des négociateurs, secret des négociations… voilà donc les entreprises, les consommateurs et les autorités publiques devant le fait accompli d’un accord qui se discute dans leur dos. Une fois arrangé au fil des années de négociation, il devient presque impossible de renégocier. D’ailleurs, la ratification politique semble envisagée comme une formalité, d’autant que des clauses d’application provisoire peuvent jouer, à l’instar du Ceta. Comment ferait-on plus illégitime et suspicieux ? Les élus de Wallonie n’ont pas apprécié et rappellent à l’UE que le Ceta ne fait pas consensus. À négocier en trop haute altitude politique, l’UE s’est coupée de sa base. L’arbitrage en question Présent dans les accords Ceta et TTIP, le règlement des litiges par arbitrage suscite la controverse. Pourtant, investir dans un pays revient à s’exposer à un risque juridique. En effet, bien qu’engagé dans un contrat avec l’investisseur, un État peut ensuite modifier son droit. En cas de préjudice, le recours à la justice de cet État deviendrait donc trop incertain. Ainsi, les mécanisme de règlement des différends entre investisseurs et États organisent un contre-pouvoir juridique neutre pour gérer ce risque. Pratiqué pour les litiges entre entreprises, l’arbitrage pose une question politique quand il s’exerce contre des États ou des collectivités issues du suffrage universel. Parce qu’ils encourront le risque d’une sanction des arbitres, les États ne pourront plus modifier leur droit sans respecter d’abord les intérêts des investisseurs. Les conflits d’intérêts entre ces investisseurs et les citoyens placeront les États dans une situation politique et financière incertaine. Au final, ces mécanismes d’arbitrage font émerger un conflit de légitimité sur la norme dominante. Entre commerce et démocratie, il faudra choisir. L’échec idéologique Délocalisations, désindustrialisation et chômage de masse : ces constats sont bien connus. Ceux qui font le procès de la mondialisation ont beaucoup d’arguments. La politique commerciale de l’UE a aussi produit un échec social. Souvent sacrifiée sur l’autel d’une concurrence dérégulée, une partie de notre population a été perdante dans l’intensification du commerce international. Les institutions européennes ont laissé l’ultra libéralisme guider le commerce de l’UE, sans produire en parallèle les politiques de protection de nos intérêts économiques et sociaux. Plus commerçante que démocratique, l’UE a perdu l’adhésion de ses citoyens. Ainsi usées, les institutions européennes ne sont plus suffisamment légitimes pour être représentatives. Critiqués pour leurs contenus et leurs mécanismes, les accords commerciaux négociés par l’UE ne répondent pas aux attentes de la population européenne. Le refus de transparence lamine l’adhésion de citoyens et accentue leur rejet de la gestion politique européenne. Par Pierre Theobald, Consultant Senior chez Vertuo Conseil
50	http://www.vertuoconseil.com/portfolio/vertuo-conseil-house-of-finance-universite-paris-dauphine/	
51	http://www.vertuoconseil.com/portfolio/week-end-square-a-tenerife-mars-2016/	
52	http://www.vertuoconseil.com/portfolio/consultants-de-vertuo-conseil-se-retrouvent-a-val-thorens-week-end-ski/	
53	http://www.vertuoconseil.com/portfolio/bale-iv-normer-cest-gagner/	 Bâle IV : normer c’est gagner vertuoconseil 2017-02-12T12:58:06+00:00 Project Description Les Echos – 5 septembre 2016 : Les propositions du Comité de Bâle (dites « Bâle IV ») sont dans la ligne de mire du lobby bancaire. Les prémisses de ce changement normatif s’annoncent comme autant de menaces pour le secteur bancaire européen. Le 12 juillet, le Conseil « EcoFin » de l’Union européenne a été l’occasion de liguer les intérêts européens. Avant même sa traduction juridique, le champ normatif apparaît de plus en plus comme un théâtre d’affrontement autonome. Imposer son modèle est la stratégie de tous. L’art d’exporter des contraintes Le Sénat a voté une résolution le 18 juin dernier pour contester les propositions de réforme du Comité de Bâle. Le courroux sénatorial s’est polarisé sur le marché du crédit immobilier. L’une des conséquences possibles des propositions du Comité de Bâle serait la standardisation des modèles de crédits immobiliers : à la clef, une diminution des risques pour les banques. Mais, dans la ligne de mire des sénateurs, il y a le risque de taux. Le modèle français repose majoritairement sur le cautionnement et la solvabilité des emprunteurs, avec un niveau d’impayés très bas et des taux fixes. Le modèle fait consensus. À l’inverse, le Comité de Bâle favoriserait d’une part l’hypothèque pour inciter à la titrisation et alléger les bilans bancaires, et d’autre part le crédit à taux variable pour faire peser le risque sur l’emprunteur. Ce transfert de risque sur les clients est particulièrement dans le viseur du Sénat, qui sait mesurer le coût politique de ce genre de proposition en année préélectorale : assèchement de l’offre de crédit et recul de l’accès à la propriété. À ce stade, la standardisation voulue par le Comité de Bâle apporterait surtout les contraintes d’autres modèles de financement dans un modèle sain. Alors, pourquoi changer un modèle qui marche et qui a été résilient ? La question fait sens pour les clients et les banques. La fortune sourit aux audacieux À l’instar des sénateurs, les présidents de la Fédération bancaire française et de l’Association des banques allemandes mettent en avant, dans leur communiqué conjoint du 6 juillet, que les propositions du Comité de Bâle donneraient aux banques américaines une position plus avantageuse. Leur argument est celui de l’importance des marchés financiers dans le financement des entreprises américaines. Un autre prisme pour une même menace : véritables infrastructures de la mondialisation financière, les normes bâloises sont l’annonce des futures règles juridiques qui vont s’imposer comme le cadre du marché bancaire. Au stade actuel de la prénorme, l’affrontement est déjà bien celui de deux modèles : les lobbyistes ne s’y sont pas trompés et le Conseil « EcoFin » a validé leur approche. Pour compter, les banquiers européens ont produit un consensus et veulent maintenant donner une envergure politique à ces sujets techniques. Cette approche audacieuse va renvoyer l’affrontement dans l’instance bâloise où les positions européennes et américaines seront confrontées à partir d’un rapport de force rééquilibré au profit des Européens. Le risque d’échec n’est pas à exclure et même à envisager : en l’absence de crise financière, un durcissement des normes prudentielles est un choix d’évolution sectoriel, mais n’emporte aucune valeur ajoutée pour l’économie. Choisir l’échec est aussi une option. Portés par le succès du traité Trans Pacific, engagé dans la négociation du marché transatlantique, les États-Unis jouent gagnants dans l’organisation des marchés mondiaux. Bâle est l’étape bancaire de cette stratégie. Ce ne sera pas la seule : les services financiers étant inclus dans les très secrètes négociations sur le Trade in Services Agreement (TISA), un nouvel affrontement s’annonce. Par Pierre Theobald, Consultant Senior chez Vertuo Conseil
54	http://www.vertuoconseil.com/portfolio/decryptage-bale-iv-vers-durcissement-regles/	 Décryptage &#8211; Bâle IV : Vers un durcissement des règles Mathilde. Taillez 2017-10-06T06:34:32+00:00 Project Description Alors même que les régulateurs travaillent encore sur la finalisation des règles de Bâle III, le chantier Bâle IV est déjà à l’ordre du jour avec de nouvelles réformes susceptibles de secouer les institutions bancaires à l’échelle internationale. Si les précédents standards Bâlois ont été des réponses à des crises majeures du système financier mondial, Bâle IV est quant à lui perçu comme une action préventive à un éventuel « Financial turmoil ». Les nouvelles réglementations sous l’égide de Bâle IV mettent l’accent sur les différents types de risques (opérationnel, crédit, marché, liquidité) et mettent particulièrement en exergue le risque de taux, qui lui n’a pas été revu depuis une dizaine d’années. Face à ce nouveau tournant réglementaire qui fait plonger les banques dans une grande incertitude, plusieurs questions se posent. Tout d’abord, de quoi est fait Bâle IV ? Quels sont les impacts par rapport à la situation actuelle ? Comment les Banques vont-elles s’adapter à ce nouveau cadre réglementaire ? Lire la suite     Par Yasmine ZAAJ, Consultante du Cabinet Vertuo Conseil
55	http://www.vertuoconseil.com/portfolio/square-talent-1/	
56	http://www.vertuoconseil.com/portfolio/reduction-couts-uniquement-etape-de-transformation-continue/	 Réduction des coûts : uniquement une étape de la transformation continue Mathilde. Taillez 2017-03-08T21:29:29+00:00 Project Description Les Echos &#8211; 22 février 2017 : La réduction des coûts ne peut plus être envisagée de manière isolée, elle doit s’inscrire dans le cadre d’une transformation continue. Plus que jamais, il est donc important de repenser la finalité et la manière de mener à bien les plans de réduction des coûts, car ces derniers s’installent de manière durable dans le paysage. Face à une conjoncture difficile caractérisée par une combinaison inédite de facteurs, un haut niveau d incertitude pèse sur les acteurs du secteur bancaire. Ils doivent désormais vivre avec des revenus financiers moins élevés, ils devront très certainement mener à bien des plans de réduction des coûts plus fréquents, et il sera de plus en plus difficile de garantir le maintien des économies réalisées d une année à l autre. Plus que jamais, il est donc important de repenser la finalité et la manière de mener à bien les plans de réduction des coûts, car ces derniers s installent de manière durable dans le paysage. Des résultats qui interrogent Les études montrent que la moitié des entreprises qui prennent des mesures de réduction des coûts n atteignent pas leur cible. Les objectifs fixés ne sont parfois pas atteints ou le sont de manière partielle, ce qui arrive lorsque la cible de réduction de coûts n a pas été déclinée de manière opérationnelle. Des ambitions irréalistes, ou une réalité opérationnelle insuffisamment prise en compte, en sont la cause principale. En l absence de reportings appropriés (pertinence, complétude, fraîcheur des données), d un diagnostic préalable et de coopération avec les acteurs concernés, il est en effet difficile de réaliser cette déclinaison. Les objectifs sont d autres fois atteints, mais pas de manière durable. Lorsque le plan de réduction des coûts est perçu comme un exercice ponctuel, les efforts ont tendance à se concentrer sur les symptômes plutôt que sur les causes, compromettant une implantation pérenne du résultat obtenu. Par exemple, une réduction des effectifs d un département peut être décidée, sans engager les actions d organisation permettant de réduire la charge de travail ; à moyen terme l achat de prestations externes viendra probablement neutraliser les résultats initialement obtenus. Seuls 10 % des entreprises réussissent à préserver les réductions de coûts au-delà de 3 ans. Pour celles-ci, une des causes principales est le manque de compréhension par les employés de la nécessité et des avantages des mesures prises. L amélioration du résultat financier ne suffit pas pour impliquer les collaborateurs et réussir un plan de réductions des coûts. Changer d approche Selon une étude américaine, 30 % des entreprises cotées en bourse peuvent faire faillite dans les 5 ans à venir. Il y a 50 ans, seuls 5 % étaient concernées ! Si une entreprise manque un changement du marché, 3 à 5 ans seront nécessaires pour rattraper son retard ; si cette entreprise manque deux changements, elle disparaîtra très probablement. L environnement est devenu bien plus risqué pour les entreprises, l amélioration continue ne suffit pas, des transformations d une profondeur accrue (sur le plan financier, organisationnel, commercial, informatique, etc.) sont nécessaires pour assurer leur survie et développement. L environnement disruptif sans précédent dans lequel nous évoluons, amène aussi à les engager de manière plus fréquente. Dans ce contexte, il n est plus possible d envisager un plan de réduction des coûts avec pour unique objectif l amélioration des résultats financiers. Traditionnellement, les bonnes pratiques préconisaient de veiller au bon alignement de ces plans avec la stratégie de l entreprise ; désormais, il faudra plutôt veiller à leur bonne articulation avec la transformation continue, en tant que moyens au service de la transformation, pour ne plus être un but en soi. En associant systématiquement réduction de coûts et transformation, une plus grande place est accordée à la finalité d un plan (le pourquoi). Sa déclinaison opérationnelle (le comment) devient également plus logique, car elle s articule autour des actions de transformation. La cible à atteindre (le combien) n est alors qu une donnée du problème à résoudre, et non plus l objectif principal. De la théorie à la pratique La réussite de cette approche passe par la capacité des banques à traduire cette association en actions concrètes. Cela peut être fait en fixant deux règles simples, à appliquer au volet réduction des coûts d un plan de transformation : un objectif d investissement doit être mis en face de chaque objectif d économie, et une approche collaborative impliquant les acteurs opérationnels doit être privilégiée. En procédant de cette manière, le ciblage des réductions de coûts est meilleur. L expérience montre que lorsque cette responsabilité est confiée à des financiers ou à des profils Lean Six Sigma, les résultats ne sont pas au rendez-vous, par un manque de connaissance approfondie du métier. Lorsque les employés sont impliqués dans le choix des actions à mener et comprennent l utilité des économies réalisées, leurs propositions sont plus pertinentes. Les actions qui compromettent le bon fonctionnement de l existant ou la création de valeur ne sont pas retenues. Par exemple, la technique consistant à générer des économies en jouant sur la réallocation des coûts sur des centres de coûts différents ne serait pas adoptée. Un autre avantage est que les résultats obtenus sont durables. Lorsque les investissements pour mener à bien une transformation sont en jeu, une grande attention est accordée à la cohérence et la pérennité des solutions choisies. Par exemple, si l entreprise procède à un gel des recrutements externes au profit de la mobilité interne pour réduire les coûts, elle investira aussi dans des plans d accompagnement sur mesure, pour gérer l écart entre les compétences du candidat en mobilité et celles requises pour le poste : l économie engendrée à court terme ne compromet pas le bon fonctionnement à moyen terme ni les investissements de long terme. Grâce à ces deux règles simples, les résultats produits par un plan de réduction de coûts gagnent en pertinence et pérennité, et la capacité à mener à bien une transformation se trouve renforcée. Conclusion Un changement culturel reste néanmoins nécessaire pour que ces deux règles puissent être appliquées dans l ensemble du secteur bancaire : les banques restent des entreprises très pyramidales où la dimension collaborative n a pas encore toute sa place ; cela a compromis la réussite des différents plans de réductions de coûts engagés ces dernières années. Nous pouvons néanmoins parier sur la nécessité d engager des transformations de rupture pour pouvoir faire face à une incertitude stratégique qui s avère persistante : elle ne peut avoir lieu qu en impliquant de nouveaux acteurs externes à la pyramide. Au-delà de la réduction des coûts, il s agit de transformer l industrie bancaire pour permettre une amélioration durable de ses revenus financiers. Comme ce fut le cas pour l industrie musicale pendant les années 2000, cette transformation ne peut avoir lieu que si les décideurs de l industrie bancaire acceptent de sortir du cadre. Par Claudio MALDONADO, Senior Manager du cabinet VERTUO conseil
57	http://www.vertuoconseil.com/portfolio/vertuo-affirme-engagement-limiter-empreinte-environnementale/	
58	http://www.vertuoconseil.com/portfolio/livre-sterling-une-anomalie-dans-le-referendum/	 Livre sterling : une anomalie dans le referendum VERTUO 2017-02-12T13:10:44+00:00 Project Description Les Echos – 24 juin 2016 : L’imminence du referendum britannique a dopé l’intérêt pour l’Euro et le dollar américain. Et pour cause, le risque d’effondrement de la Livre sterling n’est pas théorique. Sans exagérer les conséquences de ce risque, il est évident que l’économie britannique sera impactée globalement. Livre sterling contre Euro : ce jeu de spéculation est le résumé du désaccord de fond qui se cristallise aujourd’hui dans ce referendum. Mais l’arbre cache la forêt… La Livre contre l’Euro : un divorce consommé Un « no » à l’Union européenne entraînerait sans doute une fuite massive des capitaux, et donc une crise de la Livre sterling. C’est assurément une mauvaise situation pour le Royaume-Uni, mais le mal sera de court terme. Le pays ne va pas s’effondrer, contrairement aux arguments politiques entendus dans la campagne. La question est d’abord économique : les intérêts de Sa Majesté ne sont pas en ligne avec le projet européen d’une union « sans cesse plus étroite ». Tout est dans les mots. Outre-Manche, on défend la souveraineté économique nationale, à la différence de la Zone Euro qui joue collectif et essaie de s’améliorer en produisant une gouvernance acceptable. Malgré les crises et une collégialité difficile à pratiquer, la Zone Euro verse irréversiblement dans une gouvernance « sans cesse plus étroite ». La Livre sterling est seule. Fatalement, les sujets de gouvernance s’estomperont avec l’interdépendance des économies utilisant l’Euro et Londres constatera un jour que ses voisins sont une puissance économique et monétaire cohérente : à ce stade, le prix de l’indépendance économique sera peut-être trop cher. Le Royaume-Uni sera alors en position de dépendance, face à plus fort que lui. C’est l’isolement économique guette. « God save the pound », et après ? L’orgueil monétaire est à son paroxysme outre-Manche. La souveraineté économique de Sa Majesté est discutée à un moment où elle est position de force dans le jeu européen et où elle profite à plein du sentiment nationaliste porté par UKIP. L’inconvénient du referendum est qu’il ne règle pas les enjeux de souveraineté économique en cause. La question posée est un choix de stratégie économique globale : la monnaie n’est qu’un symbole nationaliste dans ce débat. On ne peut que constater que la stratégie de négociation de David Cameron est un échec, puisqu’il n’a rien obtenu par la diplomatie et a dû choisir l’option du referendum. D’où le risque réel d’un Brexit et de ses aléas en tout genre. L’aléa monétaire et financier est fort pour une économie aussi financiarisée que celle du Royaume-Uni. Mais la catastrophe ne guette pas pour autant. L’exemple pertinent est au-delà des Alpes. Le modèle suisse devrait inspirer outre-Manche : tous les avantages de l’indépendance sans les inconvénients européens. Le compromis géoéconomique de la Confédération répond aux angoisses des partisans du Brexit sur plusieurs tableaux. C’est une option en cas de Brexit. Reste qu’il repose sur un partenariat commercial avec l’Union européenne : les Suisses ne sont pas isolés, loin s’en faut. L’Europe a été bâtie sur une union commerciale qui a apporté la prospérité économique. Le risque mortel pour le Royaume-Uni est d’en sortir sans la perspective d’une autre alliance qui garantisse sa sécurité économique. Par Pierre Theobald, Consultant Senior chez Vertuo Conseil
59	http://www.vertuoconseil.com/portfolio/decryptage-retour-loi-eckert/	 Décryptage &#8211; Retour sur la loi Eckert Mathilde. Taillez 2017-10-04T10:05:01+00:00 Project Description Votée le 13 juin 2014, la loi Eckert, dont l’ensemble des dispositions est entré en vigueur le 1er janvier 2016, est destinée à accroître la protection des épargnants en rendant plus efficace la recherche des bénéficiaires de comptes bancaires inactifs et de contrats d’assurance vie en déshérence. Selon les estimations de la Cour des Comptes, dans son Rapport du 17 juillet 2013, les encours des avoirs bancaires, des contrats d’assurance vie et de capitalisation non réclamés atteignaient un montant total de 3,96 milliards d’euros (1,2 milliard d’euros pour les avoirs bancaires et 2,76 milliards d’euros pour les contrats d’assurance). Ce montant n’a cessé d’être revu à la hausse jusqu’au chiffrage de 4,6 milliards d’euros rendu public par le Sénat le 17 avril 2014. Lire la suite     Par Christelle BERNHARD, Consultante Senior du Cabinet Vertuo Conseil
60	http://www.vertuoconseil.com/portfolio/a-lheure-brexit-lelargissement-continue/	
61	http://www.vertuoconseil.com/portfolio/non-le-numerique-ne-remet-pas-fondamentalement-en-cause-le-metier-de-banquier/	
62	http://www.vertuoconseil.com/portfolio/risques-de-financement-business-model-bancaire/	 Les risques de financement du business model bancaire Mathilde. Taillez 2017-04-20T12:09:21+00:00 Project Description Banque   Stratégie &#8211; 19 avril 2017 : Le renforcement de la réglementation bancaire et financière et la révolution technologique sont deux phénomènes non liés mais qui font peser des risques nouveaux sur le modèle économique bancaire et notamment sur la capacité des banques à se financer, via ses deux principaux vecteurs, les émissions sur les marchés et les dépôts. Depuis 2007, l’industrie bancaire a été fragilisée par des faillites d’acteurs majeurs et une crise de liquidité suite à l’explosion de la bulle des subprime, puis pointée du doigt dans la spéculation contre la dette souveraine. Cette responsabilité, dans une crise financière jugée sans précédent depuis celle de 1929 et dont de nombreux pays européens peinent toujours à se relever, pèse aujourd’hui dans l’approche réglementaire suivie par les différentes autorités prudentielles actives au sein de l’Union européenne. Cette approche repose sur trois piliers principaux : le renforcement des exigences réglementaires, une surveillance rapprochée et intrusive, une politique monétaire non conventionnelle. Les conséquences réglementaires Sur le premier volet des exigences, et même si les discussions sur le calibrage du cadre réglementaire de la prochaine décennie semblent piétiner, la BCE et le Comité de Bâle se sont engagés sur la voie d’une révision massive de l’ensemble de ses thématiques de prédilection. Aussi bien sur le plan des exigences quantitatives avec une révision complète du dénominateur du ratio de solvabilité (prise en compte du risque de taux, limitation de l’usage et de l’intérêt des modèles internes, etc.) et la montée en charge du ratio de liquidité à long terme, que des exigences de gouvernance interne, de contrôle et de reporting financier. Cet accroissement significatif des exigences réglementaires quantitatives entraîne une diminution de la rentabilité des actifs bancaires parallèlement à un accroissement du coût moyen pondéré au passif. Les banques sont ainsi fortement incitées à envisager des financements les moins coûteux possibles mais en cohérence avec les nouvelles exigences réglementaires, notamment la plus grande exigence en fonds propres, particulièrement pour les grandes banques, alors même que les dettes admissibles à ce titre par les régulateurs deviennent plus complexes et doivent être de plus en plus sécurisées. Sur le second volet, la BCE fonde désormais sa supervision bancaire sur la constitution d’équipes d’audit (les Joint Supervision Teams) dédiées à temps plein à l’analyse des outils et processus internes. Enfin, sur le troisième volet, la BCE pensait, avec sa politique dite du « Quantitative Easing » d’expansion monétaire, combiner une stratégie de soulagement des bilans bancaires (la création de monnaie se faisant indirectement à travers le rachat d’actifs inscrits au bilan des banques) avec une relance de la demande de crédit (donc de la consommation des ménages et de l’investissement des entreprises) rendue possible par le maintien de taux directeurs exceptionnellement bas. Dans les faits, cette baisse des taux et cette libération des bilans se sont surtout traduites pour les banquiers et assureurs, par une remise en cause des marges de rentabilité orientant les stratégies vers des marchés actions plus rentables (mais extrêmement volatils) loin de l’objectif initial. Ce nouveau contexte faisait finalement planer une forme de risque de taux à une échelle tout à fait nouvelle, imposant une adaptation rapide et de grande envergure. L’émergence des FinTechs La responsabilité des établissements financiers pèse également, encore aujourd’hui, dans la perception du grand public de l’image et de la fiabilité des banques. Combinée à d’autres facteurs macroéconomiques, cette quasi-défiance a pu se traduire dans certains pays par un retrait partiel des dépôts et une décollecte de l’épargne sur les différents supports classiques (livrets, assurance vie, prévoyance retraite). Par ailleurs, le contexte économique général exposé plus haut a été propice à l’émergence de nouveaux acteurs profitant de facteurs structurels facilitateurs, comme l’exploitation de progrès technologiques au service d’un usage digitalisé plus proche des attentes finales des consommateurs, ou comme certains assouplissements réglementaires visant à une meilleure mise en concurrence de produits et services financiers (dans les paiements, dans l’épargne, etc.). Par exemple, la DSP2 (deuxième directive sur les services de paiement) facilite l’entrée dans l’activité de paiement de tout un ensemble d’entreprises pour lesquelles seuls un agrément d’établissement de paiement est suffisant auprès de l’ACPR. Longtemps considérées comme évoluant sur un marché non contestable, les banques se trouvent donc aujourd’hui bousculées par l’arrivée des FinTechs. Ces dernières pourraient menacer à terme les banques sur un des fondements de leur solidité : les dépôts. En effet, agrégateurs et initiateurs de paiement favorisent une moindre stabilité des dépôts. En outre, certains d’entre eux évoluent d’ores et déjà vers un statut de néo-banque et donc une captation des dépôts jusqu’alors présents dans les réseaux traditionnels. Par ailleurs, en France, la loi Macron facilite énormément le transfert du compte courant d’un client d’une banque à une autre et va renforcer cette contestabilité. Ainsi, attaquée sur ses parts de marché traditionnelles, pénalisée sur sa structure de coûts, fragilisée sur la question de la rentabilité, l’industrie bancaire doit se réinventer en profondeur afin de répondre à l’évolution des besoins et des attentes de ses clientèles et de ses investisseurs. Son salut réside dans l’innovation financière et l’établissement de partenariats technologiques qui repositionnent le client au centre de toutes les attentions. Adapter les passifs bancaires Face à ces enjeux, et depuis plusieurs années, Vertuo Conseil, en collaboration avec la House of Finance de l’université Paris-Dauphine, a initié un programme de recherche appliquée dédié aux stratégies d’adaptation des passifs bancaires. Ce programme se décline en six volets dont trois sont synthétisés dans ce numéro. Avec, d’un côté, une innovation financière tournant à marche forcée pour assurer l’émission de produits éligibles au refinancement bancaire, nous nous proposons d’apporter un éclairage sur la réaction des marchés à l’émission des obligations Contingent Convertible ainsi qu’une compréhension affinée des déterminants du spread lors de la « commercialisation » de titres tels que les Covered Bonds. Cette approche par la dette se complète d’une analyse des mécanismes de collecte bancaire, que pourraient venir mettre à mal les perspectives de développement intensif des FinTechs, ce qui consiste donc notre troisième décryptage. Autant de réflexions qui laissent augurer une longue et profonde mutation des business models et de la structure des passifs bancaires. Hervé ALEXANDRE, Professeur, Directeur du Master Banque et Finance, Membre de la House of Finance &#8211; Université Paris Dauphine David ALCAUD, Coordinateur R D, Groupe Square Management Adrien AUBERT, Senior Manager du Cabinet Vertuo Conseil
63	http://www.vertuoconseil.com/portfolio/economie-collaborative-origine-legislation/	 Economie collaborative : origine et législation Mathilde. Taillez 2017-02-12T12:50:29+00:00 Project Description Le Journal du Net – 4 novembre 2016 : Depuis plusieurs années, des entreprises innovantes à internationalisation rapide apportent sur le marché des innovations disruptives. Si la plupart d’entre elles sont d’ordre technologique, l’une s’est imposée comme un nouveau modèle économique : l’économie collaborative (ou participative). Cela fait des décennies que nous parlons d’internationalisation, de mondialisation ou de globalisation. Il est même envisageable de considérer cette expansion comme inhérente à l’évolution humaine : de tout temps, l’Homme étend son territoire, se regroupe en communautés de plus en plus grandes, de plus en plus organisées. Les nouvelles technologies, la standardisation des procédés et l’abaissement des barrières des marchés ont rapetissé le monde. L’internationalisation n’est plus seulement l’apanage des sociétés fortement implantées sur leur territoire national mais s’offre à toute entreprise ambitieuse. De nouvelles sociétés audacieuses se lancent dès leur naissance vers des marchés extérieurs. Pour elles, la notion de frontière est relative. Ce qui importe c’est que les marchés ciblés soient semblables au marché domestique. Ces entreprises à internationalisation rapide sont surnommées les « Born Global ». Les difficultés que rencontrent ces « Born Global » sont grandes : ils doivent opérer dans différentes langues, s’adapter à la culture de chaque pays, être conformes aux législations internationales et locales et être flexibles dans un contexte économique mouvant. En réponse aux récentes crises économiques, climatiques et sociales sans précédent à une échelle mondiale, certains « Born Global » sont à l’orée d’un nouveau mode de consommation : ils ont inventé l’économie collaborative. Ils permettent de mettre rapidement en contact des particuliers qui souhaitent échanger des biens ou services. C’est le cas notamment d’Airbnb, VoulezVousDiner ou BlablaCar. Cette économie, parfaitement adaptée au contexte de récession actuel, est en pleine croissance : selon le cabinet PwC, le chiffre d’affaires mondial du secteur est en phase de connaître une évolution de plus de 1 400 % sur 12 ans en passant de 20 milliards (en 2013) à plus de 300 milliards d’euros d’ici 2025. En France, le marché représente actuellement 3.5 milliards d’Euros. Les régulateurs y voient ainsi une opportunité et souhaitent mieux le réglementer. Ces start-ups du collaboratif à l’orée d’une nouvelle législation Si le système actuel ne présente pas de vide juridique en tant que tel, il faut reconnaître qu’il n’est pas prévu pour encadrer cette nouvelle forme d’activité économique : les particuliers éprouvent des difficultés à identifier les revenus à déclarer ou non à l’administration fiscale. En effet, seuls les revenus relevant du partage de frais ne sont pas imposables. Aujourd’hui par exemple, les revenus perçus via le site BlablaCar ne sont pas imposables tant que l’opération de covoiturage relève de la co-consommation. Ils le sont en revanche s’ils représentent un moyen de s’enrichir (opération commerciale). A ce jour, l’administration fiscale n’est pas capable d’identifier les revenus provenant de l’économie collaborative mais des mesures sont prises pour pallier ce manquement. Par exemple, depuis fin 2015, le site Airbnb collecte lui-même – et reverse aux mairies de certaines grandes agglomérations – la taxe de séjour pour le compte de ses hôtes. Dans le même sens : la loi de finance de 2016 demande aux sites relevant de l‘économie participative d’informer leurs utilisateurs des sommes qu’ils doivent déclarer à l’état – sachant que le montant à déclarer dépend de plusieurs facteurs dont la nature des sommes et leur périodicité. L’objectif est de mieux informer pour inciter les déclarants à être exhaustifs quant à leurs sources de revenus : un sondage de TNS Sofres datant de 2014 montre que seuls 15 % des sondés déclarent, ou ont l’intention de déclarer, à l’administration fiscale les recettes provenant de l’économie participative. Ces mesures permettront d’augmenter le nombre de déclarants. Malgré la mise en place de ces règles, cette nouvelle économie échappe toujours en partie à l’Etat. Des travaux sont en cours pour y pallier et c’est dans ce sens que Pascal Terrasse (député de l’Ardèche) a écrit un rapport début 2016. Celui-ci pose les grandes lignes pour une fiscalité équitable tout en y apportant un meilleur cadre réglementaire. Concernant l’imposition des usagers, la 19ème préconisation du rapport stipule que « sans aller jusqu’à prendre en charge la collecte de l’impôt, les plateformes qui ont connaissance des revenus dégagés par leurs utilisateurs pourraient en communiquer les montants aux administrations sociales et fiscales en vue de fiabiliser les déclarations des contribuables ». Quel avenir pour le secteur ? Alors que le manque de réglementation permettait à ces start-ups d’imaginer leur marché et de créer leurs propres règles, elles sont en passe aujourd’hui de disposer d’un cadre réglementaire. Cela représente une contrainte supplémentaire à laquelle elles doivent s’adapter. Toutefois ce rempart législatif bénéficiera aux entreprises traditionnelles – concurrentes des plateformes collaboratives – qui joueront désormais à armes égales avec elles. La SNCF par exemple, pour lutter contre la menace que représente pour elle le site Blablacar, a développé son offre de voyages en bus dotés de wifi et a racheté le site 123envoiture.com. Ce nouveau modèle économique créé par ces « Born Global », qui y ont vu une nouvelle manière de consommer à une échelle mondiale, redonne du pouvoir aux usagers. Ils ne réinventent pas l’économie mais offrent une alternative crédible aux entreprises traditionnelles en supprimant au maximum les intermédiaires entre l’offre et la demande. Aujourd’hui, ce modèle fonctionne surtout sur la thématique de l’ « échange de biens ou services ». Il est envisageable que, demain, tous les secteurs soient impactés (l’énergie ou la grande distribution pour en citer quelques-uns). L’évolution des cadres juridiques et fiscaux est à venir et les régulateurs devront être flexibles. Enfin, l’enjeu principal pour les Etats sera de veiller à la bonne application du cadre juridique. Par Rémi Joffre consultant du cabinet VERTUO conseil
64	http://www.vertuoconseil.com/portfolio/derriere-proces-a-lindustrie-auto-allemande-realite-brutale-lobligation-de-cofinancer-revolution-marche/	 Un cartel industriel, et alors ? Mathilde. Taillez 2017-09-23T09:43:15+00:00 Project Description Interview d Adrien AUBERT, Senior Manager du Cabinet Vertuo Conseil, réalisée par Jean-Luc MOREAU (RMC) le 1er septembre 2017 : Pour écouter le podcast, cliquez ici. Avec le scandale du soi-disant cartel automobile allemand, le grand public redécouvre le principe des ententes sectorielles qui ont marqué ces dernières années le monde des télécommunications ou de l’agro-alimentaire et de la grande distribution. Un nouveau dossier sur les émissions polluantes des automobiles ne pouvait pas trouver un meilleur tempo médiatique que la période actuelle marquée par la menace de 6ème grande extinction animale, l’urgence climatique sur fond géopolitique pour le moins complexe ou les annonces de plusieurs gouvernements européens (Norvège, France et… Allemagne tiens tiens !) d’interdire la commercialisation de voitures à moteur thermique à l’horizon 2030-2040. Face à la complexification des normes, plusieurs industriels allemands auraient convenu ensemble des procédés techniques à mettre en place pour respecter ces seuils réglementaires de la façon la plus efficace (comprendre la moins coûteuse à produire ou à acquérir auprès d’équipementiers spécialisés). En l’absence de documents consultables à ce stade de l’affaire, difficile d’évaluer précisément le niveau d’illégalité d’une telle démarche. Reste à considérer qu’elle n’a absolument rien de nouveau : de telles conventions existent depuis longtemps dans le secteur automobile sur un large panel d’activités. Car cette forme de standardisation, qui est le socle des économies d’échelle tant recherchées par une industrie capitalistique en quête de rentabilité, ne nuit pas nécessairement au consommateur final. Au contraire, elle s’inscrit dans un pur cadre de recherche   développement qui doit apporter toujours plus de valeur ajoutée au produit final (parfois même au détriment de la différenciation des marques). C’est vrai pour les dispositifs de cartographie routière, de navigation et de géolocalisation (au moins jusqu’à il y a quelques mois). C’est vrai dans l’exploitation de la fibre de carbone pour tenter de lutter contre l’obésité galopante de voitures toujours plus confortables, sécurisées   connectées. C’est vrai pour la conception et l’industrialisation de boîtes de vitesses toujours plus sophistiquées et efficaces sur le plan de la consommation de carburant. C’est vrai dans la fabrication de composants d’habitacles ou de fonctionnalités devenues standard (compresseur de climatisation). C’est vrai jusque dans l’outil industriel dont la flexibilité permet aujourd’hui de produire plusieurs voitures différentes à partir d’un même châssis. A partir de là, pourquoi pas des équipements de dépollution si les exigences réglementaires sont respectées ? Il est intéressant de faire un parallèle avec d’autres industries. Par exemple comment se comportent nos banques françaises lors de la publication de nouvelles normes réglementaires ? Après l’action habituelle du lobby pour tenter d’aménager les textes de lois dans leur version la moins défavorable, démarre une phase d’interprétation des textes afin d’adapter les processus internes et les systèmes d’information en conformité avec les dispositions en vigueur. Peut-on imaginer une seule seconde que les fleurons de la finance européenne puissent engager des fortunes dans la conduite de projets complexes (amélioration des normes comptables, lutte contre la fraude fiscale, prévention des risques systémiques, etc.) sans un minimum de concertation entre les différents acteurs impactés ? Il en va de la cohérence même de la mise en œuvre d’une norme qui par définition cherche à établir une pratique a minima. Ainsi, le déploiement progressif de la voiture autonome sur nos routes constitue autant un défi technologique qu’un challenge réglementaire qu’un constructeur seul ne peut adresser : à nouveau c’est la place dans son entièreté qui se mobilise pour établir des conventions. Avec un Dieselgate impliquant toujours davantage de constructeurs depuis maintenant près de 2 ans, la traque quasi-paranoïaque de manipulations de grande ampleur, grande obsession médiatique et large pourvoyeuse de clics lucratifs, doit plutôt nous amener à nous interroger sur le nomadisme d’une civilisation sédentaire qui n’a jamais été autant connectée par les réseaux numériques et donc à nous interroger sur la place que la société est prête à concéder à l’automobile. Une automobile qui aura toujours besoin d’énergie pour se mouvoir : qu’elle soit fossile, solaire, électro-hydraulique, hydrogène ou autre, elle ne fait au final que relever d’un arbitrage entre les coûts et les émissions, un arbitrage politique auquel les constructeurs eux-mêmes ne semblent pas souvent conviés. Jusqu’à ce qu’un cartel soit dévoilé ? Pour écouter le podcast, cliquez ici. Par Adrien AUBERT, Senior Manager du Cabinet Vertuo Conseil
65	http://www.vertuoconseil.com/portfolio/les-back-offices-des-banques-face-a-de-nouveaux-enjeux/	 Les back offices des banques face à de nouveaux enjeux VERTUO 2017-01-12T11:26:58+00:00 Project Description L’activité bancaire est une des premières à avoir inventé une forme de dématérialisation par l’utilisation du papier comme mode de paiement dès le VIIème siècle. L’emploi d’effets de commerce permettait de sécuriser et de simplifier le paiement à des bénéficiaires éloignés géographiquement. Certains de ces moyens de paiement sont d’ailleurs toujours utilisés de nos jours. Les établissements bancaires n’ont cessé de dématérialiser leur activité depuis lors. La carte bancaire ou le paiement en ligne sont des exemples de notre quotidien qui illustrent cette dynamique. Aujourd’hui, le développement des technologies digitales entraîne des changements importants dans le secteur bancaire et donne lieu à la mise en place de nouvelles formes de numérisation. Dans ce contexte, où les systèmes informatiques jouent une nouvelle fois un rôle clé, les outils utilisés par les back-offices bancaires doivent répondre à de nouveaux enjeux… L’automatisation des traitements back office Le back-office bancaire est chargé du traitement administratif des opérations instruites par les agences bancaires, les centres d’affaires ou, dans certains cas, directement par les clients des banques eux-mêmes. Ils sont principalement chargés de réaliser des contrôles de conformité par rapport à la réglementation et la politique de l’établissement bancaire et de l’enregistrement de l’opération dans les livres comptables et extra comptables de l’établissement. Développés dès les années 1970-1980 les systèmes opérations utilisés par ces équipes permettent d’automatiser et de sécuriser une partie des actions et, notamment, d’envoyer les données nécessaires aux systèmes comptables et de gestion du risque de la banque. Aujourd’hui, pour répondre à de nouveaux besoins nés de la technologie et à une concurrence de plus en plus présente et diversifiée, les banques doivent s’adapter et s’engagent dans une transformation dans laquelle les outils informatiques jouent naturellement un rôle clé. Un secteur en pleine transformation Les utilisateurs de services bancaires, à l’image de l’acronyme IKWIWAIWIN (I Know What I Want And I Want It Now) sont plus connectés, autonomes, exigeants et zappeurs que jamais. Ils demandent plus d’instantanéité dans l’accès à l’information ou le traitement de leurs opérations. Aussi les banques se sont-elles engagées dans une modernisation de leur offre de services. Deux illustrations de ce mouvement peuvent être citées : l’arrivée des banques en ligne dès le début des années 2000, par exemple, ING Direct ou Fortuneo et le développement des services bancaires en ligne pour les particuliers et les entreprises ; la digitalisation des agences bancaires comme le montre l’ouverture d’une agence digitale innovante Caisse d’Epargne à Metz en avril 2015. Par ailleurs, l’évolution du champ d’application des technologies crée des opportunités pour de nouveaux entrants. L’apparition de services bancaires « désintermédiés » et innovants offerts par des Fin Tech tels que Kisskissbankbank ou Leetchi en sont des exemples. L’entrée d’Orange sur le secteur par le rachat récent de Groupama Banque vient confirmer cette tendance. Ces nouveaux acteurs sont souvent plus agiles et moins contraints par la législation. Ils captent la clientèle bancaire en se démarquant par une réponse nouvelle à des besoins existants en offrant des services simples, innovants et peu coûteux. Les acteurs traditionnels doivent ainsi faire face à une concurrence montante et réactive qui écrit une vision nouvelle du modèle bancaire. Les systèmes d’opérations doivent évoluer pour répondre aux nouveaux enjeux du marché Les systèmes opérations jouent un rôle clé dans la capacité des banques à faire face aux enjeux actuels du secteur. Comme le relève l’Observatoire des métiers de la banque, l’impact du numérique sur la banque concerne en grande partie les processus opérationnels. Les systèmes opérations sont au cœur de ces processus. Ils doivent ainsi donner aux équipes opérationnelles la capacité à alimenter les offres de services des banques. Ce qui permet à ces dernières de se mettre à niveau de ou de se différencier de la concurrence. En conséquence, les besoins d’automatisation, de sécurisation, de traçabilité et de rapidité des traitements se renforcent-ils afin de pouvoir améliorer l’efficacité opérationnelle des équipes. Les systèmes opérations doivent permettre aux équipes back-office de garantir un traitement plus rapide des opérations et alimenter en temps quasi réels les différents canaux de communication dont les portails en ligne et les applications mobiles. Néanmoins, les traitements des équipes back-office doivent aussi respecter des procédures opérationnelles lourdes pour garantir la sécurité, la confidentialité et la conformité de l’opération concernée. Acteurs clés de l’activité économique, les banques, qui offrent un panel de services complet à leurs clients contrairement à la plupart des Fin Tech, doivent respecter réglementations nationales et internationales complexes et en constantes évolution et renforcement. Ainsi, les banques doivent-elles d’une part offrir un service instantané à une clientèle volatile et d’autre part renforcer la sécurité de leurs opérations. Afin de limiter le temps de traitement des équipes, les systèmes opérations doivent permettre d’automatiser aux mieux les processus. Le STP ou « Straigh To Processing », qui désigne les processus de traitement sécurisé et automatisé sans rupture de charge, se généralise au sein des établissements bancaires. Les systèmes STP permettent de supporter un processus opérationnel en flux tendu depuis l’instruction de traitement et ainsi de garantir une réponse rapide à la demande client tout en respectant des exigences de sécurité. Aussi, les systèmes opérations doivent gérer une grande complexité en s’interfaçant avec l’ensemble des systèmes de la chaîne de traitement et en respectant différentes normes d’échange : depuis l’instruction multi canal : via le chargé clientèle ou d’affaires, via un service de banque en ligne ou une application mobile, via e-mail ou via un système de communication interbancaire; aux référentiels produit, clients, comptes, ainsi qu’aux différents systèmes de gestion des lignes de crédit ou de contrôle de la conformité de l’opération ; aux systèmes d’enregistrement comptable dans les livres de la banque et les systèmes de gestion et de reporting de risque ; au compte-rendu d’opéré transmis en mode multi canal. Par ailleurs, le contexte actuel demande à ces systèmes d’être capables de gérer une grande flexibilité. En effet, la concurrence accrue demande aux banques de valoriser l’expérience client et ainsi de lui répondre de façon personnalisée. Les systèmes opérations doivent en outre: permettre de gérer des processus opérationnels répondant à un traitement ciblé pour différentes typologies de clients ; alimenter les systèmes de données et notamment les systèmes « big data » qui permettent d’analyser les comportements des clients et ainsi de mieux les connaître pour mieux les servir. Enfin, les systèmes opérations doivent être évolutifs. Ils doivent permettre de s’adapter promptement aux évolutions du marché par leur capacité à gérer un nouveau canal de communication ou évoluer rapidement pour permettre le traitement de nouvelles offres ainsi que de s’adapter rapidement aux évolutions réglementaires. L’informatique et plus particulièrement les systèmes opérations sont aujourd’hui un des piliers de l’activité bancaire. Les évolutions technologiques sont au cœur des enjeux de transformation des banques et du panel de services qu’elles peuvent offrir pour répondre aux besoins de leurs clients et pour faire face à une nouvelle concurrence qui déploie des services simples, rapides, intuitifs… bref, innovants… Les établissements bancaires ont l’atout d’être les acteurs historiques et incontournables du secteur. Ils doivent maintenant renforcer leur position en prenant le tournant technologique. Par Noémie Pegon Project Manager du cabinet Vertuo Conseil
66	http://www.vertuoconseil.com/portfolio/savoir-evaluer-qualite-de-donnees/	
67	http://www.vertuoconseil.com/portfolio/loptimisation-couts-consequence-collaterale-lean-six-sigma-entreprise/	
68	http://www.vertuoconseil.com/portfolio/lutte-contre-la-corruption-la-loi-du-minimum/	
69	http://www.vertuoconseil.com/portfolio/decryptage-revue-fondamentale-trading-book/	 Décryptage &#8211; La Revue Fondamentale du Trading Book Mathilde. Taillez 2017-07-05T09:53:05+00:00 Project Description Bien que la crise des subprimes trouve son origine dans le risque de crédit, c’est le trading book des banques qui fut le plus impacté par les pertes occasionnées lors de la crise financière de 2008. Face à ce constat, la réglementation a réagi en décidant, dès 2009 (Bâle 2.5), de la mise en place de nouvelles mesures permettant d’appréhender, au mieux, les risques contenus au sein du trading book. Si ces nouvelles mesures sont entrées en vigueur dans un but de solutionner rapidement les défaillances observées dans le calcul de fonds propres au titre du risque de marché, elles ont entrainé une fragmentation des méthodes de calcul pouvant causer des écarts de fonds propres prudentiels. Avec la Revue Fondamentale du Trading Book (RFTB), l’objectif est désormais de dresser un cadre réglementaire cohérent permettant de mesurer, de la manière la plus juste possible, les risques encourus par l’activité de négociation des banques afin d’en déduire les montants de capitaux propres nécessaires pour y faire face. Lire la suite     Par Juliette GUERIN, Consultante du Cabinet Vertuo Conseil
70	http://www.vertuoconseil.com/portfolio/reglementation-sts-permettra-t-nouvel-elan-marche-de-titrisation/	 La réglementation STS permettra-t-elle un nouvel élan du marché de la titrisation ? Mathilde. Taillez 2017-02-12T12:05:03+00:00 Project Description Les Echos – 26 janvier 2017 : Le texte réglementaire relatif à la titrisation STS (Simple, Transparente, Standardisée) sera voté au Parlement Européen début 2017. À l’heure où les différents amendements sont débattus entre les membres du parlement, il paraît difficile de prédire le succès de ce nouveau projet réglementaire. Ne plus revivre 2008 La titrisation, ce mécanisme financier qui consiste à transformer des actifs (prêts, créances…) en titres négociables sur les marchés, fut, pour beaucoup, à l’origine de la crise financière de 2008 : l’opacité des pools d’actifs titrisés ayant contribué à la mauvaise appréhension du risque par les investisseurs. Aujourd’hui, le marché européen de la titrisation est loin des standards de l’époque : il est passé de 819 milliards investis en 2008 contre moins de 200 milliards d’émissions en 2015, ce qui montre la méfiance des investisseurs à l’égard des opérations de titrisation. Toutefois, dans un contexte économique morose, la titrisation apparaît être un outil adéquat à la relance économique et à l’investissement. En effet, elle représente un outil de déconsolidation, car une fois l’actif sorti du bilan, les établissements de crédit peuvent à nouveau financer l’économie réelle facilitant l’accès au crédit et au passage améliorer leurs ratios prudentiels. De plus, grâce à la titrisation les banques peuvent se refinancer à moindre cout leur permettant par la suite de financer à nouveau l’économie. C’est dans cette optique que la Commission Européenne souhaite utiliser, et redorer par la même occasion la titrisation, mais pas à n’importe quel prix. Réglementer la titrisation entre dans le plan d’action visant à construire l’Union des Marchés des Capitaux pour 2019, projet qui vise à créer une meilleure intégration financière au sein de l’Union européenne. La réglementation STS a donc pour objectif d’éviter de nouvelles dérives, telles que l’asymétrie d’information ou l’aléa moral, semblables à celles qui ont lieu en 2008, pour permettre de regagner la confiance des investisseurs. Avant que ne soit voté le texte au Parlement Européen, certains amendements sont actuellement sujets à débat. Un cadre réglementaire renforcé suscitant de nombreuses interrogations Le texte actuel prévoit d’augmenter la responsabilité du cédant sur le risque de crédit des actifs via l’obligation de rétention économique, soit la part que le cédant doit conserver, qui devrait passer de 5% à 20%. Cette mesure paraît légitime et appropriée seulement pour les produits dont les taux de défaut sont importants, tels que les prêts hypothécaires commercial (Commercial Mortgage Backed Securities aux Etats-Unis) ou encore la titrisation synthétique, mais moins pour des produits pour lesquels les taux de défaut sont quasi nuls, l’Auto ABS par exemple. Autre point clé, l’attribution du label STS. La tendance n’est pas de déléguer cette fonction à une autorité compétente, mais il incombera principalement au cédant « d’auto certifier » l’opération par l’interprétation de la réglementation. Des manquements aux critères STS entraîneront des sanctions pouvant aller jusqu’à 5 millions d’euros. Il paraît alors difficile de croire que les investisseurs auront une confiance aveugle en la qualité de l’opération en l’absence d’un tiers certificateur compétent, délivrant la certification STS. En parallèle du projet STS, la révision du projet réglementaire CRR, Capital Requirements Reglementation, se joue également en ce moment, puisque les amendements au texte seront bientôt proposés par la Commission Européenne. Le texte prévoit notamment le passage des exigences de capital pour les investisseurs en titres seniors AAA de 7 à 10 % si ces derniers sont issus d’une titrisation STS et à 15 % si la titrisation n’est pas STS. Cette tendance fera sans doute réfléchir les investisseurs tels que les institutions bancaires avant d’investir dans ce type d’opérations, mobilisant davantage de capitaux et impactant le bilan pour des produits performants. Ajoutés à cela un critère de concentration, c’est-à-dire le poids des créances dans le portefeuille, limité pour l’instant à 2%, la centralisation des données relatives aux pools d’actifs sur une plate-forme commune ou encore l’obligation de démontrer que l’opération bénéficie à l’économie réelle, qui viennent alourdir un cadre réglementaire et opérationnel déjà exigeant. En conclusion, si la réglementation STS paraît indispensable à la relance du marché de la titrisation, elle pourrait cependant déboucher sur un cadre législatif trop strict et trop générique lorsqu’on sait que le taux de défaut des produits structurés européens n’a pas dépassé 1,5% et que des produits tels que les Autos ABS connaissent des taux de défaut quasi nuls. Il conviendrait peut-être d’adapter la réglementation en fonction du sous-jacent. Si les tendances se confirment, il est possible que la réglementation vienne diminuer l’appétit des investisseurs pour des produits déjà performants et « surs » en les rendant moins attrayantes via des coûts supplémentaires, une structuration plus dense et plus longue. D’un autre côté, elle favoriserait les opérations de titrisations sur des produits dits plus risqués tels que les RMBS/CMBS. Par Ritchie Vellien consultant du cabinet VERTUO conseil
71	http://www.vertuoconseil.com/portfolio/trump-reculer-mieux-chuter/	
72	http://www.vertuoconseil.com/portfolio/square-idea-3/	
73	http://www.vertuoconseil.com/portfolio/la-performance-extra-financiere-est-en-roues-libres/	 La performance extra-financière est en roues libres VERTUO 2017-02-12T13:21:18+00:00 Project Description Les Echos – 24 février 2016 : La performance financière n’est plus l’unique paramètre d’évaluation économique. Le triptyque « ESG » (Environnement, Social, Gouvernance) donne les trois dimensions de la performance extra-financière, qui devient peu à peu un critère incontournable. Aussi ambitieux que minimaliste, le droit n’impose rien aux agences de notations spécialisées, alors que des questions de fonds demeurent et que d’importantes opportunités émergent. En partenariat avec Sustainalytics, la société d’analyse financière Morningstar va publier les évaluations « ESG » des fonds d’investissement qu’elle suit. Prévues pour les professionnels à fin 2015, ces nouvelles évaluations vont concerner tous les fonds analysés par Morningstar : chacun d’eux aura cette notation complémentaire. C’est un changement d’échelle, qui valide l’importance économique prise par l’extra-financier chez les investisseurs. Rapporter : la loi du minimum Il n’y a pas de régime juridique disponible pour évaluer le degré d’éthique d’une entreprise. Les référentiels disponibles sur le sujet sont surtout déclaratifs. Au niveau international, les Principes pour l’investissement responsable portés par les Nations Unies depuis 2006 et les Principes de l’Equateur proclament des objectifs environnementaux et sociaux. D’autres textes existent dans cette veine. Plus directe sur le sujet, l’Union européenne a voté la Directive 2014/95/UE, applicable en décembre 2016, qui contraint les entreprises cotées et celles de plus de 500 salariés à une obligation de rapportage extra-financier. La France avait été pionnière en légiférant dès 2001, puis en 2010 à la suite du « Grenelle 2» de l’environnement. L’obligation d’informer est donc la seule règle juridiquement opposable en matière extra-financière. Et encore, il s’agit presque d’une règle de forme, puisque la Directive européenne renvoie aux textes internationaux pour déterminer les informations de fond et n’oblige qu’à décrire la politique menée et les résultats atteints. La coquille juridique étant vide de contraintes, il n’y a de facto qu’un exercice de communication. A chacun sa méthode Ces reportings obligatoires composent une large partie de la matière analysée par les agences de notation extra-financière. Celles-ci ne disposent que de l’information publiée par les entreprises, complétées de questionnaires, de rencontres, d’audits et de partages d’information avec d’autres parties prenantes, comme des ONG ou des syndicats. Ces agences certifient la conformité extra-financière. En fournissant la légitimité « ESG » des produits et des entreprises, elles créditent ainsi leur réputation. La performance extra-financière est l’un des véhicules de l’image de marque d’une organisation : c’est un argument marketing qu’il faut gérer. Aucun référentiel universel n’existe à ce jour : chaque agence organise sa notation en déterminant les critères et leur pondération. Par ailleurs, elles ne sont pas contrôlées par des acteurs publics à même de garantir leurs méthodes et la fiabilité de leurs évaluations, ce qui ajoute à la subjectivité de leur modèle. Une tentative de standard européen En 2004, un standard européen volontaire a été mis en place entre plusieurs agences européennes, pour se donner des principes méthodologiques communs (Voluntary Quality Standard CSRR-QS 2.1). Cette standardisation de la méthodologie a été mise à jour en 2012, avec le standard méthodologique ARISTA 3.0. Or, si cette démarche d’alignement méthodologique est louable, elle n’a que peu de portée : l’audit de validation méthodologique n’est pas rédhibitoire et seuls quelques acteurs de la Place européenne y ont souscrit. On peut aussi s’étonner qu’il n’y ait pas une extension de la démarche d’audit aux processus de notation appliqués. Se limiter à revoir l’application de principes dans la méthodologie de notation des agences est trop réduit pour justifier d’une qualité finale de notation. Ce standard n’apporte aucune contrainte, ce qui permet aux agences d’évaluer comme elles le souhaitent. Le carbone : la bouffée d’air du secteur Malgré un marché européen des émissions de carbone étouffé par un prix très bas de la tonne (environ 8€), la performance dans les rejets de carbone est devenu un facteur clef de succès pour toute stratégie de responsabilité sociale et environnementale. Nouveau baromètre de l’air planétaire, cette empreinte carbone est pleinement intégrée au modèle économique des agences de notation, qui ont acheté des agences spécialisées pour ajuster leur offre, comme par exemple l’agence norvégienne Point Carbon A/S rachetée en 2010 par Thomson Reuters. L’économie décarbonnée est une véritable opportunité de croissance pour ces agences, qui bénéficient de référentiels internationaux aboutis tant sur la comptabilisation et la déclaration des émissions (Green House Gas Protocol), que sur le cadre de reporting avec le Climate Disclosure Standards Board. Ces outils sont disponibles et le marché va croître à mesure que la sensibilisation au danger des émissions de gaz à effet de serre se déploie. Les accords de Paris suite à la COP 21 vont intensifier le besoin d’accompagnement vers la réduction des émissions de CO2, la mesure de celle-ci et la communication qui la valorisera. La crise financière de 2007-2008 a remis en question le modèle des agences de notation puisque l’évaluation des acteurs avait été défaillante. La confiance était mal placée et mal organisée. Désormais soumises à des obligations de surveillance en Europe et aux Etats Unis, celles-ci présentent une gouvernance assainie (prévention des conflits d’intérêts, régimes de contrôles, transparences de méthodologies de notation, etc.). En roues libres dans leurs pratiques, les agences de notation extra-financière doivent être encadrées ou mises sous la tutelle d’une institution internationale de référence. C’est un enjeu de crédibilité pour leur développement à venir. Par Pierre Theobald, Consultant Senior chez Vertuo Conseil
74	http://www.vertuoconseil.com/portfolio/le-brexit-ou-lart-de-faire-comme-avant/	 Le Brexit ou l’art de faire comme avant VERTUO 2017-02-12T13:00:03+00:00 Project Description Les Echos – 23 août 2016 : Aberrant, l’article 50 du Traité de Lisbonne abandonne le processus juridique de sortie de l’Union européenne au sortant. L’empressement n’est pas d’actualité : les haut-le-cœur politiques de la rupture étant passés, c’est maintenant l’heure de conserver patiemment les intérêts britanniques. L’Union européenne attendra : elle l’a voulu. L’agenda de ces dames Si les dirigeants des institutions européennes répètent en boucle combien le vote britannique est irréversible et le marché commun un tout non négociable, Londres et Berlin ont toutefois une lecture commune du contexte. Leurs économies sont très liées et leurs agendas politiques internes les poussent à une communauté d’intérêts. Theresa May va matérialiser le rêve des « Brexiters », mais elle hérite d’un Royaume-Uni en recomposition politique et dont la souveraineté économique est incertaine. Fonder une nouvelle stratégie commerciale est la mère de toutes ses politiques. Pour ce faire, elle a besoin de temps pour s’organiser et chercher de nouvelles perspectives aux échanges commerciaux britanniques. L’avantage procédural de l’article 50 est précaire : une fois activé, le rapport de force va s’inverser en faveur de Bruxelles. Le Royaume-Uni n’est pas encore prêt et aucune des options n’est claire, alors Theresa May donne du temps au temps. On aurait tort de la critiquer : elle applique le droit européen. Cette tactique arrange aussi sa consœur chancelière. Angela Merkel est tombée de son piédestal : sa politique migratoire lui a coûté sa popularité et son aile droite. L’influence de « Mutti » sur la « Mitteleuropa » a été élimée par les murs et les barbelés qui ont provoqué l’effondrement de facto des accords de Schengen. L’Europe enchaîne les crises, l’Allemagne a des problèmes sécuritaires et les Allemands voteront en 2017. Alors, plus que jamais à la croisée des routes européennes, Angela Merkel ne peut être considérée, une nouvelle fois, comme celle qui déstabilise le continent. Fuis-moi, je te suis… Le populisme simpliste de la campagne s’est arrêté net sur la réalité géoéconomique : dans un monde enchevêtré d’accord commerciaux, on ne peut demeurer seul. Le principe de la séparation politique voté vient le temps de la continuité économique. Celle-ci va s’exercer en deux temps : lors des négociations où Londres aura pleinement accès au marché unique, puis après la séparation juridique avec l’Union européenne, où l’accès à ce marché est à négocier. Être un État tiers (c’est-à-dire sans accord commercial avec l’Union européenne) ou négocier des accords sectoriels serait les deux options les plus incertaines pour l’économie britannique. L’ampleur du commerce avec les pays européens est telle que le Royaume-Uni serait perdant. L’AELE (Association européenne de libre échange) est une autre option. Le retour de Londres dans cette organisation poserait deux problèmes. Celui du fonctionnement, puisque les décisions y sont prises à l’unanimité de ses membres (Norvège, Islande, Liechtenstein et Suisse), et celui des enjeux : la taille de l’économie britannique déséquilibrerait toute l’organisation. Liés à l’Union européenne dans un « Espace économique européen », où circulent librement biens, services, personnes et capitaux, ces pays appliquent en retour le droit européen. Quitter l’Union européenne pour appliquer sa règlementation dans une autre organisation : voilà une option indéfendable ! Reste l’exception suisse. Membre de l’AELE, la Confédération a privilégié des accords bilatéraux plutôt que d’adhérer à cet espace. Ce libre échange sur mesure serait une option réaliste pour Londres : être dehors, tout en gardant assez de liens dedans… D’ici là, il est urgent d’attendre ! Le soutien allemand et les échéances électorales de 2017 en France et en Allemagne nourrissent cet attentisme. Pêché d’orgueil d’une Union qui se voulait un horizon indépassable pour tout un continent, l’article 50 du Traité de Lisbonne produit une incertitude stérile qui discrédite des institutions et un projet européen dépassés par les crises successives. Par Pierre Theobald, Consultant Senior chez Vertuo Conseil
75	http://www.vertuoconseil.com/portfolio/les-gafa-et-autres-fintechs-ne-condamneront-pas-les-banques/	 Les GAFA et autres FinTechs ne condamneront pas les banques VERTUO 2017-02-12T13:29:49+00:00 Project Description Les Echos – 16 décembre 2015 : Les Cassandres, qui prédisent la disparition prochaine de ces dinosaures que seraient les banques, sous-estiment la place qu’elles occupent dans l’actuelle transformation de l’industrie financière ainsi que les fondamentaux métiers. Il y a 15 ans, tout le monde condamnait la banque « traditionnelle » à disparaître avec l’émergence des banques en ligne. En France, bien que nous soyons légèrement en retrait des tendances observées dans certains pays d’Europe, seulement 7 % des Français possédaient un compte en ligne à fin 2014. Non, les banques ne vont pas disparaître sous les coups de boutoir des nouveaux acteurs qui tentent de se faire une place au paradis des services financiers et produits bancaires. Une banque, c’est une structure dédiée à l’expertise financière, à la connaissance des produits, des services, des opérations et des flux qui assurent le financement en temps réel de l’économie. Cette expertise unique s’appuie sur des fonctions supports (gestion des risques, normes comptables, etc.) soumises à un package réglementaire d’une complexité qui fait référence. Qui peut réellement en dire autant ? Banques et nouveaux acteurs : concurrence frontale ou opportunités de croissance ? Il est vrai que de nouveaux acteurs économiques sont venus enrichir l’écosystème de l’industrie bancaire. Plus précisément, il s’agit de trois typologies d’acteurs. En premier lieu, les fameux GAFA (Google, Apple, Facebook, Amazon). Prenons l’exemple d’Amazon, aujourd’hui numéro 2 mondial du e-commerce derrière le géant chinois Alibaba, qui propose une offre de financement (Amazon Lending) à ses clients PME utilisant la Marketplace Amazon. Cette offre s’inscrit dans une volonté d’accompagner ses clients sur toute la chaîne de valeur et de faire du cross-selling, plus qu’une volonté de concurrencer directement les banques. De son côté, Google ne révolutionnera pas son modèle économique, aujourd’hui basé sur les revenus publicitaires, avec une structure de coûts bien plus favorable que celle d’une banque. Google c’est la donnée, et cette donnée ne demande qu’à être monétisée, faisant du moteur de recherche un potentiel apporteur d’affaires pour les acteurs bancaires. Une opportunité de croissance plus qu’une menace, donc. En second lieu, les start-ups de la finance ou FinTechs. Certaines d’entre elles émanent des banques (ex. : S-Money, filiale de BPCE spécialisée dans les transferts d’argent), d’autres sont rachetées et consolidées par les banques elles-mêmes, afin notamment d’internaliser des technologies, des méthodes de travail plus agiles et des laboratoires d’idées (ex. : rachat de Leetchi par le Crédit Mutuel-Arkéa). Là encore, une opportunité de croissance. Enfin, l’émergence de la finance collaborative (crowdfunding, crowdlending, crowdequity) via des plateformes de mise en relation multipartites (B2B, C2B, B2C ou C2C). Il s’agit d’un mouvement très intéressant étant donné les difficultés d’accès au crédit ou au marché primaire pour de nouveaux acteurs. Une menace alors ? Le volume d’encours est pour l’heure bien trop peu significatif rapporté à la masse globale des affaires. De plus, ce sont les banques qui, en s’associant aux projets de plateformes, exploitent ce canal comme véhicule de commissions parallèles (ex. : lancement de Proximea par la Banque Populaire Atlantique). Pas de menace non plus sur ce front. Par conséquent, c’est un tort de systématiquement chercher à opposer les banques historiques aux nouveaux acteurs : il y a de la place pour une relation « gagnant-gagnant », fondée sur le partage de technologies, de méthodes, de connaissances de clientèles à fort potentiel, etc. Autrement dit, les banques font face à de nouveaux défis qui constituent, in fine, une évolution de plus parmi toutes celles que le métier a déjà connues… ces derniers siècles. La banque, actrice de son propre changement Les exemples précédents illustrent le fait que les banques ne sont pas victimes, ne sont pas en train de subir une transformation, mais bien qu’elles en sont actrices. La Société Générale a décidé de fermer 20 % de ses agences d’ici 2020 : c’est vrai, le fonctionnement d’une agence et le maillage territorial s’adaptent aux nouvelles pratiques. Mais si le « comment » évolue, c’est-à-dire les modes de consommation et de recours aux prestataires, le « quoi », à savoir l’offre fondamentale et donc l’expertise métier, ne change pas : la nature même des opérations bancaires n’a finalement que peu évolué avec l’essor du digital, le banquier demeurant LE conseiller financier. Ainsi, il reste toujours le mieux placé pour établir un bilan patrimonial, optimiser un financement, orienter vers une solution plutôt qu’une autre. C’est toujours lui qui a la meilleure connaissance des produits et de la situation financière réelle de son client : il sait combien vous gagnez et combien il vous reste à la fin du mois. Le métier de banquier, s’il maintient son niveau d’expertise et de conseil, n’apparaît donc pas condamné. La banque de 2020 ne sera pas celle de 2015 qui, elle-même, n’est plus celle de 2008. Enfoncer une porte ouverte en annonçant l’apocalypse bancaire est le réflexe le plus naturel face à cet écosystème mouvant. Pourtant, les grands succès économiques ne se sont pas accomplis dans l’immobilisme. Entreprendre, c’est plutôt accepter le changement pour devenir acteur puis, peu à peu, meneur d’un mouvement. Les banques l’ont parfaitement compris et, ce qui s’avère rassurant, ont embarqué dans le train « digital » dont elles prennent progressivement les commandes. Karim Terbeche, Project Manager et Adrien Aubert, Senior Manager, du cabinet VERTUO Conseil
76	http://www.vertuoconseil.com/portfolio/square-la-clusaz-2015-night-day-part-1-day/	
77	http://www.vertuoconseil.com/portfolio/nearshoring-vs-offshoring-tendance-2017/	 Nearshoring vs. Offshoring : quelle est la tendance pour 2017 ? Mathilde. Taillez 2017-03-17T12:15:40+00:00 Project Description La période comprise entre 2011 et 2014 a été active en termes de délocalisation des activités des entreprises françaises à l’étranger. Le rapport publié par l’INSEE le 12 juin 2013 estime à 20.000 le nombre d’emplois supprimés sur le sol français dû aux délocalisations sur cette période de 3 ans. C’est un chiffre important, largement repris dans la presse, mais qui fait échos aux 20.000 emplois que la France détruisait annuellement 10 ans plus tôt. Il est donc important de ne pas tirer de conclusion trop hâtive à la lecture des résultats statistiques. L’argument numéro un mis en avant par les entreprises qui délocalisent leurs activités est le prix de la main d’œuvre. Rien qu’au sein de l’Union Européenne, les coûts horaires de main d’œuvre varient de 4,10 € (Bulgarie) à 41,30€ (Danemark) soit 10 fois plus cher. La France se situant à 35,10€ (chiffres Eurostat 2015). Mais les salaires ne sont pas le seul argument : coûts commerciaux (transport notamment), coûts de production (charges sociales), rapprochement avec des entreprises ayant déjà délocalisé, recherche d’environnements stables (facilité d’embauche, flexibilité de l’emploi, qualification des salariés, etc.) sont des critères également importants dans ce choix stratégique. Au sein des établissements financiers, si le secteur de l’ITO (Information Technology Outsouring) a été historiquement plébiscité (datacenters, centres de développements, de recette). Aujourd’hui, c’est celui de BPO (Business Process Outsourcing) qui concentre l’essentiel des demandes. Conserver les services « Front Office » en France et délocaliser toutes les autres fonctions en dehors du territoire ne pose en effet aucune difficulté. Et inutile de penser que toutes les activités partent pour un seul et même pays : Royal Bank of Scotland a ainsi choisi de délocaliser sa comptabilité en Pologne, alors que l’IT est largement sous-traité en Inde. Les systèmes d’information étant interconnectés, les échanges d’information s’opèrent de manière fluide, et ne posent aucune difficulté théorique. D’autant que les économies financières réalisées sont substantielles, et capables de gommer un service client dégradé. En Europe, c’est le Luxembourg qui a adopté une réelle stratégie de communication en vue d’attirer les entreprises souhaitant délocaliser. L’excellente stabilité politique, financière et législative permet facilement aux entrepreneurs de se projeter durablement. D’autres pays reviennent en force et notamment l’Irlande qui, après un épisode tourmenté ces dernières années, concentre effectivement une grande partie des services des fonds d’investissement. Délocaliser son activité en Irlande, c’est également se rapprocher des acteurs influents et des partenaires certifiés et ainsi profiter d’une expertise très localisée. Côté Maghreb, c’est le Maroc qui est historiquement le partenaire privilégié de la France, et ce, malgré un ralentissement des activités de nearshoring ces dernières années (source Office des Changes). Secteur stratégique au Maroc, le Plan « Offshoring 2016-2020 » a été adopté et offre de belles perspectives pour les acteurs français et marocains. Pour les multinationales, 2016 a marqué un temps d’arrêt dans les annonces de délocalisation de leurs activités en dehors de l’hexagone. La proximité des élections présidentielles en France expliquant très certainement ce phénomène. Sans être repris par les politiques, des entreprises annoncent régulièrement rapatrier toute ou partie de leurs activités en Europe. Cela a été le cas d’AXA en 2016 avec l’annonce de la fin de ses activités de production informatique à Bangalore. En cause ? Un turnover des salariés trop important impactant la qualité de services rendus. Retour du protectionnisme ? Si l’échiquier mondial des délocalisations ne devrait pas être bouleversé en 2017, celui des délocalisations américaines semble pour l’instant à l’arrêt. Les annonces du nouveau président Américain Donald Trump du 4 décembre 2016 ont en effet mis à l’arrêt certaines velléités de grands groupes : Ford vient ainsi d’annoncer l’abandon de son usine au Mexique au profit d’un site dans le Michigan. General Motors n’a pas encore annoncé l’arrêt de son projet vers le Mexique également. Quant à Toyota, il était le 6 janvier dernier dans les radars du Président Trump qui menaçait le géant japonais de barrières douanières dissuasives. Simples effets d’annonces ou réelles intentions de modifier les règles du jeu, 2017 marquera sans aucun doute un retour au protectionnisme qu’il soit américain ou chinois. Le choix des entrepreneurs restera certes souverain, mais à quel prix ? Celui des menaces ? 2017 sera très certainement une année de transition où les aspirations nationalistes se feront entendre bien plus fort qu’auparavant. Le dernier mot appartiendra aux consommateurs qui se montrent plus attentifs que jamais à la provenance de leurs achats (source Credoc). Définition : La délocalisation peut s’entendre sous deux formes les plus communément admises : Nearshoring : délocalisation d’une activité économique dans une zone géographique proche, permettant un contrôle des délais de livraison et de la qualité de la prestation fournie (moins de surprises). Offshoring : délocalisation d’une activité économique dans une zone géographique généralement éloignée où la main d’œuvre est très bon marché par rapport au pays d’origine. Par Sébastien BOUDARD, Project Manager du cabinet VERTUO Conseil
78	http://www.vertuoconseil.com/portfolio/renforcer-lobbying-europeen/	 Renforcer le lobbying européen Mathilde. Taillez 2017-05-04T06:10:51+00:00 Project Description Les Echos &#8211; 27 avril 2017 : À l’heure du Brexit et du Trumpisme, l’Europe et ses banques doivent protéger leurs intérêts dans les discussions avec le Comité de Bâle. Depuis presque 10 ans, après une des plus grosses crises financières mondiales, les réformes réglementaires se succèdent à un rythme effréné et semblent commencer à porter leurs fruits. D un revers de main, Monsieur Trump pense tout balayer. À l heure où le cadre réglementaire de demain s écrit, l Europe se trouve face à ses responsabilités et doit apprendre à défendre ses intérêts. Depuis novembre 2011, le FSB, Financial Stability Board, livre chaque année la liste des grandes banques systémiques. Il s agit des célèbres banques   too big to fail  , trop grosses pour tomber. Autrement dit, la faillite d une de ses banques mettrait à mal le système bancaire international, mais également l économie mondiale. Suite à la chute de Lehman Brothers et preuve à l appui que leur taille n est pas un rempart contre la faillite, elles doivent en réalité être d autant plus surveillées. Dans la dernière liste publiée en novembre 2016 par le FSB, sur les 30 banques systémiques, dix sont américaines, 13 européennes et 7 asiatiques. À eux deux, l Europe et les États-Unis représentent trois quarts du risque systémique mondial. FSB, IASB, Comité de Bâle, BCE, EMA, BIS, EBA, autant d acronymes derrière lesquels se cachent des institutions internationales, dont l objectif est d assurer la stabilité du système financier international et d éviter une nouvelle crise du système bancaire. Mais la pluralité des intervenants, le manque d une action coordonnée et l absence d une traduction des différentes propositions dans un droit international applicable à tous mettent en évidence la difficulté de la tâche. Et pourtant, pas d autres choix pour assurer un système financier stable que de viser à un cadre réglementaire partagé. Les États-Unis, toujours réticents à l application de réglementations qui leur paraissent toujours trop strictes, avaient tout de même cédé ces dernières années à la tendance réglementaire du moment et avaient fini par se lancer dans l application de réglementations telles que Volker ou Dodd-Franck. Mais l arrivée, de Mr Trump au pouvoir semble changer à nouveau la donne puisque le nouveau président des États-Unis souhaite prendre un virage à 180°, la FED étant sommée de limiter son intervention, tout du moins sur les banques américaines. La philosophie de Monsieur Trump : business, business et encore business. Laissons les entreprises, quelles qu elles soient, se gérer, ne les contraignons pas avec des réglementations infructueuses et coûteuses et laissons les Américains s endetter à nouveau. Car moins de 10 ans après cette crise majeure, les États-Unis ont réussi à remettre sur pied un système opaque qui s autoalimente et qui aux premières difficultés économiques du pays risque de faire effondrer tout un système : les titrisations sont de retour et les prêts immobiliers ont été remplacés par les prêts aux étudiants. En cas de récession, les étudiants, parmi les premières cibles touchées lors des crises économiques, ne pourront pas rembourser leur prêt. Les États-Unis sont prêts à mettre en risque de nouveau le système financier mondial. De l autre côté de l Atlantique, l Europe, le bon élève, qui se lance à chaque fois sans compter dans l ensemble des réformes proposées par les régulateurs, fédérations et autres instances. À la seule date du 1er janvier 2018, ce n est pas moins de trois réformes majeures qui sont attendues : IFRS9, la mesure du régulateur comptable international, visant à ce que les établissements mesurent et provisionnent au mieux les risques de crédit qu elles encourent. Réforme internationale, mais qui ne s applique pas aux établissements américains qui déclarent toujours leurs comptes en US GAAP. BCBS 239, réforme de fond poussée par le comité de Bâle, proposant différents principes de gouvernance des données et des systèmes d information que doivent appliquer les établissements bancaires, pour que l information qui y circule et qui est exploitée soit fiable et maitrisée. Enfin, Anacrédit, reporting ultra-détaillé, flexible et agile, demandé par la BCE, dont seule la première phase est attendue pour début 2018, qui traduit le souhait de la BCE d exploiter les données fiabilisées par la réforme du comité de Bâle, évoquée ci-dessus. Appliquées par tous, ces réformes pourraient permettre de fluidifier l échange d information, permettre une plus grande transparence et aider à la gouvernance partagée d un système mondial. Mais comment croire à l efficacité de ces réformes quand seule une partie des banques les appliquent ? Le bon élève, le plus zélé ne finira pas premier de la classe, et ne pourra empêcher à lui seul une faillite collective. Depuis toujours, trouver un cadre réglementaire commun entre Européens et américain a été extrêmement complexe, voire impossible. À l heure où Monsieur Trump entraine son pays et 10 des plus grandes banques mondiales dans la dérégulation, où les effets du Brexit sur les orientations réglementaires de la Grande-Bretagne nous sont encore inconnus, et où la Chine minimise les risques encourus par ses banques et développent le shadow banking, il est temps pour l Europe de se poser les bonnes questions, d apprendre à imposer son point de vue et son marché plutôt que de subir de plein fouet les réformes négociées par d autres. Au contraire des États-Unis, l Europe ne doit pas faire marche arrière vis-à-vis des réformes des dernières années, car si celles-ci sont coûteuses et pas toujours faciles à mettre en oeuvre, force est de constater qu elles sont bénéfiques, comme l a illustré l année dernière le cas de la Deustche Bank. L Europe doit faire valoir un principe d application pour tous ou aucun. Car ces réglementations n ont de sens et ne sécurisent le système financier que si elles sont appliquées de manière uniforme. La distorsion de concurrence réglementaire n est pas envisageable. Au moment où le prochain cadre réglementaire bâlois (une future réforme Bâle 4) est en train de s écrire et où le Trumpisme et Brexit sonne le retour en force du protectionnisme, l Europe et ses banques doivent renforcer leur lobbying et protéger au maximum les intérêts européens. C est un réel défi proposé à l Union européenne, saura-t-elle protéger ses intérêts économiques et servir de rempart à une dérégulation irraisonnée ? Par Aude COUDERC, Project Manager du cabinet VERTUO Conseil
79	http://www.vertuoconseil.com/portfolio/crise-russe-crise-ukrainienne-des-sanctions-inattendues-pour-les-banques-francaises/	
80	http://www.vertuoconseil.com/portfolio/le-offshoring-durgence-ou-la-necessaire-evolution-des-business-models/	 Le offshoring d’urgence ou la nécessaire évolution des business models VERTUO 2017-02-12T13:24:17+00:00 Project Description Les Echos – 18 février 2016 : La conférence des Nations Unies pour la lutte contre le changement climatique relance la question de la résilience économique. Tandis que certains y voient simplement une nécessaire révision des stratégies de continuité d’activité, d’autres parlent de modifications plus profondes du business. La COP21 fait partie de ces nombreux événements qui viennent nous rappeler que tout niveau de résilience est en permanence remis en jeu. Que l’on parle de plans de continuité d’activité ou de stratégies de recouvrement, l’enjeu de la résilience demeure le maintien d’un niveau d’activité assurant à la banque de ne pas « couler » à court terme. À l’heure de la réduction des coûts et des programmes d’efficience opérationnelle, la tendance est à la révision des stratégies de continuité et à la recherche de solutions à la fois économiques et efficientes. Le grand vainqueur en la matière est le fameux « cross back-up » ou l’offshoring temporaire et d’urgence des processus critiques. Si l’idée est séduisante, beaucoup argumentent qu’elle est utopique, car elle se heurte à de nombreux obstacles techniques, fonctionnels et humains. Prenons l’exemple d’une société A implantée en Europe et en Asie. Suite à un incendie dans les locaux parisiens, La Cellule de crise Française se convoque et décide de déclencher la procédure de repli dite de « cross back-up ». Les processus critiques sont alors envoyés à Tokyo pour une durée indéterminée. Si les systèmes informatiques communiquent, si les processus réceptionnés sont traitables à Tokyo, si la charge de travail est absorbable localement sans affecter le volume et la qualité japonaise et si les procédures et compétences humaines sont similaires, le Groupe A peut avec conviction informer ses clients, ses actionnaires et ses contreparties que l’activité ne souffrira pas de cet incident. De nombreuses conditions sont donc nécessaires afin de garantir une solution de cross back-up d’urgence efficace qui de plus respecte la durée maximale d’interruption admissible (les fameux RTO : Recovery Time Objectives ou durée d’interruption absorbable par le métier). Les exigences et la stratégie semblent particulièrement complexes, mais la solution reste néanmoins viable et moins consommatrice que certaines dans son implémentation. Reste à l’adresser correctement et à absorber les changements organisationnels majeurs qu’une telle solution implique. Mappemonde et ventilation des processus critiques D’un point de vue technique, les plateformes, outils et applications utilisés doivent être les mêmes en France et au Japon, sinon interfacés de manière à garantir : le transfert de données sans perte la reprise effective du travail. L’exigence fonctionnelle porte ensuite sur la correspondance des activités de Paris et de Tokyo qui doivent être identiques sur les deux sites, supposant par ailleurs que les compétences des Français soient maîtrisées par les Japonais. Dans un second temps, l’étude de faisabilité doit envisager plusieurs localisations cibles, identifier les processus délocalisables ainsi que les compétences essentielles associées, les processus périphériques existants et les RTO correspondants. Ce travail effectué, l’étude valide systématiquement via une campagne de tests de charge la liste des processus délocalisables ainsi que les pays cibles et leur niveau d’absorption. In fine c’est une cartographie mondiale des activités critiques avec correspondance de compétences, de capacité et de durée maximale de prise en charge qui doit apparaitre. Contraintes et évolutions managériales D’un point de vue managérial, de telles stratégies viennent indéniablement challenger les méthodes actuelles. En évoquant une délocalisation des processus, l’éclatement des équipes et donc la gestion de la production et des compétences à distance, le management dématérialisé devient une réalité. Or, même si on parle de offshoring temporaire, la responsabilité de l’activité demeure au site nominal. L’exigence de résilience porte donc véritablement sur un suivi et des contrôles qui, surtout en temps de crise, nécessitent d’être renforcés, et ce d’autant plus si la situation est amenée à durer. Une connaissance approfondie du métier et des activités est alors indispensable et permet de définir un protocole de contrôles garantissant le même niveau de maîtrise des risques que sur le site nominal. Cette étape de l’analyse est incontestablement la plus complexe dans l’exercice de mise en place de la stratégie de offshoring. Elle suggère in fine une évolution radicale des méthodes de gestion et de contrôle qui s’inscrivent dans une époque dite de digitalisation n’ayant aucune limite géographique. Face aux conséquences du changement climatique et de manière beaucoup plus générale dans l’idée de répondre à des besoins de résilience de plus en plus importants et imprévisibles, les réponses les plus cohérentes semblent être la capitalisation géographique et la multiplication des stratégies de repli. Loin d’être irréalisable, le cross back-up temporaire nécessite une analyse poussée et un investissement financier qui effrayent tant ce projet de nomadisme à échelle mondiale pourrait accélérer une modification profonde des business models actuels. Au-delà de la simple délocalisation des activités et de « l’harmonisation » des compétences humaines, en permettant une maîtrise internationale des processus critiques et en faisant du management dématérialisé une réalité, le cross back-up s’inscrit inévitablement dans une logique d’évolution managériale exigeant une transformation inéluctable des modes de pensées et de gestion. Catherine Epinat Consultante du cabinet Vertuo Conseil
81	http://www.vertuoconseil.com/portfolio/le-titre-restaurant-dematerialise-des-habitudes-a-bousculer/	
82	http://www.vertuoconseil.com/portfolio/apres-revolte-urnes-equilibre-fragile-entre-opportunites-menaces/	 Après la révolte des urnes, un équilibre fragile entre opportunités et menaces Mathilde. Taillez 2017-05-17T05:48:31+00:00 Project Description Les Echos – 12 mai 2017 : Depuis la crise de 2009, pour faire face à leur coût du risque qui a fortement augmenté, les banques recherchent de nouveaux leviers et rationalisent au maximum leur activité. Avec 2 desseins, diminuer les charges et répondre aux exigences réglementaires. Ainsi, ces dernières années, les programmes d efficacité opérationnelle se sont succédé, et les coupes budgétaires aussi. Mais est-ce vraiment la trajectoire qui permettra aux banques de trouver ce nouveau relai de croissance ? Il semble que tout au plus (et ce n est pas rien), cela renforce le sentiment de fiabilité de ces colosses au pied d argile. Mais les bouleversements de cet équilibre   mou   viennent aujourd hui de chocs politiques. La sortie du Royaume-Uni de l UE ainsi que le rejet de la classe politique traditionnelle américaine, matérialisée par l élection de président Trump, sont des événements qui relancent la donne. Ce new deal imposé par les votes britanniques et américains, malgré les désastres sociaux et économiques annoncés par les chroniqueurs, vont impulser de nouveaux équilibres et de nouvelles opportunités. Malgré la grogne populaire qui a suivi ces événements, les marchés restent dans l expectative et suivent pas à pas les évolutions et les négociations du Royaume-Uni et de l UE tout comme les désignations et amendements proposés aux USA. Si les bourses US et britanniques ne se sont pas effondrées, c est que les enjeux sont énormes et que les investisseurs attendent la matérialisation des risques anticipés. En espérant que le populisme ne contamine pas la sphère économique, où les actuels dirigeants anglais et les institutions américaines (FED ou congrès) sont connus pour leur pragmatisme. Il y a un objectif commun derrière ces mouvements électoraux, créer un environnement compétitif favorable aux entreprises et employés de ces pays. Aux oubliettes le modèle globalisant anglo-saxon, fini le dogme du marché unique et du   ensemble, on sera plus fort  . Les espaces de libres échanges se réduisent. Retour de l interventionnisme et des barrières fiscales&#8230; Voici pêle-mêle les messages reçus et les sources d angoisses identifiées. Les ruptures annoncées par le nouveau président américain et madame le Premier ministre anglais sonnaient assurément comme un défi à la raison. Mais paradoxalement, mêlées à du pragmatisme, cela représente surtout une opportunité de changement de cadre concurrentiel. Car qui peut croire que les USA et la Grande-Bretagne vont d un coup s isoler du monde ? La redéfinition des règles de protectionnisme de ces 2 intervenants majeurs des économies occidentales sera une source d asymétries supplémentaires et par voie de conséquences d arbitrages de places. Pour exemple, les anticipations de reflux des activités financières sur Londres et leur report vers d autres places majeures. Cette perspective permet de redistribuer des cartes : changement de fiscalités et règles de circulation des capitaux, lieu de domiciliation, politique de distribution de dividendes&#8230; Les réformes potentielles peuvent redynamiser le rendement des BFI. Car sans augmenter le volume des transactions la concurrence qui pourra s exercer entre les différentes places boursières devrait permettre aux établissements de dégager, à minima, des marges plus significatives. En outre, la remise en cause de certaines réglementations (par exemple : Dodd Frank, voire MIFID) va peut-être permettre d assouplir le cadre réglementaire global. Pour un principe de concurrence équitable, il ne serait pas acceptable que le cadre réglementaire des établissements de l UE soit plus rigide que pour leurs concurrents américains et britanniques. Un assouplissement qui pourrait être appuyé par les états européens, car si les capitaux ne circulent pas sur les places européennes, la croissance de la zone sera en danger. Dans cet espace en déséquilibre, le pragmatisme reste le principal rempart au chaos. Au-delà des incertitudes citées plus tôt, d autres menaces s annoncent. Après le rejet des élites américaines et anglaises arrive la crainte du réflexe nationaliste. En effet, les mouvements euro-sceptiques, remettant en cause la capacité de la zone euro notamment à relancer la croissance, à mener une politique de sécurité commune, à respecter le sentiment d autonomie des états membres, sont de plus en plus forts à l aube des scrutins en France et en Allemagne. Un point sépare toutefois ces démarches des logiques anglo-saxonnes, un manque de pragmatisme lié à une doctrine du repli sur soi. Si les investisseurs demeurent aujourd hui dans une certaine expectative concernant l Angleterre et les USA, il y a fort à parier qu en cas de votes majoritairement contestataires lors des présidentielles française et allemande, un mouvement de panique et de rejet verra le jour. Le Royaume-Uni est sorti d une zone d intérêts économiques, mais il a conservé son autonomie monétaire. De même, les États-Unis, avec leurs institutions fortes pourront certainement conserver un cap économique cohérent. Mais qu en serait-il d une zone euro qui devrait composer sans ses leaders français et allemands ? Et que deviendrait la France, avec son franc non adossé sur une zone d intérêts commune ? Comment serait perçue cette situation inédite depuis la Seconde Guerre mondiale ? Quoi qu il en soit, il s agirait d une entrée violente dans une nouvelle ère. Par Lionel LAFONTAINE, Consultant Senior du cabinet VERTUO Conseil
83	http://www.vertuoconseil.com/portfolio/renault-leternel-phenix/	 Renault, l’éternel phénix VERTUO 2017-02-12T13:19:50+00:00 Project Description La Tribune – 25 février 2016 : Victime de l’Etat actionnaire ? Victime de ses syndicats ? Renault serait en réalité victime d’une ambition inconstante. En effet, après une année 2015 marquée par des échanges houleux entre l’Etat, son partenaire nippon et le reste de sa structure actionnariale, l’année 2016 démarre fort mal pour la marque au losange, dont l’une des stars des ventes européennes, le Captur, trahit une politique d’investissements techniques minimalistes. Une ambition desservie par des investissements inégaux ? Beaucoup de conducteurs ont encore en tête la tentative de Renault d’entrer chez les constructeurs généralistes premium au début des années 2000, avec les résultats que l’on sait : Une Vel Satis bien trop décalée sur un marché par nature conservateur Une Avantime bâclée dont les finitions   motorisations étaient indignes de sa silhouette de « coupéspace » qui n’aura pas non plus su séduire Des Espace IV, Laguna II   Mégane II avec, à l’époque, un fort contenu technologique mais trop peu mature, qui s’est soldé par une dramatique débâcle qualitative, avec notamment un taux record de pannes immobilisantes Renault manque le train SUV Comme un dernier camouflet adressé à sa gamme, Renault manque alors le train SUV dont le marché explose réellement en Europe à partir du milieu des années 2000. Cette série noire, non contente de générer une image assez peu valorisante, trahissait surtout l’incapacité du constructeur à aller au bout de sa démarche, soit par manque de moyens (des plates-formes   motorisations loin des standards allemands) soit par frilosité (des sous-traitants et outillages industriels sélectionnés en priorité sur une logique de coût plutôt que de rapport qualité/prix). La course aux volumes Après ces épisodes malheureux, la quête de croissance a pris une toute autre tournure, avec une réorientation vers la course aux volumes pour compenser des marges unitaires parmi les plus basses du marché (inférieures alors à 3%). C’est dans ce contexte que Dacia a resurgi du passé et a, pendant de nombreuses années, sauvé les ventes du groupe en Europe ; et c’est également dans cette optique que naquirent les modèles de la seconde moitié des années 2000, simples, robustes, très standardisés, focalisés sur des petits moteurs Diesel, une sorte de « Back to basics by Renault » en quelque sorte. Une posture alors totalement à contre-courant de la stratégie outre-Rhin, pourtant remarquable. L’échec du véhicule électrique Dans la foulée était inaugurée une gamme de véhicules « zéro émission », dont les performances commerciales, qui plus est rapportées aux prévisions initiales de vente, laissent à nouveau perplexe sur la lucidité de certaines instances dirigeantes (notamment eu égard à la lenteur prévisible de notre administration sur la question des infrastructures dédiées aux véhicules électriques). En 2009, Carlos Ghosn visait pour 2020 10% du marché européen électrifié… « Suroptimisme » ou méthode Coué, toujours est-il qu’entre 2010 et 2016, Renault a écoulé à peine plus de 60 000 véhicules « ZE », soit environ cinq fois moins que prévu… Le pari dangereux sur l’Amérique du Sud et la Russie Un management à nouveau mis sous les feux des projecteurs lors du licenciement à l’été 2014 du numéro 2 du Groupe, Carlos Tavarès, grand amateur de la question automobile et de compétition (il est principalement à l’origine de la relance d’Alpine), dont l’ambition pour Renault l’amènera… au poste de grand patron de PSA ! Très médiatisé, il commençait à faire de l’ombre et à infléchir les positions de son numéro un, qui en a repris les rôles et fonctions. Un management d’autant plus ébranlé qu’il a longtemps parié avec une croissance structurelle rapide sur deux grands marchés : l’Amérique du Sud au début des années 2000, et plus récemment la Russie. Or, ces zones, en particulier la Russie, connaissent depuis maintenant de nombreux mois, des décroissances à deux chiffres qui viennent remettre en question les prévisions annuelles et donc la stratégie de développement. Ironie du sort, Renault s’implante tardivement en Chine, qui annonce de sérieux signes d’essoufflements. Finalement, le marché européen a beau être saturé, il n’en demeure pas moins une valeur sûre, plus résiliente lors des retournements de cycle économiques et génératrice de marges bien plus importantes. Nouvelles équipes, nouvelles approches Mais ces erreurs appartiennent peut-être au passé. En l’espace de 18 mois, la gamme vient d’être totalement revisitée : un vaste plan produit est en marche et son déroulement a, entre autres succès, boosté le phénomène des crossovers urbains dont le Captur a pris la tête des ventes en Europe, et plus globalement donné une vraie cohérence à la gamme, du point de vue stylistique (signatures lumineuses, calandres, etc.) et technique (généralisation de la plate-forme modulaire, du système 4-Control, des technologies digitales embarquées, etc.), laissant au final planer une onde d’espoir pour ce constructeur plus que centenaire. Le partenariat avec Mercedes, initié pour des synergies entre Smart et la dernière Twingo, s’est renforcé avec des échanges de bonnes pratiques sur les procédés industriels, des conseils sur le choix de matériaux et les méthodes de contrôle qualité en bout de chaîne d’assemblage. La politique du moindre effort Est-ce que cela sera suffisant ? Renault reste coutumier d’investissements « du moindre effort », comme en témoignent les insuffisances techniques de ses dispositifs de dépollution, dont les conséquences en réputation et image, confirmées par une claque boursière le vendredi 15 janvier dont on espère qu’elle servira de leçon à la CGT Renault, adepte du communiqué de presse assassin, coûteront bien plus cher à moyen terme que la campagne de rappel afférente. Les succès en F1, mais sans retombées Autre illustration de ces paradoxes récurrents : Renault dépense des fortunes pour ses multiples activités en compétition automobile, avec un succès indéniable notamment en F1 (11 titres mondiaux en 35 ans, 4 titres de motoriste en 6 ans), mais avec une sous exploitation marketing désolante : quel résultats sur les ventes, quels bénéfices dans les technologies embarquées, quelles offres premium ? L’Alliance Renault Nissan, qui a notablement tardé à s’implanter en Chine (en leur souhaitant que la crise actuelle s’en tienne à un choc conjoncturel), dispose en son sein de vraies pépites. Par exemple, une marque Infiniti positionnée sur les segments premium (avec ce que cela suppose de procédés industriels, de technologie, de réseau et de gestion SAV), des modèles Nissan mondialement reconnus tels que la GT-R (qui ne demanderait sans doute qu’à mutualiser ses coûts). On ne peut alors que s’étonner de la faible capitalisation de l’Alliance sur ces atouts de taille : pour s’en tenir à nos deux exemples, Infiniti a dû aller chercher un partenaire industriel chez Mercedes Benz plutôt qu’en interne et la gamme Renault Sport ne compte que deux modestes véhicules (dont un en fin de carrière). Quant à la relance d’Alpine (qui prévoit à court terme la commercialisation d’une berlinette puis à moyen terme d’un SUV), en l’absence d’informations techniques confirmées, il est trop tôt pour se prononcer. « Ce qui est criminel ce n’est pas d’échouer, mais de viser trop bas. » Certains vont rétorquer que trop d’ambition peut nuire à la santé et à la clairvoyance de certaines décisions. Ce ne sont probablement pas Toyota, englué dans de monstrueuses campagnes de rappel sur l’ensemble de ses marchés, ni Volkswagen, piégé par la fraude aux émissions polluantes, qui affirmeront le contraire. A sa façon, Renault tente régulièrement, rate souvent, renaît à chaque fois. En attendant davantage de recul sur les ventes et la fiabilité d’une gamme renouvelée et repositionnée vers le haut, il faudra s’en contenter : pour le moment, le rêve n’est pas encore permis. Par Adrien Aubert, Senior Manager du cabinet VERTUO conseil
84	http://www.vertuoconseil.com/portfolio/vers-la-fin-de-la-fraude-fiscale-internationale/	
85	http://www.vertuoconseil.com/portfolio/transformation-digitale-emergence-des-banques-en-ligne-quel-avenir-pour-les-agences-bancaires-en-france/	 Transformation digitale, émergence des banques en ligne : quel avenir pour les agences bancaires en France ? VERTUO 2017-01-12T11:48:34+00:00 Project Description La France possède aujourd’hui un des réseaux bancaires les plus denses au monde avec près de 38.000 agences. Pourtant, à l’heure du digital et de la banalisation des banques en ligne, les habitudes des clients ont été bousculées : 50 à 60% (Rapport McKinsey – accélérer la mutation numérique des entreprises 2014) d’entre eux utilisent désormais Internet pour leurs opérations bancaires courantes (virements, ouvertures de comptes épargnes, commandes de chéquiers,…). Force est de constater que la fréquence des visites en agence ne cesse de s’effriter, rendant, de fait, les agences traditionnelles de moins en moins rentables. Aujourd’hui, 15% (Baromètre 2015 des réseaux d’agences bancaires – cabinet Score Advisor) d’entre elles coûtent plus cher qu’elles ne rapportent. Comment « réinventer » ces points de vente ? A quoi ressemblera l’agence bancaire de demain ? Les grands réseaux rivalisent de nouveaux concepts, même si les initiatives s’avèrent souvent similaires : tablettes tactiles, wifi, aires de jeux pour enfants. Dans ce contexte, comment adapter voire transformer le métier des banquiers ? La nécessité d’un nouveau modèle pour rendre les agences bancaires plus attractives Chaque enseigne mène sa propre réflexion, mais la volonté commune reste de proposer des espaces mieux adaptés aux nouvelles attentes des clients et, sous la pression des syndicats, de ne pas fermer massivement d’agences afin de préserver les emplois. On note deux principales stratégies : Le recentrage de l’agence sur les activités à plus forte valeur ajoutée (focalisation des agences sur le conseil, tandis que les opérations courantes sont automatisés et/ou traitées à distance) : il s’agit par exemple du modèle adopté par BNP Paribas, qui tente d’adapter son réseau selon les besoins des clients en proposant trois catégories d’agence : des « express » avec automates ou tablettes pour les opérations du quotidien, des agences « conseil » avec des conseillers généralistes, et des agences « projets » avec des conseillers experts, par exemple sur le prêt immobilier ou la prévoyance. Le déploiement de nouveaux formats d’agences physiques différenciés pour optimiser les coûts et mieux répondre aux attentes des clients. Dernier exemple en date à la Caisse d’Epargne avec l’ouverture le 7 avril dernier d’une agence 100% innovante à Metz. En bref : 300 m2, deux niveaux, écran interactif sans contact à l’extérieur, borne d’accueil tactile pour se signaler directement aux conseillers, wifi, table tactile dans le bureau du conseiller et jeux pour enfants. La Société Générale se positionne également sur ce modèle, avec son agence de Dijon où se déploie un mur digital et musique avec parfum d’ambiance à l’entrée. La Société Générale et la Banque Populaire ont adopté une stratégie de redéploiement des horaires de leurs agences en corrélation avec la fréquentation. En effet, à certaines heures de la journée en semaine, les agences sont très peu fréquentées alors qu’elles le sont plus en fin de journée et le week-end. Notons que les banques 100 % en ligne, comme Boursorama ou ING Direct, n’ont convaincu à ce jour que 7% des Français. Cela signifie en réalité que le digital n’est pas un concurrent pour les banques, mais bien un nouveau canal à appréhender, à développer et à insérer dans l’expérience client. Un nouveau métier pour le conseiller de clientèle en agence ? Même si la fréquence des visites en agence tend à s’espacer, plus de la moitié des Français (55% selon l’Observatoire de l’image des banques 2014 réalisé par BVA pour la Fédération bancaire française.) souhaite toujours disposer d’un conseiller attitré, notamment pour les opérations plus complexes comme la souscription d’un crédit immobilier ou d’une assurance-vie, etc. Face à ce double constat, la question peut se poser : le traditionnel conseiller de clientèle en agence est-il voué à disparaître au profit d’un conseiller virtuel joignable uniquement par le « chat » ? Pas nécessairement mais le modèle du conseiller bancaire, unique interlocuteur d’un client va se faire de plus en plus rare. La transformation digitale dans les banques doit nécessairement s’accompagner d’une nouvelle organisation du travail pour les conseillers de clientèle. En effet, avec le développement du numérique et dans un contexte de défiance accrue des clients (notamment depuis la crise des subprimes), les banques doivent repositionner la relation client au cœur de leur stratégie. Aujourd’hui, le client attend une certaine proximité de la part de sa banque, de la personnalisation et une réelle expertise, et ce, quel que soit le canal emprunté (mail, chat, sms, téléphone). Grâce à Internet, un client qui se déplace en agence est un consommateur averti. Les conseillers doivent donc répondre à un enjeu majeur : satisfaire les attentes sans cesse plus complexes des clients sur une gamme de plus en plus large. Le métier du conseiller est ainsi amené à évoluer vers plus de qualitatif et des conseils adaptés à la situation de son interlocuteur. L’important n’est plus de placer beaucoup de produits, dont le client n’aura pas forcément besoin, mais de vendre à bon escient. Le numérique doit permettre aux usagers de leur faciliter la vie au quotidien et de pouvoir traiter leurs opérations courantes. Il permet par ailleurs au conseiller d’être responsabilisé et autonome par la mise à disposition d’outils innovants et ergonomiques : mise à disposition de tablettes, d’espaces dédiés à la gestion de la relation client, de réseaux d’entreprises permettant le partage du savoir et des bonnes pratiques, de vidéo-conférences avec des experts pour des entretiens tripartites avec le client, etc. Les conseillers, ainsi délestés de leurs tâches à faible valeur ajoutée, peuvent ainsi se concentrer sur la pertinence et la personnalisation de leurs conseils, et l’intensification de la relation client. En somme, créer un modèle de « banque à distance sans distance ». Deux options s’offrent toutefois aux banques, portant sur la spécialisation, ou non, du conseiller clientèle sur une expertise en particulier : afin de répondre au mieux à ses clients, doit-il leur assurer un niveau de spécialisation sans faille quel que soit le domaine d’intervention ? Ou doit-il au contraire avoir des compétences plus généralistes, et mettre en relation ses clients avec les experts concernés en fonction de leur besoin ? Il s’agit là de réels choix qu’il sera propre à chaque banque de définir, en fonction de leurs stratégies respectives. La transformation digitale dans les banques se gagnera donc via l’utilisation de leviers digitaux pertinents et à fort ROI pour mieux acquérir et fidéliser les clients, via la qualité de conseil délivré au client quel que soit le canal emprunté, en d’autres termes, une alliance parfaite entre le digital et l’humain. Par Maïa Grangier, Project Manager du cabinet VERTUO Conseil
86	http://www.vertuoconseil.com/portfolio/square-idea-4/	
87	http://www.vertuoconseil.com/portfolio/vertuo-groupe-square-decrypte-la-strategie-de-psa-au-jt-de-tf1/	
88	http://www.vertuoconseil.com/portfolio/square-se-hisse-a-la-6eme-place-du-classement-2016-des-entreprises-de-500-salaries/	
89	http://www.vertuoconseil.com/portfolio/brexit-la-guerre-des-places-financieres/	 Brexit : la guerre des places financières VERTUO 2017-02-12T12:55:09+00:00 Project Description Les Echos – 7 octobre 2016 : Le constat unanime du Brexit est que la City de Londres va perdre en influence et en ressources. Paris et Francfort ont tout à gagner. Le rapprochement entre les bourses de Londres et de Francfort tombe mal dans le contexte du vote britannique sur le Brexit . Voulue par les actionnaires, la fusion de Deutsch Börse et du London Stock Exchange poursuit son processus en dépit du vote britannique. Créer un leader mondial des opérations de marché est en ligne avec leur vision globalisée des marchés et l’état de la concurrence américaine et asiatique. Néanmoins, le contexte actuel renforce les opposants à cette fusion, qui y voient une concentration excessive des activités de courtage. Les autorités européennes n’ont pas encore validé l’opération . Au-delà de la question concurrentielle, il est souhaitable que la Commission européenne (CE) enterre cette fusion. En dehors de l’intérêt des actionnaires, pourquoi encourager un pont boursier entre Londres et Francfort ? Il y aurait un vrai décalage avec l’évolution politique actuelle de l’Europe. La fusion de NYSE avec Euronext répondait déjà à une vision globalisée des marchés financiers. Plus tard, celle entre NYSE Euronext et Deutsch Börse fut bloquée par la Commission européenne en 2012, pour des motifs concurrentiels. Au final, ces évolutions montrent que les places financières européennes glissent vers New York ou Londres. Ne serait-il pas plus visionnaire de pousser au développement d’acteurs européens ? C’est une question de stratégie économique pour l’Union européenne (UE) et de cohérence pour ses États. Si l’objectif est d’approfondir l’UE, alors il faut promouvoir l’européanisation de nos entreprises dans le contexte de la globalisation financière. L’un n’empêche pas l’autre. La bataille du siège Basée à Londres, l’Autorité bancaire européenne (ABE) va devoir déménager . Frankfurt Main Finance et Paris Europlace représentent les intérêts des deux places. Le lobby parisien a des arguments à faire valoir pour convaincre l’institution d’y venir. En effet, Francfort abrite déjà le siège de la Banque Centrale Européenne (BCE), de l’Autorité européenne des assurances et pensions professionnelles et du Comité européen du risque systémique. La répartition est donc inégale, puisque Paris n’accueille que le siège de l’Autorité européenne des marchés financiers (Esma). C’est donc un enjeu d’équilibre avec la métropole francfortoise. Ajouter l’ABE à sa liste d’institutions financières en ferait une nouvelle City. Il y aurait là un déclassement de la place parisienne, qui peut aussi se targuer de sa proximité géographique avec les institutions de Bruxelles pour asseoir sa légitimité à accueillir l’ABE. Avec le siège de la BCE, la place de Francfort dispose déjà de l’institution financière publique par excellence. En fait, cet atout supposé défavorise sa candidature : les institutions européennes se caractérisant par le partage des sièges institutionnels. Pour Paris et Francfort, l’attribution du siège de l’ABE serait une victoire stratégique qui impacterait positivement le niveau d’influence de leur place. Cette bataille n’est pas encore gagnée. Il n’est d’ailleurs pas exclu que la question soit traitée dans les futures négociations entre Londres et Bruxelles sur la sortie du Royaume-Uni. Si l’antagonisme franco-allemand ne trouvait pas d’issue, Milan ou Dublin seraient des choix par défaut. Évitons un énième consensus mou en Europe : Paris est la meilleure option. Par Pierre Theobald, Consultant Senior chez Vertuo Conseil
90	http://www.vertuoconseil.com/portfolio/paiement-instantane-ca-pourrait-changer-particuliers/	
91	http://www.vertuoconseil.com/portfolio/retraite-salaries-francais-travaillant-europe/	 Quelle retraite pour les salariés français travaillant en Europe ? Mathilde. Taillez 2017-07-21T13:16:53+00:00 Project Description Les Echos &#8211; 17 juillet 2017 : La mobilité professionnelle participe à la dynamique du marché de l’emploi européen. Revue de détail de ce qu il advient de la retraite d’un salarié français dans un contexte de mobilité européenne. Aujourd’hui, l’Europe compte environ 4 personnes actives (16-62 ans) pour chaque personne âgée de plus de 63 ans . D’ici 2060, 2 employés actifs cotiseraient pour chaque personne retraitée. Ce changement démographique constitue un obstacle non négligeable pour les gouvernements de l’UE, concernant leur responsabilité ultime vis-à-vis des pensions publiques de sécurité sociale. De plus, dans le contexte actuel de marchés financiers volatiles, de règles comptables complexes et de règlementations renforcées, les entreprises sont davantage conscientes des risques financiers associés à leurs régimes de retraite et sont en recherche active de nouveaux moyens d’en réduire les coûts et les risques. L’un des axes de remédiation est la mobilité professionnelle compte tenu de sa participation à la dynamique du marché de l’emploi européen. Cependant, un impératif persiste : l’actualisation des plans et schémas d’épargne-retraite à échelle locale, mais aussi européenne. De ce fait, qu’advient-il de la retraite d’un salarié français dans un contexte de mobilité européenne ?   Quelles sont les options de cotisations d’un Français, depuis l’étranger ? En France, plusieurs systèmes permettent à un salarié mobile, détaché ou expatrié, de continuer à cotiser en France, depuis l’étranger. Le PERCO , système efficace garantissant une solidarité d’épargne collective, tout en permettant à un souscripteur mobile de continuer à en bénéficier si le lien contractuel est maintenu, Le « contrat retraite article 83 », plus adapté à un contexte mobile et permet comme le PERCO, une portabilité, dans une certaine mesure, Un employé mobile français peut également poursuivre ses versements à l’AGIRC et à l’ARRCO, de façon volontaire.   Existe-t-il un moyen efficace de centraliser son épargne en tant qu’employé mobile ? La meilleure solution pourrait provenir d’une directive établie par le Conseil européen en 2003 : l’IORP. Cette dernière porte sur les activités des Institutions de Retraite professionnelle et mènerait vers une stratégie de retraite européenne plus complète, permettant une économie de coûts financiers, une compétitivité accrue ainsi qu’une mobilité transfrontalière plus fluide. Selon la directive, un plan de retraite établi dans un État membre de l UE peut étendre sa couverture aux salariés travaillant dans d autres États membres. Un tel plan faciliterait la gestion de retraite d’un salarié mobile grâce à ses différents compartiments et à son trait d’adaptabilité en fonction des normes et lois locales selon le pays d’accueil. Le but de la directive est de créer un marché intérieur pour les régimes de retraite professionnelle en leur permettant d opérer de façon transfrontalière. L’aspect de portabilité repose sur la collaboration entre employeurs et employés et l IORP. L&#8217;employeur joue un rôle essentiel, non seulement dans l établissement du régime de retraite, mais aussi dans son financement. En vertu de cette directive, les actifs et passifs d un fonds de pension transfrontaliers peuvent être combinés au sein d une seule entité juridique. Cela offre la possibilité aux entreprises multinationales de gagner en efficacité et de réaliser des économies d échelle, en réduisant les coûts imposés, mais aussi en limitant les risques opérationnels. Pour l&#8217;employeur, la mise en oeuvre d un plan de pension paneuropéen présente l avantage de réduire les coûts de tenue de compte en centralisant et en standardisant les diverses activités des régimes de retraite (gestion d actifs, administration, et communication des risques). Pour les employés, une efficacité importante pourrait entraîner soit une hausse des prestations reçues soit une baisse des cotisations versées. Pour les petits effectifs situés dans différents pays européens, un régime de retraite paneuropéen pourrait améliorer les perspectives d investissement, puisque les salariés peuvent profiter des avantages liés à faire partie d une entité plus vaste. Et les employés mobiles peuvent éviter une série complexe de transferts d un plan de retraite à un autre et disposeraient alors d’un hub pour leur retraite, en centralisant leurs prestations dans un fonds unique européen .   De l’IORP à l’IORP II : une réforme réussie ? Hormis les aspects attrayants qu’offre l’IORP, les plans de retraite paneuropéens ne sont pas tous efficaces pour l’intégralité des employeurs et peuvent être confrontés à des barrières locales ou règlementaires. En 2016, l’U.E. ne comptait qu’environ 75 plans transfrontaliers actifs, où 80 % de ces plans provenaient du Royaume-Uni et des Pays-Bas. C’est la raison pour laquelle la Commission européenne a opté pour une revue de la directive en décembre 2016, afin de proposer l’ IORP II . Les axes d’amélioration en question ont porté essentiellement sur : Les transferts transfrontaliers : ces derniers ont été simplifiés ; seul le pays qui reçoit le plan de retraite devra donner la permission pour tout transfert, La communication : l’information juridique, financière ou administrative doit être clairement établie et fournie dans son intégralité au souscripteur, L investissement à long terme : c’est l un des principaux moteurs de cette nouvelle directive, La transparence : tout plan devra être entièrement transparent avec son souscripteur, tant au niveau des risques de l’entreprise gérant le plan, qu’au niveau de la rentabilité future estimée. De manière générale, l’IORP II a été appréciée par les entités concernées. Cependant, la réforme n’incluait ni la diversification des statuts d’entités sociales des institutions de retraite ni l’adoption de nouvelles règles prudentielles. Ainsi on peut se demander si l’ensemble des amendements apportés suffira à rendre les plans paneuropéens plus attractifs. Par Eric NOUJAIM, Consultant du Cabinet Vertuo Conseil
92	http://www.vertuoconseil.com/portfolio/square-soiree-sportive-2017/	
93	http://www.vertuoconseil.com/portfolio/transaction-penale-la-corruption-se-paie/	 Transaction pénale : la corruption se paie VERTUO 2017-02-12T13:17:10+00:00 Project Description Les Echos – 22 mars 2016 : La loi « Sapin » qui sera présentée en Conseil des ministres le 30 mars prochain annonce une innovation dans le droit pénal des affaires : les faits de corruption commis par les entreprises pourront se régler par une transaction alignée sur le chiffre d’affaires. La perspective d’un procès pour corruption mené contre une entreprise va devenir théorique. Mais les bons comptes ne feront pas forcément les bons amis : l’entreprise fautive devra intégrer un surveillant externe. La peine de payer La mise à jour anglo-saxonne du droit français acte l’obsolescence des textes actuels. Ces derniers tranchent avec la pratique américaine des amendes dissuasives. Percutante et pragmatique, la pratique américaine donne à sa justice une arme efficace pour frapper les entreprises qui pratiquent la corruption. Les » deferred prosecutions agreement » sont des contrats entre un procureur et une entreprise sur la base de faits de corruption avérés : on y troque la non-culpabilité pénale de l’entreprise contre une forte amende publique. Avec ce projet de loi, l’entreprise ne sera plus un justiciable comme les autres : sa responsabilité pénale disparaît dans une rédemption financière. C’est assurément éloigné de la tradition juridique française. La loi Sapin semble avoir choisi le principe de réalité. Ce n’est peut-être qu’un début : il serait cohérent que la transaction pénale s’étende à d’autres délits de la vie économique. Moins de risque judiciaire, plus de risque administratif La loi Sapin va permettre de mieux canaliser le risque judiciaire lié au délit de corruption, puisque l’aléa du procès et sa durée seront neutralisés. Surtout, l’absence de déclaration de culpabilité protège le crédit d’image de l’entreprise fautive, qui ne verra pas son nom traîné dans la boue. Même si elle a commis des faits de corruption, il n’y aura ni culpabilité ni condamnation pour ce motif. L’honneur sera sauf ! La réputation des entreprises fautives sera préservée. Une situation idéale de prime abord, mais il faut nuancer : en l’état le projet envisage une communication publique de la transaction. Il faudra être attentif aux exigences du Parlement à ce sujet : toutes les informations seront-elles publiques ? L’identité des personnes concernées, les flux financiers, les montages juridiques seront-ils exposés au public ? Il y a là un enjeu critique de transparence. Par ailleurs, la loi Sapin ne visant que les personnes morales, les dirigeants des entreprises fautives pourront toujours être poursuivis pénalement à titre individuel. Pour eux, le risque demeure. La dépénalisation est dans l’air du temps. Le Conseil constitutionnel a décidé en mars 2015 dans une affaire de délit d’initié (dite « affaire EADS ») que la double poursuite (administrative et pénale) était inconstitutionnelle. Cela inclut les sanctions décidées par les régulateurs de marchés (ARCEP, AMF, etc.) qui sont des autorités administratives. On pressent avec cette décision, que le pouvoir de sanction va se concentrer sur les régulateurs de marché. Le juge pénal s’éloigne toujours davantage de l’entreprise. Une peine à double détente Après avoir payé, l’entreprise fautive sera surveillée : l’inspiration américaine de la loi Sapin est allée jusqu’à transposer le dispositif du « monitorship », c’est-à-dire d’une surveillance des programmes de conformité mis en œuvre suite à la transaction. C’est un vrai droit de veto qui se profile, puisqu’on pressent toute l’importance du rapport de conformité qui sera produit par ce contrôleur. L’entreprise fautive est donc placée sous tutelle d’un tiers, choisi par la Justice, mais rémunéré par l’entreprise. S’agira-t-il d’un auditeur externe qui certifie, ou d’un administrateur judiciaire partie prenante de la gouvernance de l’entreprise ? Qui contrôlera sa déontologie et son niveau de rémunération ? Les contours de cette future fonction sont encore flous. Dans l’entreprise, il faudra pourtant lui faire de la place et définir avec soin son rôle, son périmètre et ses moyens. Il serait concevable qu’il ait un rôle actif dans les fonctions juridiques et de contrôle interne, présumées trop faibles au regard de la situation. De même, qu’il serait pertinent d’associer le contrôleur aux opérations les plus risquées, afin de gérer le risque de corruption au plus près des réalités et dégager une éthique de terrain. La loi Sapin va moderniser le traitement juridique de la corruption en France. C’est une question de crédibilité pour notre pays, qui ne brille pas dans les classements de transparence des ONG. Il appartiendra prochainement au législateur de construire des règles efficaces et réalistes pour les entreprises confrontées au risque de corruption dans leurs marchés. La pénalisation a échoué à produire une éthique des affaires : viendra-t-elle d’une régulation plus lucide sur la vie économique ? Par Pierre Theobald, Consultant Senior chez Vertuo Conseil
94	http://www.vertuoconseil.com/portfolio/la-banque-en-ligne-est-elle-vraiment-incontournable/	 La banque en ligne est-elle vraiment incontournable ? Mathilde. Taillez 2017-06-01T05:57:08+00:00 Project Description Les Echos – 22 mai 2017 : À l’heure du lancement imminent d’Orange Bank, les activités de banque en ligne se sont imposées, sur le papier, comme incontournables dans le secteur bancaire. Avant que le dernier arrivé ne nous fasse mentir, ce marché demeure un pari sur le (très) long terme tant sa pénétration sur le marché des produits et services financiers progresse lentement et sa rentabilité financière peine à s’affirmer. À la fin des années 90, nombre d analystes voyaient en l éclosion des premières activités en ligne l avenir de la banque, qui allait devoir s adapter de toute urgence sous peine d être gobée par de nouveaux acteurs ambitieux. Au début des années 2010, l apparition des premières   fintechs   a fait planer une ombre similaire. En réalité, cette industrie lourde qu est la banque s est adaptée et a pris sous son aile les évolutions qui auraient pu la mettre en danger, pour moderniser son offre, ses pratiques, son infrastructure et son organisation interne tournée vers la valeur apportée au client. Comme souvent, prédire le pire n est qu un propos oscillant entre la facilité d une analyse partiale et la méconnaissance des organisations tant décriées. Au moment où un nouvel entrant promet de se tailler une part conséquente du gâteau bancaire français, une prise de recul doit au contraire nous amener à nous intéresser à la performance des entités proposant des produits et services bancaires   full on-line   comme étant un indicateur potentiel de la valeur réelle que lesdites activités apportent réellement aux banques d une part et aux clients d autre part. Or, il convient de gratter un peu le vernis convenu des nombreuses communications institutionnelles, passées maîtres en l art de l utilisation de termes marketing (dont le florilège innovation, robotisation, transformation, expérience client, etc.) au mieux surexploités, sinon galvaudés. Car la réalité du marché de la banque en ligne, près de 20 ans après sa naissance en France, c est une pénétration inférieure à 10 % qui relativise l importance globale du secteur, comme le souligne l institut Xerfi dans sa revue sectorielle annuelle. En dépit de ses multiples campagnes publicitaires, un acteur historique, sans doute pénalisé par une offre très limitée, a mis plus de 15 ans à atteindre son premier million de clients, sans pour autant parvenir à asseoir sa rentabilité. Les derniers arrivés ont, en grande majorité, revu à la baisse les prévisions initiales de leur business plan. Bref, ce n est pas l eldorado prévu. Il faut bien admettre que l effondrement des taux, maintenus à des niveaux artificiellement bas par la politique monétaire expansionniste de la BCE pour soutenir investissement et consommation à travers le crédit, a quelque peu rogné des marges déjà rabotées et donc incité les établissements à compenser ces faibles marges unitaires par les volumes. Or, dans cette quête du volume, l appétence des clients à la nouveauté a elle aussi été surestimée, non seulement parce que la banque traditionnelle a elle-même étendu son panel de services sur les applications mobiles, tendant à restreindre l opportunité du changement du point de vue du client, mais aussi parce que de nouveaux acteurs du paiement, de la tenue de compte, de l épargne ou du crédit se sont introduits avec de nouvelles offres (de finance participative, de trading robotisé, d agrégation de compte, etc.) dans le panorama bancaire, s adressant directement aux clients les plus appétents à l innovation technologique ou à l innovation d usage, ceux-là mêmes qui sont les plus actifs des clients de la banque en ligne. Sur un segment hyperconcurrentiel, les banques en ligne peinent en réalité à sortir du cliché du client métropolitain, jeune, connecté et disposant d une assise financière (revenus, patrimoine, etc.) supérieure à la médiane nationale. Résultat : elles s y livrent une guerre des prix sur laquelle est venue se greffer une course à l innovation pour se différencier et différencier leurs clients. En arrière-plan, la réalité quotidienne prend la forme d un combat contre les coûts qui font de ces entités des laboratoires de recherche : méthodes de travail pilotées par l agilité et la frugalité, consécration de la donnée et des algorithmes l exploitant en lieu et place de lourds et couteux dispositifs humains, organisations décentralisées et moins verticales. C est aussi et surtout sur le plan informatique qu apparaît une des principales différences : en partant d une feuille blanche, les dernières nées des banques ont pu s affranchir d une majorité des contraintes techniques auxquelles leur maison mère est habituellement confrontée avec leurs systèmes opérationnels à la limite de l obsolescence. Avec à la clé des architectures simples, claires, efficaces, exploitables, performantes et au final moins couteuses. Et, en l état de leurs performances commerciales ou financières, c est précisément en cela que ces acteurs prennent une place de plus en plus précieuse dans le giron des grands groupes bancaires. Elles leur offrent en effet une rapidité d exécution qui permet d étoffer progressivement l offre de produits et de services, au point aujourd hui de s orienter vers les métiers de l assurance (hors assurance vie), qui promettent des rentabilités autrement plus attirantes. La banque en ligne est-elle incontournable si elle doit se diversifier vers une offre assurantielle ? Indéniablement, l effet générationnel, avec la bancarisation des clientèles nées dans le tout-numérique, finira par imposer la gratuité de certains services financiers, ce qu est incapable de proposer aujourd hui la banque traditionnelle. Par Adrien AUBERT, Senior Manager du Cabinet Vertuo Conseil
95	http://www.vertuoconseil.com/portfolio/reseau-social-dentreprise-eviter-lintranet-de/	 Réseau social d’entreprise : éviter l’intranet de trop Mathilde. Taillez 2017-05-23T06:08:05+00:00 Project Description Jive, Yammer et le dernier né de Facebook, Workplace ou « Facebook at work » lancé le 10 octobre 2016, sont autant de plateformes proposant aux entreprises des réseaux sociaux d’entreprise. Créés dans les années 2010, ces réseaux, surfant sur la vague de l’agilité dans les projets, ont permis de développer une nouvelle stratégie de communication interne plus collaborative. Bien qu’ils soient implantés dans 80 % des entreprises du CAC 40, 90% des réseaux rencontrent des succès mitigés voire des échecs selon une étude de Gartner en 2013. Pourtant les entreprises continuent à les plébisciter, à l’instar de la Caisse d’Epargne Rhône Alpes qui a lancé le sien en janvier 2017. Alors comment faire de votre projet de réseau social d’entreprise un véritable succès et non un fiasco financier ?   L’atout confiance Un réseau social d’entreprise doit préalablement concerner une communauté bien définie. Mais, au fond, qu’est ce qu’une communauté ? La définition communément admise est la suivante : « Ensemble de personnes unies par des liens d intérêts, des habitudes communes, des opinions ou des caractères communs ». Dans un cadre professionnel, elle peut être constituée de personnes exerçant les mêmes fonctions, partageant des intérêts communs, des commerciaux spécialisés sur une branche particulière par exemple. Idéalement cette communauté, existe déjà hors réseau social, « in real life » selon le terme approprié. Il est plus difficile pour un groupe de se retrouver virtuellement si ses membres ne se connaissent pas. L’énergie d’une communauté ressort d’autant mieux que cette communauté partage une histoire commune. Le réseau social est la place sur laquelle tous les membres sont en capacité d’interagir sur un même sujet sans l’ombre d’une appréhension pour des échanges libres. En mon sens, le sentiment de liberté d’expression, dans le respect de ses membres, doit être une des principales caractéristiques du réseau social. Regrouper des personnes aux savoirs différents quelque soit leur grade ou leur fonction. La convivialité devrait être un synonyme de cette communauté : seuls la bienveillance et le respect à l’égard de ses membres doivent primer. Cela inclut également l’éradication de tout sentiment de « fliquage ». La fluidité, moteur d’un outil attrayant Attirer les collaborateurs sur un nouvel outil est un réel sacerdoce. Sortir les personnes de leur zone de confort ne fonctionne uniquement s’il y a un avantage comparatif par rapport aux moyens de travail habituels. Pourquoi poster spécifiquement tel contenu sur tel espace alors que je pourrais utiliser mon mail ? Pour cela, revenons aux prémices : quels sont les objectifs de la création d’une communauté. A partir de ceux-ci il en découlera des catégories et thématiques ciblées en fonction du groupe d’utilisateurs concernés. Le réseau social se compose avant tout d’un flux d’informations. Ce flux, si un système de tags ou de citations existe, permettra à chacun de retrouver les informations cherchées. Et oui, pensons aux tags ! Appropriés ils mettront en valeur les documents et toucheront plus de membres. Les systèmes de notifications de catégories et de tags seront fatals s’ils perdent l’utilisateur. Les utilisateurs récalcitrants pourront être charmés par les plugs in. Ces petits outils pratiques offrent l’ultime avantage de pouvoir poster directement des conversations et des documents Office depuis leurs ordinateurs vers le réseau. Plus qu’un tutoriel, une prise en main aidée peut permettre à l’utilisateur de comprendre cette nouvelle interface. L’animation clé de réussite L’objectif est de le maintenir en vie une fois les premiers fondements ci-dessus décrits construits. L’animation du réseau est donc vitale. Selon l’observatoire AFRC / Orange Business Services, la tendance en 2020 est d’avoir des réseaux sociaux plus spécialisés, à plus fort contenu. Ce contenu peut être boosté par les community managers. Les community managers ont un rôle central pour plusieurs raisons. Sans eux, le réseau social peut manquer de soutien participatif. Il doit être en capacité de fournir des réponses aux questionnements professionnels que ce soit par leurs connaissances ou en faisant appel aux référents du sujet questionné tout comme la veille à l’approvisionnement de contenus. Des informations métiers régulières avec, par exemple des interventions d’experts, constituent un bon moyen de garder l’œil des membres vif. Attention cependant à l’information uniquement descendante ! Evitons l’intranet « de trop ». Faisons réagir les utilisateurs. L’application de serious games comme les chasses aux trésors virtuelles permet de rebooster une communauté. Le sujet est fun et crée de la convivialité. Le membre joueur sera stimulé par un gain final et par ce jeu, continuera à creuser sa connaissance des rouages du réseau social. Pour les membres plus compétitifs les principes de classements des membres les plus actifs peut inciter à plus se servir du réseau social. Le sentiment de reconnaissance et de valorisation personnelle doit être au centre des primo déploiements de ces outils si nous voulons s’assurer d’une utilisation constante par les collaborateurs. Savoir dire « stop » Nombre de groupes peuvent être créés au sein d’une communauté si les règles de création sont laxistes : des groupes funs oui, bonne idée ; à condition que l’on sache s’arrêter lors d’un manque avéré d’utilisation. La multitude des groupes risque de les voir se transformer en « fantôme » sans membre actif. Lire les statistiques et en tirer des conclusions quant à la marche à suivre derrière est la meilleure solution pour garder un réseau social dynamique. Fermer un groupe n’est pas un échec, ça permettrait au contraire de démontrer que le réseau est vivant.   En suivant ces trois pistes, chacun devrait être à même d’élaborer un espace d’échange qui fédère autant qu’il participe à rendre efficient le travail en équipe. Enfin, dernier atout choc: les jeunes sont un public à ne pas négliger. Si on se réfère à l’édition 2017 du barème Deloitte / OpinionWay, la nouvelle génération plus digitale, la génération « 3.0 », rêve d’une entreprise plus collaborative « où leur voix doit compter ». Moins de chichi, plus d’échanges directs. La génération smartphone, née avec un internet fluide en main, est le leader évident pour l’appropriation des espaces virtuels sociaux et faire naître une culture collaborative. Par Florence LAUTHIER, Consultante Senior du cabinet VERTUO Conseil
96	http://www.vertuoconseil.com/portfolio/vertuo-la-mutation-bale-iv/	
97	http://www.vertuoconseil.com/portfolio/cash-management-la-metamorphose-dun-secteur/	 Cash Management : la métamorphose d’un secteur VERTUO 2017-02-12T12:56:54+00:00 Project Description Finyear – 27 septembre 2016 : Sans trésorerie, il n’y a qu’un pas à franchir pour entamer une procédure de liquidation judiciaire. Elle représente le nerf de la guerre pour les entreprises et la tâche n’est pas simple : en une décennie, elles ont vu leur gestion de trésorerie bouleversée ; principalement suite à la crise financière de 2008 et à l’avènement des technologies numériques. Ces grands changements marquent une tendance de fond que les institutions financières ont largement ressentie. Aujourd’hui, plusieurs questions se posent : de quelle manière le secteur s’est adapté à ces secousses et quelles leçons en tirer ? Car bien qu’aujourd’hui les Directeurs Financiers et les banques déploient une véritable « stratégie de gestion du cash », il est important qu’ils soient en mesure d’anticiper les évolutions futures. Aversion au risque : la prise de conscience des entreprises Si, avant la crise financière, les grandes entreprises recherchaient des solutions dans le but d’optimiser au maximum la rentabilité de leur trésorerie, la tolérance au risque a grandement évolué depuis. En se basant sur les composantes principales de plusieurs primes de risques, la Banque de France (Documents et Débats n°2 – fev/09) a estimé que l’aversion au risque a été multipliée par plus de 25 entre 2007 et 2009. Les placements risqués sont majoritairement écartés. Intensification de la fraude Alors que les effets de la crise se faisaient pleinement ressentir sur l’économie, la fraude aux entreprises est venue amplifier ce phénomène d’aversion au risque. L’essor massif des technologies dites « digitales » représente ainsi, avec le développement de nouveaux canaux de paiement et produits, un effet d’aubaine et de nouveaux axes d’attaque pour les fraudeurs. L’effet est sans appel : le vol de données s’est intensifié de plus de 78% pour la seule année 2014 (d’après le CNRS). La réaction des entreprises Face aux mouvements erratiques de l’économie et la professionnalisation de la fraude, les entreprises ont plus que jamais besoin de sélectionner les banques les plus solides. Afin de mieux sélectionner leurs partenaires, elles n’analysent plus seulement la performance commerciale et la solidité des banques. La politique de conformité avec la législation internationale, la solidité opérationnelle et la capacité à répondre aux situations de crise sont aussi analysées. La stratégie des banques Ces grands changements impactent les banques en profondeur qui mettent désormais en place des plans précis pour réagir le plus rapidement possible aux éventuelles crises. Elles investissent dans des plans de continuité d’activité, la sécurisation des transactions, les nouveaux moyens de paiement ainsi que dans l’électronique bancaire. Avec le fort développement de ce que l’on appelle les « fintechs », ces start-ups de la finance, il est envisageable que le rôle des institutions financières en tant qu’intermédiaire dans les transactions des entreprises soit en partie remis en cause. Pour faire face à cette menace, les banques devront évoluer une nouvelle fois, plus rapidement. De nouveaux business models devront être choisis pour pallier à la désintermédiation. Un moyen pour y parvenir serait qu’elles entrent dans cette course avec les fintechs afin de proposer des innovations disruptives ; tout en utilisant la force provenant de leur réseau et de leur expérience. La blockchain constitue à ce jour une excellente opportunité : cette technologie pourrait permettre de créer un ou plusieurs réseaux sécurisés à l’instar de SWIFT (réseau privé sécurisé permettant aux institutions d’échanger des informations) qui serait moins onéreux à mettre en place, évolutif, plus transparent et sécurisé. Chaque banque pourrait ainsi créer une offre d’électronique bancaire qui viendrait apporter une véritable valeur ajoutée aux entreprises. Si le modèle traditionnel de la banque fut secoué par la crise économique et l’avènement du Digital, il le sera encore à l’avenir mais les institutions seront mieux préparées. Par Rémi Joffre consultant du cabinet VERTUO conseil
98	http://www.vertuoconseil.com/portfolio/intrapreuneur-statut-de-demain/	 Intrapreuneur statut de demain ? Mathilde. Taillez 2017-06-06T06:08:31+00:00 Project Description Comment développer l’intrapreneuriat, éviter la fuite des cerveaux, favoriser le dynamisme des entreprises et leur croissance ? « Je monte ma start-up ! ». Qui n’a pas aujourd’hui dans son entourage une personne qui a, ou va créer, sa propre entreprise ? L’aventure tente de plus en plus de jeunes actifs mais pas seulement, on comptabilise plus de 12 000 start-up crées en Île-de-France d’après le magazine Capital (c est plus que Londres ou Berlin). Les barrières psychologiques sont tombées, les carrières dans les grands groupes séduisent de moins en moins, les étudiants qui entrent dans le marché du travail ont aujourd’hui des envies de liberté, d’être leur propre patron et rêve de success story à la Airbnb ou BlaBlaCar qui a réussi à lever 117 millions d’euros en 2015. Venir travailler en jean/tee-shirt dans des bureaux cools à la déco « hypsterisante » faite de meubles chinés, où les réunions se font autour du babyfoot est devenu un symbole de réussite. L’échec est également moins stigmatisé, entreprendre semble aujourd’hui plus valorisé que de ne rien tenter. Les barrières matérielles également sont moindres : il s’est développé un écosystème très favorable à la création d’entreprise, avec le développement des incubateurs qui accompagnent les start-up et qui croulent sous les demandes (+de 600 chaque mois pour certains). On peut citer entres autres The Family, la Halle de Xavier Niels ou Numa. Le succès de projets tels que Blablacar, Dropbox, Drivy, Leetchi basés sur des idées simples et pas forcément révolutionnaire participe aussi au phénomène. Certaines entreprises surfent déjà sur ce momentum pour à la fois développer des projets innovants en interne et dans le même temps retenir des talents. Si les Groupes du digital sont depuis longtemps favorables à l’intrapreneuriat (Gmail de Google en est un des exemples les plus connus), l’initiative séduit de plus en plus les autres secteurs et notamment la Banque/Assurance. La Big Factory au sein de Natixis ou DigitalForAllNow de la Société Générale qui vise à développer les idées innovantes sélectionnées par le groupe ou encore AXA qui a installé son Lab directement à San Francisco pour être au plus près des nouvelles tendances en sont des exemples. Les salariés participant à ces projets sont incités à remonter leurs bonnes idées lors de campagnes dédiées. Il existe également des partenariats entre entreprises et jeunes pousses : les premières proposant une structure et s’offrant en retour à la fois une bouffée d’air frais et une possibilité de prise de participation. Pourquoi ne pas aller plus loin ? Ne pas se contenter de lancer des grandes campagnes de recueils d’idées mais permettre à tout moment à quiconque de laisser exprimer leur fibre créative et innovante, en proposant leurs idées de start-up : en un mot développer l’intrapreunariat ! Comment ? La valorisation du statut d’entrepreneur associé aux moyens que peuvent mettre à disposition les entreprises pour accompagner les projets sont des leviers qui doivent favoriser l’intrapreunariat. En revanche, un individu peut se demander : pourquoi mettrais-je mes idées et mon énergie au service de mon entreprise quand je pourrais la développer en dehors ? La question fondamentale de la rétribution se pose alors. Une des pistes pourrait être la mise en place de plateformes de financement participatif (Crowdfunding) en interne : l’intrapreneur et porteurs du projet pourraient devenir actionnaires de leur « intra start-up ». Ce système permettrait à chaque salarié d’investir dans un projet auquel il croit et dont il connait l’initiateur, ce qui a un effet rassurant. Des études ont en effet prouvé que les projets proposés sur les plateformes de Crowdfunding sont quasi-exclusivement financés non pas par une communauté de bienfaiteurs mais par des proches, on appelle cela « le Love Money » (Sur KissKissBankBank, seulement 1% des contributions proviennent d’anonymes). De manière générale, cela contribuerait à renforcer le sentiment d’appartenance au groupe tout en proposant des idées d’investissement dans lesquelles chaque salarié pourrait prendre une participation, investir son plan épargne entreprise et s’ouvrir l’accès à l’économie collaborative qui ne serait plus simplement un concept vague et lointain. A chacun de devenir l’acteur principal des réussites de son entreprise et plus globalement de l’économie. En France, Mazars via son Lab a lancé un système proche : Le crowfunding entre différentes Business Units dans le but de favoriser le partage d’idées et les synergies entre ses différentes entités. Une unit peut investir dans un des projets proposés et ainsi contribuer à sa mise en œuvre rapide. Pour aller plus loin, un groupe pourrait également prendre des participations dans le projet ou inversement rétribuer le porteur du projet en participation (lui reverser un certain % des gains réalisés) en fonction du caractère professionnel ou personnel du projet. Le type de gouvernance à mettre en place pour la sélection des projets et leur analyse peut être plus ou moins lourd en fonction de la structure et taille de l’entreprise. L’entreprenariat est déjà un mode de fonctionnement encré dans la société, l’intrapreunariat peut devenir le statut de demain si cet élan est favorisé par l’entreprise et surtout initié et soutenu par les salariés eux-mêmes. Tout système permettant de créer une dynamique positive, inculquer la culture de l’innovation et la valoriser justement encouragera cette démarche. Par Séverine SAING, Consultante Senior du Cabinet Vertuo Conseil
99	http://www.vertuoconseil.com/portfolio/rallye-princesses-journal-de-bord-de-lequipage-123-solelles/	 Rallye des Princesses &#8211; Journal de bord de l équipage 1,2,3 Sol Elles Mathilde. Taillez 2017-06-09T07:55:16+00:00 Project Description La veille du départ, découverte de 90 voitures de collection et d exception, vérifications administratives et techniques, rencontre avec le staff et déjà l impression d être une princesse. Le 1er jour sous le thème des 1ères fois : les 1ers réglages du matériel, les 1ères spéciales, la 1ère frayeur en fin de journée lorsque la danseuse cale et ne redémarre pas. 1ere déception même si le bilan n est finalement pas si catastrophique, 28ème en fin de journée. Le 2ème jour sous le signe de la remotivation, les réglages s affinent avec la pilote et la danseuse, les paysages sont magnifiques mais nous restons concentrées sur notre objectif : il faut remonter au classement. Bilan : 15èmes au classement general, il faut continuer comme ça. La 3ème journée sous le signe des virages, on s attaque aux routes sinueuses, à la petite montagne avec pour destination l Alpe d Huez. Les automatismes sont là et nous sommes particulièrement à l aise sur ces routes&#8230; 13èmes au classement général, la pression monte. La 4ème journée, la fatigue se fait sentir, mais il ne faut rien lâcher, nous sommes dans le top 15, et pas à l abri de faire un bon résultat. On se dirige vers le sud mais en passant par les cols, et c est décidément dans ces étapes qu on se sent le mieux. 12èmes au classement général en arrivant à Mandelieu. Soirée bien méritée au bord de la mer. On ne se déconcentre pas pour la dernière journée, le classement est serré et tout peut encore se jouer dans les dernières épreuves particulièrement techniques. On s attaque aux spéciales mythiques du Monte Carlo&#8230; les routes sont spectaculaires. Le résultat tombe : 13èmes à 3 points des 12èmes, soit 3 secondes&#8230;. nous décrochons toutefois la 1ère place des primo participantes, soit 70% du plateau. Arrivée à saint Tropez, soulagées de l avoir fait, impressionnées par le décorum, et c est certainement pour cela que sous le coup de l émotion la danseuse cale et refuse obstinément de redémarrer. C est poussées par le staff que nous franchirons toutes les 3 la ligne d arrivée ! Une semaine hors du temps (tout en essayant d être dans les temps), une évasion, des rencontres&#8230; Merci à Square pour cette belle aventure ! La danseuse se refait une beauté pendant l été et nous repartirons dès la rentrée pour de nouvelles aventures&#8230; à suivre 🙂 Interview de Maïa GRANGIER, Senior Manager du cabinet Vertuo Conseil
100	http://www.vertuoconseil.com/portfolio/mecenat-square-institut-les-100-arpents/	
101	http://www.vertuoconseil.com/portfolio/interview-de-jeremy-girard-co-fondateur-de-fiduceo/	 Interview de Jérémy Girard, co-fondateur de Fiduceo Mathilde. Taillez 2017-06-23T08:21:30+00:00 Project Description Interview de Jérémy Girard, co-fondateur de Fiduceo, fintech leader français des solutions de gestion de finances personnelles en ligne (PFM ou Personal Finance Management), achetée et intégrée à Boursorama en 2015     Qu’est-ce que Fiduceo ? Fiduceo est une fintech née de la fusion de 2 start-up : la première, que j’avais moi-même créée, proposait de l’agrégation de documents, et la seconde, créée par Pierre Villeroy de Galhau et David Godat, offrait un outil de gestion des finances à ses clients. Nous avons réuni nos savoir-faire respectifs en 2012 pour créer Fiduceo. Notre ambition : simplifier la gestion administrative et financière des français. Fiduceo développait et fournissait des outils technologiques optimisant la relation bancaire en ligne tels que l’agrégation de comptes bancaires et de documents, la catégorisation automatique des dépenses, le coffre-fort numérique et le service d’alertes personnalisables. Boursorama souhaitait proposer un espace bancaire unique et mobile en France par la richesse de ses fonctionnalités. C’est ainsi que nous avons commencé à travailler avec la Banque en 2014 et qu’en 2015, nous rejoignions le Groupe. Aujourd’hui, ce sont 15 collaborateurs qui travaillent sur la solution au sein de Boursorama.   En quoi Fiduceo est une FinTech ? Fiduceo est une start-up qui utilise la technologie pour repenser certains services financiers et bancaires. En cela, c’est une fintech.   Quel est votre produit ? Comme je l’ai précisé précédemment, Fiduceo fournissait aux banques un système bancaire innovant, non-comptable, contenant 3 fonctionnalités : tout d’abord, la catégorisation automatique des opérations (reconnaissances des mouvements Carte Bleue), puis l’agrégation de comptes bancaires et l’agrégation de documents (factures, relevés d’impôts). Pour illustrer cette dernière fonctionnalité, prenons l’exemple d’une facture SFR, reliée automatiquement à la ligne d’opération bancaire « prélèvement SFR ». En un clic, le client peut visualiser sa facture SFR directement sur son Espace client bancaire. Au-delà des services innovants et personnalisés proposés aux clients, ce système permet aussi au service marketing de disposer d’un outil flexible pour pouvoir sortir rapidement de nouvelles fonctionnalités à moindre coût par rapport aux systèmes bancaires plus rigides. Après l’ajout d’un service de gestion de budget via un système d’alertes personnalisées, la dernière évolution de l’agrégation permet de réaliser des virements depuis des comptes bancaires externes sans quitter l’univers Boursorama. Pour mieux comprendre, vous prenez le cas d’un client qui a besoin de récupérer 500€ sur son compte Boursorama depuis son compte détenu dans une banque concurrente ; il peut initier ce virement directement depuis son Espace Client Boursorama.   En étant intégré à Boursorama, filiale de Société Générale, travaillez-vous aussi pour d’autres entités du Groupe ? Nous travaillons effectivement avec certaines entités du groupe Société Générale. Crédit du Nord propose à ses clients le service d’agrégation de comptes bancaires depuis octobre 2016 et le réseau Société Générale l’a lancé en début de cette année. D’autres fonctionnalités sortiront prochainement.   Quelles sont les évolutions attendues de votre produit ? Avec l’entrée en vigueur en 2018 de la seconde Directive sur les Services de Paiements (DSP2) et la libéralisation de l’accès aux données bancaires et aux opérations de paiement, de nombreuses évolutions sont à prévoir ! Nous avons d’ailleurs ouvert la voie en étant les premiers à permettre d’initier des mouvements entre les comptes ! L’émancipation de l’agrégation de comptes, le déclenchement de virements sans être une banque, le déclenchement des virements et opérations bancaires de tiers, etc., autant d’évolutions à valeur ajoutée pour le client.   Comment est composée la concurrence de votre solution ? La concurrence est jusqu’à présent très limitée. En revanche, on peut penser qu’elle tendra à se développer, et notamment au-delà des frontières de l’Hexagone, avec la mise en application de la DSP2.   Quels sont les déterminants du rapprochement récent avec Boursorama ? Boursorama était l’un des premiers clients Fiduceo. Des liens étroits ont été noués entre les deux entreprises, plus particulièrement avec les équipes techniques web. En parallèle, nous avions d’autres contrats avec d’autres banques. Boursorama qui avait déjà cette volonté d’optimiser et d’innover en permanence l’expérience offerte à ses clients, nous a un jour approché pour nous proposer d’acheter et d’intégrer Fiduceo… et nous avons accepté ! L’opération s’est finalisée en avril 2015. Depuis, Fiduceo n’existe plus en tant que telle.   Qu’est-ce que Boursorama vous a apporté ? Le premier intérêt est la maitrise de bout en bout de l’expérience client. En effet, un éditeur de logiciel n’a qu’une maitrise back office. Nous concevons un logiciel qui convient au plus grand nombre. Aujourd’hui, nous travaillons sur un outil spécifique, dédié. Avoir la main sur l’expérience client permet d’innover, d’appréhender et d’adresser de meilleures solutions. Le second intérêt est l’intégration dans le système d’information. Cela nous permet d’éviter des contraintes techniques et d’adapter la solution. Cela apporte une appréhension du métier de manière différente.   Comment les travaux de Fiduceo s’intègrent au sein de Boursorama ? L’intégration des travaux s’est déroulée du passage d’une solution unique à une solution dédiée exclusivement à Boursorama. Il est intéressant de se considérer comme un éditeur de logiciel au sein même de Boursorama. Nous cultivons cette culture lorsque nous accueillons de nouveaux collaborateurs dans l’équipe afin de pérenniser cet état d’esprit libre et agile qui va de toute façon dans le sens de la culture d’entreprise de Boursorama qui, à son origine, était aussi une start-up, la première à proposer du courtage sur minitel !   Le marché bancaire est en forte évolution ces dernières années (réglementations, digitalisation, automatisation…) ; comment voyez-vous votre activité dans 2-3-5 ans ? A court terme, comme nous l’évoquons tout à l’heure, l’entrée en vigueur de la DSP2 va apporter son lot d’évolutions. L’enjeu est d’intégrer pleinement cette réglementation européenne en adaptant l’agrégation de comptes. Ensuite, la digitalisation et l’automatisation croissantes modifieront sans doute et encore les nouveaux usages. Les prochaines solutions restent donc à créer et nous souhaitons que Boursorama devienne un point de passage pour toute opération sur comptes.   Interview réalisé par Frédéric GERARD, Consultant du cabinet Vertuo Conseil
102	http://www.vertuoconseil.com/portfolio/lunion-des-marches-de-capitaux-na-pas-le-vent-en-poupe/	 L’Union des marchés de capitaux n’a pas le vent en poupe VERTUO 2017-02-12T13:03:34+00:00 Project Description Les Echos – 22 juillet 2016 : La sortie de l’Union européenne du Royaume-Uni, acteur très influençant au sein de l’Europe en raison notamment de sa place financière dominante à la City, remet en question le projet d’Union des marchés de Capitaux (UMC), dont le porteur et ancien commissaire européen aux Services financiers, Jonathan Hill a donné sa démission en juin dernier. Le projet, adopté à l’automne dernier, s’inscrit dans une démarche de déréglementation financière et affiche un objectif ferme : relancer la croissance et les créations d’emplois en manque de dynamisme depuis la dernière crise financière. Pour atteindre ce but, la commission européenne prévoit de renforcer l’intégration financière et de favoriser l’investissement ainsi que la libre circulation des capitaux en formant un marché unique en Europe. Plus concrètement, par extension à l’Union bancaire européenne, il s’agit principalement de permettre aux PME de se financer par des canaux non bancaires et de relancer la titrisation dans un cadre européen réglementé et uniforme. Le premier rapport d’étape publie les mesures pour lesquelles la Commission européenne bûche actuellement. Le bilan dressé par Bruxelles sur ces six premiers mois d’études est déjà positif. Les problématiques abordées sont variées et concrètes : faire évoluer la législation sur Solvabilité II, réforme réglementaire pour les assureurs, dans le but de galvaniser les investissements dans les infrastructures, réviser le régime de prospectus afin de simplifier et accélérer la levée de fonds en bourse pour les entreprises, proposer un programme de convergence de surveillance visant à améliorer la transparence des marchés financiers Mifid 2, relancer la titrisation sur les marchés ou encore redynamiser l’assiette commune consolidée pour l’impôt sur les sociétés pour une fiscalité équitable et efficace. L’approche adoptée par la commission européenne se veut pragmatique : avancer par étapes sur des mesures précises jusqu’à son aboutissement en 2019 en assurant une stabilité réglementaire et une politique commune de supervision. Malgré le caractère à long terme de ce projet, le plan reste ambitieux étant donné le nombre de mesures à mettre en place et le caractère versatile de la commission européenne qui n’arrive pas à prendre clairement position sur une conduite à adopter entre la libéralisation du marché et la réglementation financière. En effet, elle gère d’un côté l’UMC qui préconise de développer des prêts non bancaires favorisant le shadow banking et doit d’un autre côté être en adéquation avec la réglementation Mifid 2 qui requiert un niveau de protection des investisseurs élevé et une transparence accrue vis-à-vis des instruments financiers. De même, le projet d’harmonisation de la taxe sur les transactions financières visant à ponctionner les titres et les produits dérivés financiers, qui n’a pour le moment convaincu que 11 pays, est contradictoire avec l’idée de favoriser le financement par les marchés, ambition majeure du projet d’UMC. Quant à la relance du marché de la titrisation, il s’agit sans doute de la position la plus dure à justifier. Ce procédé financier consistant à transformer des créances en titres négociables a été en effet la cause même de la propagation de la crise financière de 2007. Le régulateur européen veut rassurer les plus perplexes en définissant une titrisation « Simple, transparente et standardisée ». En admettant que l’on puisse établir une titrisation de qualité, cette technique financière n’est pas la solution pour la relance des investissements envers les petites et moyennes entreprises, car même aux États-Unis, pays de référence en matière de titrisation, les opérateurs financiers ne titrisent pas de crédits aux PME. De plus, la passation de pouvoir ne facilite guère la poursuite du projet dans de bonnes conditions. En effet, son successeur, Valdis Dombrovskis, qui est l’actuel vice-président de la commission européenne, devra sans doute lui-même instruire le nouvel arrivant sur les sujets qu’il gère. Ce projet ayant tous les éléments pour persuader le Royaume-Uni de rester au sein de l’Union européenne n’a visiblement pas montré des preuves suffisamment convaincantes. Le départ du Royaume-Uni doit donner le signal à la commission européenne pour se décider sur une politique cohérente afin d’asseoir sa crédibilité et de rassurer les autres pays membres pour éviter un nouvel exit. Le plus judicieux est sans doute d’abroger le projet d’UMC actuellement en perte de vitesse et de se reconcentrer sur le modèle européen traditionnel de financement de l’économie en fixant un cadre réglementaire commun. Par Laura Nguyen Consultante du cabinet Vertuo Conseil
103	http://www.vertuoconseil.com/portfolio/refontes-reglementaires-de-la-revue-des-indicateurs-risque-vers-une-transparence-maitrisee-les-etablissements-bancaires-a-la-croisee-des-chemins/	 Refontes réglementaires : de la revue des indicateurs Risque vers une transparence maitrisée, les établissements bancaires à la croisée des chemins VERTUO 2017-01-24T20:09:39+00:00 Project Description Les crises et les faillites qu’a pu connaître le système bancaire ces dernières années ont rappelé aux autorités de tutelle la nécessité de transparence induite par le caractère systémique de l’activité bancaire et entrainé une revue significative des normes bancaires afin de renforcer les exigences de bonnes pratiques et limiter les dérives spéculatives et non maîtrisées sur certaines activités. Ainsi, dans le cadre de ses évolutions réglementaires, Bâle 3 et la suite, le comité de Bâle travaille sur 2 axes principaux : la méthode avec une révision des outils de mesure des risques et la capacité des établissements à restituer l’information avec une exigence en termes de réactivité et de granularité. Concernant la mesure du risque, depuis l’entrée en vigueur de Bâle 2 début 2007, les régulateurs n’ont de cesse de faire évoluer les métriques de suivi des risques tandis que, de leur côté, les établissements ne cessent d’intégrer des nouveautés réglementaires et de s’en approprier les indicateurs, afin d’optimiser leur consommation de fonds propres, leur trésorerie et de fait leur pilotage financier. Aujourd’hui, tous les pans de l’activité bancaire sont touchés par des évolutions ou possibilités d’évolutions réglementaires : le risque de crédit avec la consultation en cours sur la refonte de la méthode standard, le risque de contrepartie avec l’intégration par Bâle 3 de la CVA crédit, le risque de marché avec la revue fondamentale du trading book, le risque de liquidité avec la mise en place des LCR   NSFR. Dans cette revue globale, certaines mesures standards et communément utilisées pour le pilotage sont remises en cause. C’est le cas de la Value at Risk (VaR), mesure actuelle du risque de marché, qui pourrait être remplacée par l’Expected Shortfall (ES). Cette évolution représente un enjeu conséquent en termes de pilotage d’activité, car la VaR communique sur le risque maximal encouru sur un portefeuille selon un seuil de confiance donné (99%) tandis que l’ES informe sur le risque moyen encouru au-delà de ce seuil. Selon l’historique de chocs observés pour calculer l’une ou l’autre des métriques, l’impact peut être significativement différent. La VaR va par exemple, considérer la faillite de Lehman Brothers comme un choc extrême, là où l’ES va intégrer ce choc dans sa moyenne de pertes possibles au-delà des 99%, ce qui représente évidemment un tout autre impact en termes de consommation de fonds propres. Cet exemple illustre l’importance de la définition de la mesure du risque et les impacts profonds qu’apportent les évolutions incessantes des régulateurs. Si les crises des dernières années ont montré la faiblesse de la réglementation en termes d’anticipation et donc mis en évidence la nécessité d’un changement, la question se pose de savoir si les multiples modifications entreprises ces derniers mois ne rendent pas plus fragiles le pilotage des banques. Car les évolutions réglementaires portant sur les indicateurs de risque impactent bien plus que le seul calcul des ratios réglementaires. Elles remettent en cause des indicateurs connus et largement utilisés dans le pilotage et l’analyse du risque et entrainent des changements importants dans la communication face aux marchés et aux investisseurs. L’autre volet des réformes actuelles concerne la restitution des données. Les dernières évolutions des reportings réglementaires ont entrainé des changements majeurs dans l’activité de reportings avec une demande de données plus granulaires, une fréquence accrue et un temps de production diminué. Par ailleurs, le suivi du risque de liquidité a eu également des impacts conséquents : usuellement, les banques reportent leur bilan à rythme trimestriel pour les autorités de contrôle et mensuellement pour des comités internes. Dorénavant, les indicateurs réglementaires de risque de liquidité, mis en place en 2014, sont contrôlés au moins mensuellement voire sur une fréquence quotidienne. Les banques ont-elles les moyens de répondre à ces exigences, et plus précisément, les circuits d’informations des établissements sont-ils en mesure de fournir les informations demandées ? L’Asset Quality Review (AQR) est riche d’enseignements. Cet exercice sans précédent, qui portait sur la collecte d’informations qualitatives sur les grands portefeuilles et le stress des expositions de 130 banques européennes, a mis en évidence les carences des systèmes d’informations bancaires et notamment leur capacité à restituer l’information. Car si les informations sont traitées et collectées opérationnellement, elles ne sont pas toujours portées dans les circuits de restitution. Par exemple, lors de cet exercice, le template T4A a demandé aux banques une vision précise et détaillée de l’ensemble du portefeuille des garanties. Or, la nature hétérogène des garanties prises par les établissements européens rend leur définition et l’harmonisation de leur traitement difficile. Ce constat généralisé justifie grandement la mise en œuvre d’une norme telle que la BCBS 239, qui vise à imposer une exigence minimale quant à la gestion des données requises pour l’évaluation du risque bancaire, car en 2016 les données doivent êtres fiables, disponibles rapidement et validées par des agents dédiés à cette fonction. Au-delà de la gestion de la donnée, BCBS 239 veut imposer des règles en terme d’architecture SI. Flexibilité, efficacité et robustesse des systèmes d’information sont les enjeux de demain, dans un contexte d’augmentation de la cadence de production et de granularité d’information toujours plus fine. Si cette norme va indéniablement dans le bon sens, elle n’en demeure pas moins particulièrement impactante pour les établissements bancaires, qui se concentrent sur des projets de revue qualitative, comprenant qualité du SI, qualité de la donnée, qualité des reportings, sans oublier l’organisation autour de ces trois thèmes. Elles doivent surtout apprendre à penser autrement : il n’est plus possible de tirer l’ensemble des informations et voir le plus petit dénominateur commun sur lequel agréger l’information. Il faut en permanence penser à la cible et optimiser son alimentation. Optimisations méthodologiques, challenges techniques et stratégiques, les banques font face à d’importants enjeux quant à la mise en place des récentes normes bâloises. Et les établissements n’ont pas le choix : dans l’incapacité de se conformer à ces exigences, ils seraient perçus en défaut de maîtrise des risques, soit a minima un risque de réputation et surtout une non-conformité inquiétante. Les banques européennes sont donc aujourd’hui à la croisée des chemins avec la nécessité de faire évoluer en profondeur leur organisation et leurs systèmes, et de s’approprier de nouveaux indicateurs de risque. La tâche est ardue mais les banques doivent être en capacité d’évoluer afin d’évaluer de manière réactive leur coût du risque et par ce biais, évaluer de manière pertinente leurs opportunités de développement en arbitrant le couple rendement-coût du risque, base du pilotage de tout établissement bancaire. Ainsi, les sujets d’études orchestrés par le régulateur sont plus que jamais des enjeux de place et de pilotage interne. Si le régulateur s’est invité dans le cockpit et cherche à s’assurer de l’efficacité des systèmes de pilotage, ce n’est pas pour autant que les banques comptent lui céder les commandes. A ce titre, les banques se doivent de participer aux consultations de place, de manière à faire entendre leur voix, engageant leur crédibilité et leur compétitivité. Transparence et maîtrise des risques pour la BCE contre gestion du coût du risque et détection d’avantages concurrentiels pour les banques : la conciliation de deux ces visions est l’enjeu central des échanges actuels. Par Lionel Lafontaine, Consultant Senior², Vertuo Conseil
104	http://www.vertuoconseil.com/portfolio/persistance-du-contexte-de-taux-bas-le-defi-assurantiel/	 Persistance du contexte de taux bas : le défi assurantiel VERTUO 2017-01-12T11:29:30+00:00 Project Description Opportunité pour les uns, challenge pour les autres, l’environnement actuel de taux bas a un impact significatif sur de nombreux segments de l’économie, en particulier sur l’industrie de l’assurance vie. Suite à la crise financière de 2007-2008, les banques centrales ont fait appel à différents leviers afin de stabiliser le système financier et d’encourager la croissance économique. Ceci se place dans le cadre de leurs objectifs d’atteindre une inflation stable de 2% ainsi que de stimuler le crédit et la liquidité sur les marchés. L’abaissement des taux directeurs ainsi que la mise en application de politiques monétaires non conventionnelles de « Quantitative Easing » (achat massif d’obligations souveraines et autres titres sur le marché) ont considérablement œuvré dans la réduction significative des taux d’intérêt de la dernière décennie. Cette situation économique a une influence non négligeable sur les prix de marché, les variations des flux de trésorerie et donc nécessairement sur la gestion du risque qui en découle. Elle constitue une menace majeure pour les assureurs vie proposant des contrats « en euros » pour lesquels ils supportent entièrement le risque d’investissement. De ce fait, la pression s’est intensifiée afin de maintenir la rentabilité de leurs activités tout en restant capables de payer les taux garantis aux clients. Plus les maturités et les taux garantis sont importants et plus le risque de déséquilibre entre le coût du passif et le rendement de l’actif s’intensifie. LA MENACE DU DÉSADOSSEMENT Afin de pouvoir respecter leurs engagements, les compagnies d’assurance placent majoritairement à long terme leurs investissements dans des d’actifs à revenus fixes de type Investment Grade (obligations souveraines ou d’entreprises de bonne qualité). Celles-ci offrent un rendement leur donnant la possibilité, avec une forte probabilité, de servir les sorties de leurs portefeuilles dont la majorité sont des rachats. Cette allocation d’actifs engendre un risque de taux pouvant se matérialiser sur les profits, le coût du capital et la liquidité. Les profits du domaine assuranciel correspondent à la marge entre le rendement du portefeuille d’actifs et les taux crédités aux clients. Pour les supports « en euros » ces taux sont établis contractuellement et ne peuvent donc pas être abaissés. Dans l’environnement économique actuel, les rendements des investissements peuvent ne plus suffire à faire face aux engagements contractés auprès des assurés. Concernant la liquidité, le contexte économique peut engendrer un déséquilibre bilanciel. Les assureurs visent à faire correspondre les flux de l’actif avec ceux du passif pour éviter d’avoir à mettre en place des réserves « ALM » supplémentaires. Certains assureurs européens, notamment allemands, se sont récemment vu downgrader par S P en raison de leurs choix d’allocation d’actifs et du risque de réinvestissement engendré par le gap de duration. En réponse aux améliorations de l’activité économique et des conditions du marché de l’emploi, les économistes prévoient un arrêt progressif et prudent de la politique de quantitative easing visant une évolution contrôlée des taux d’intérêt. L’adaptation des sociétés d’assurance à l’évolution des taux est primordiale. Dans ce cadre, il est nécessaire pour elles de déterminer contre quels scénarios elles doivent se prémunir. LES ÉVOLUTIONS FUTURES DES TAUX ET LEURS IMPACTS En termes économiques, les taux d’intérêt bas ont mécaniquement induit une augmentation de la valeur actuelle tant des passifs que des actifs des assureurs vie. En pratique, les marchés financiers ne proposent pas suffisamment d’actifs à long terme ce qui implique que les investissements des assureurs-vie ont une maturité inférieure à celle de leurs passifs. Ainsi, si la faiblesse des taux d’intérêt persiste, une fois la maturité de ses titres de créance atteinte, l’établissement sera contraint de replacer les fonds à un taux d’intérêt moindre. Ce phénomène peut être qualifié de risque de réinvestissement. La marge financière pourrait de ce fait être particulièrement affectée, notamment sur les segments où les taux garantis sont élevés. Il faut néanmoins préciser que cet impact peut être compensé, du moins partiellement, par les plus-values réalisées sur l’actif obligataire. La propension à supporter des taux bas est directement corrélée à la structure du portefeuille d’actifs ainsi qu’à la capacité à puiser dans les provisions. Une continuité dans la vente de contrats « en euros » pourrait améliorer la marge financière uniquement dans le cas où le taux garanti baisserait davantage que le rendement de l’actif. La structure des investissements ainsi que les paramètres contractuels des nouveaux contrats vendus auront donc un impact significatif sur l’évolution de la marge financière. Un scénario de taux durablement bas diluerait significativement la marge financière et l’augmentation des passifs éroderait les fonds propres. Le respect des rendements garantis aux assurés éprouveraient alors la solvabilité et la stabilité financière. Un relèvement des taux d’intérêt, quant à lui, déprécierait mécaniquement la valeur liquidative du portefeuille d’actifs à revenus fixes de l’assureur, d’une manière d’autant plus marquée que l’échéance de ceux-ci est éloignée. Pour ce qui est du passif du bilan, les taux du marché ont un impact considérable sur le comportement des clients d’assurance vie et engendre un risque de rachat. En effet, une hausse des taux permettrait aux sociétés assurancielles récemment arrivées sur le marché de proposer des taux garantis plus élevés qu’une société historique. De par l’ancienneté de son portefeuille, un assureur déjà en place ne pourra pas augmenter ses garantis de façon similaire. Ses clients pourraient donc être amenés à racheter leur contrat afin d’obtenir un rendement supérieur chez la concurrence. Cette situation représente pour l’assureur un risque de liquidation dans la mesure où, pour répondre à cette demande, il devrait vendre ses obligations dans ce contexte défavorable, ce qui pourrait engendrer des moins-values. L’actif de l’assureur diminuerait en valeur alors que son engagement resterait le même, amplifiant la situation de désadossement actif-passif. Pour les assureurs, la marge financière serait délicate à maintenir après l’augmentation des taux en raison de l’écart entre le rendement de l’actif et celui du marché, compenser les dépenses serait éprouvant. Les spécificités fiscales des contrats d’assurance-vie réduisent néanmoins considérablement le comportement de rachat des assurés dans la mesure où ils sont incités à ne pas rompre le contrat pendant une période donnée afin de pouvoir bénéficier de réduction d’impôt. Une augmentation progressive des taux d’intérêt serait le scénario idéal aux yeux des assureurs car il permettrait d’ajuster les taux crédités en adéquation avec les taux de marché et de faire correspondre les actifs avec les passifs. La hausse des taux se produira inéluctablement mais son intensité et sa rapidité restent toutefois inconnus. Les stratégies d’adaptation envisageables des assureurs s’articulent autour de 3 axes majeurs : la réduction des garanties, la diversification ainsi que la gestion de l’actif. LA RÉACTION DES ASSURANCES La réponse des assureurs a été d’abaisser les taux garantis appliqués aux nouveaux contrats et de réduire les taux crédités sur l’in force. Ceci peut toutefois porter potentiellement atteinte à la réputation de l’entreprise. Un volume supplémentaire des contrats « en euros » accroit le risque de rachat ainsi que le besoin en capital sous Solvabilité II. Pour rester rentable, l’activité épargne des sociétés d’assurance doit évoluer. Il parait donc primordial de réorienter les assurés via la promotion des contrats « en unités de compte », de la prévoyance et de la santé en imposant une limite sur les provisions des contrats « en euros ». La tentation naissante est de s’orienter vers des placements plus risqués afin d’atteindre des rendements supérieurs. Il est possible de supputer la prise croissante de risques de crédit et de marché depuis un déplacement dans la courbe du risque dans le crédit Investment Grade voire High Yield et l’utilisation de produits moins liquides (immobilier, infrastructures…). La diversification du portefeuille d’activités correspond à une stratégie de protection efficace en contexte de taux bas comme la crise japonaise des années 90 a pu le montrer. On assiste ainsi au développement de nouveaux produits à supports mixtes et à la mise en place de certaines stratégies de couvertures. En ce qui concerne la gestion de l’actif, il est possible d’envisager un allongement de la duration, une recherche de rendements plus élevés à duration donnée ou encore une utilisation d’actifs à revenus fixes non traditionnels (loans, ABS…). Ces stratégies d’allocation d’actifs ne sont néanmoins pas dénuées de risques pour la stabilité financière à court ou à moyen terme. Il est nécessaire de souligner que les exigences des autorités de surveillance imposent que les investissements dans des instruments offrant des rendements supérieurs – donc plus risqués – soient particulièrement consommateurs de fonds propres. Dans un contexte de compétitivité exacerbée par la prolongation de la situation de taux bas, la quête de marge financière se complexifie sous l’œil attentif de l’ACPR. L’institution prudentielle réaffirmait récemment la nécessité sur le marché d’adapter le modèle économique et le calcul des réserves. Il appartient donc aux assureurs de trouver le juste équilibre entre offre proposée aux clients, rentabilité et coût en capital. Par Alexandre San Martin consultant du cabinet VERTUO conseil
105	http://www.vertuoconseil.com/portfolio/les-prerequis-dun-projet-de-conduite-de-changement/	 Les prérequis d’un projet de conduite de changement VERTUO 2017-01-12T11:34:33+00:00 Project Description Le changement, l’adaptation, la flexibilité sont les incontournables du quotidien de nos entreprises. Souvent éprouvée et mise en situation, la conduite du changement prouve que son efficacité est essentielle dans la réussite d’un projet. Cependant, il reste encore de nombreux écueils et toutes les sociétés ne valorisent pas la conduite du changement avec le même degré d’importance. La connaissance et la maîtrise de ses principes, mais également la culture d’entreprise et sa taille sont essentiels pour maximiser sa réussite. En outre, existe-t-il une maturité au-delà ou en deçà de laquelle les entreprises peuvent s’en passer ? Peut-on tirer des conclusions sur les causes d’échec d’un dispositif de « Conduite de changement » ? LE CHANGEMENT EST CONSTAMMENT PRESENT Depuis deux décennies, les entreprises sont confrontées aux changements de manière plus rapide et plus constante. L’arrivée des nouvelles technologies et les nouveaux modes de consommation poussent les sociétés à adapter leurs modèles économiques. La concurrence se renforce et les entreprises mettent en œuvre des projets de transformations stratégiques visant de nouveaux marchés, produits et services. Ainsi, une forte pression s’instaure au sein des équipes en charge de trouver des solutions adaptées aux demandes projet. Dans le secteur de la banque/assurance, les exigences en matière réglementaire et vis-à-vis des autorités de régulation nécessitent une vigilance accrue sur les processus et un suivi spécifique et détaillé des risques auxquels ils sont exposés. D’une manière générale, assimiler le changement est un exercice intellectuel à la fois difficile et compliqué. Cela nécessite une remise en question des modèles « historiques» et du savoir-faire existant. Et force est de constater que cette remise en question n’est pas toujours présente au sein des entreprises. Selon le PMI (Project Management Institute), seules 18% des entreprises gèrent efficacement leurs projets de changement suite à une initiative stratégique. Aux Etats-Unis, cette même étude du PMI relève que 64% des entreprises interviewées ont une gestion moyennement efficace, voire médiocre de leurs projets. En chiffres, cela se traduit par une perte de 149 M$ annuellement. Si l’échec des projets est souvent lié à des problèmes de pilotage, de communication et de leadership, d’autres facteurs doivent être analysés : la capacité de l’entreprise à assimiler le changement, sa maturité dans l’exercice, et le sens que les collaborateurs donnent à tout projet de transformation. LE CHANGEMENT, OBJET D’ETUDE Le changement au sein des organisations a fait l’objet de nombreuses études et recherches universitaires. Des chercheurs ont par exemple identifié les leviers nécessaires pour conduire le changement de manière fluide lors de la mise en place d’un projet d’entreprise. Ainsi, la « Conduite de changement » peut être abordée comme l’adéquation parfaite entre un temps de bascule rapide et une appropriation humaine maximale. Pour comprendre comment le changement est appréhendé, ces études ont été menées autour des modèles mentaux des individus et du processus de deuil appliqués à la vie d’entreprise. Premièrement, la structure mentale d’une personne se définit par un « Paradigme ». Un paradigme est un modèle, composé par des croyances, des éléments culturels, des expériences, des habitudes, etc., qu’une personne suit toute au long de sa vie pour faire face à une situation donnée. Les paradigmes peuvent conditionner l’assimilation et l’acceptation du changement (assimilation du deuil). La « Culture organisationnelle » se structure à partir des « Paradigmes » des acteurs qui la composent. Lorsque le changement va à l’encontre des paradigmes des individus, l’effet de groupe peut freiner certains projets. La conduite du changement vise justement à limiter ce frein. Deuxièmement, la « Courbe de deuil » permet de représenter le ressenti des personnes impactées par la mise en place d’un projet. Il est important de noter que la difficulté et la complexité du projet à mettre en place peut également rendre difficile l’appropriation. LES PRINCIPALES CAUSES D’ECHEC D’UN PROJET D’ACCOMPAGNEMENT AU CHANGEMENT Comme expliqué précédemment, chaque entreprise possède son propre contexte, un management et une culture spécifiques. Son acceptation au changement est donc lui aussi spécifique, et devra être appréciée au regard des 4 points suivants : Le style de management et la façon dont les dirigeants managent l’entreprise donnent la direction à prendre et peuvent faire adhérer plus facilement les collaborateurs aux changements. L’identité et les valeurs de l’entreprise ainsi que l’engagement des collaborateurs sont des éléments clés permettant d’identifier le niveau d’interaction de ses membres. La culture organisationnelle permet d’identifier quelle est la position des leaders par rapport au changement, la capacité de délégation, d’innovation et d’adaptation aux risques et aux changements. Enfin, la flexibilité et la rapidité des processus facilitent également les échanges et la capacité à s’adapter à un éventuel changement. Afin d’identifier le niveau de tolérance des entreprises au changement, il convient de se poser les questions suivantes sur la direction de l’entreprise : lors du démarrage d’un projet de transformation, les dirigeants des entreprises comprennent-ils l’objectif du changement ? En connaissent-ils les conséquences ? Sont-ils convaincus ou se sentent-ils obligés de changer ? Sont-ils ou deviennent moteur de changement ? Par ailleurs, le dirigeant n’est pas seul dans le processus du changement : les collaborateurs sont également impliqués. A ce stade, les organisations sont suffisamment matures pour faire face aux changements ? Quel est le taux de réussite des projets ? Comment le changement est-il géré au sein des organisations ? Comment faire pour faire adhérer et fédérer les équipes ? Y-a-t-il vraiment une volonté des collaborateurs de suivre la stratégie mise en place par l’entreprise ? Comment faire pour faciliter la transition et raccourcir les délais d’appropriation ? Les facteurs d’échec de l’accompagnement au changement peuvent être classés en trois typologies : Style de management : lorsque le management n’interagit pas suffisamment avec les collaborateurs. Une évaluation du niveau de délégation d’activités, de centralisation des décisions et la capacité du management à communiquer les valeurs de la société, permettra d’identifier si le manager est en mesure d’accompagner ses collaborateurs au changement. Culture organisationnelle : lorsque l’entreprise ne s’adapte pas facilement aux exigences du marché et de la concurrence, elle a une aversion au changement et au risque, ou lorsque les équipes ne sont pas soudées et réactives. Organisation et processus : lorsque les processus ne sont pas assez flexibles (complexité et lenteur des processus, lourdeurs pour les amender ou les adapter) En général, une organisation mature en conduite du changement fera preuve d’une plus grande proactivité au niveau du management des collaborateurs. Ce dynamisme entraîne un goût pour la nouveauté, ce qui facilite les changements. Si on utilise la courbe du deuil pour déterminer le niveau de maturité, comment identifier dans quelle situation se trouve l’entreprise ? La première phase : Elle est « réfractaire » au changement. A ce stade, la structure même n’est pas apte à la mise en place des changements. En revanche, si les leaders sont déjà sensibilisés, cela peut permettre une transition plus facilement. Si ce n’est pas le cas, un travail de coaching est à faire pour les sensibiliser. La deuxième phase : Elle est « dubitative », elle se remet en question mais l’ensemble de personnes n’a pas encore accepté le changement. L’entreprise est sur la voie du changement mais elle n’est pas encore en situation optimale pour se lancer. La troisième phase : Elle est « résignée », les collaborateurs ont déjà fait leur deuil et sont prêts à coopérer. A partir de cette phase, il faut considérer que l’entreprise commence à avoir suffisamment de maturité pour entreprendre le changement. Des ajustements sont encore à faire pour attirer et faire adhérer plus de collaborateurs. La quatrième phase : Elle est « volontaire», l’entreprise est mature, elle est l’entreprise du changement. Le modèle de l’entreprise permet mettre en œuvre divers projets plus facilement. QUELLES SONT LES CLES POUR UN ACCOMPAGNEMENT AU CHANGEMENT REUSSI ? Une fois qu’on sait dans quelle phase de deuil l’entreprise se trouve, cela permettra de mettre un œuvre le plan d’accompagnement adapté aux besoins de l’entreprise en tenant compte des requis suivants : 1 – L’engagement des dirigeants et managers La difficulté d’un projet repose principalement sur son capital humain et dans sa capacité à appréhender les difficultés. Dans un premier temps et avant de lancer le projet d’accompagnement, le dirigeant doit analyser les avantages et les inconvénients du projet. Dans certains cas, un coaching est nécessaire car le processus de deuil peut être long et dur à vivre selon les personnes. En général, il est confronté à un changement de ses paradigmes. Lorsqu’il accepte la réalité et le changement, le manager devient « moteur » du changement. 2 – L’analyse et les résultats des études menées Il est nécessaire de mener plusieurs études dans les différentes phases de l’accompagnement : Une étude pour évaluer la maturité de la société via la « Courbe de deuil » Une étude de perception du projet en avant phase Une étude d’impact organisationnel afin de cibler les populations les plus impactés Une étude au démarrage du projet ayant pour but d’identifier les correctifs à mettre en place L’analyse et l’emploi des résultats est la clé pour définir la stratégie d’accompagnement à la transition. 3 – La stratégie d’accompagnement La stratégie doit définir le quoi, pourquoi, comment, pour qui et quand de la mise en place de l’accompagnement et le plan d’actions à suivre. Elle doit définir la gouvernance du projet d’accompagnement au changement, le plan de communication, le plan de formation et le dispositif de déploiement opérationnel. En parallèle, il serait convenable de mettre en place un programme de coaching des managers pour les préparer aux refus des collaborateurs et pour les aider à fédérer et à faire adhérer. 4 – La communication et le leadership. Des campagnes de communication et des espaces de dialogues entre le management et les collaborateurs sont indispensable afin d’identifier leurs craintes et mieux comprendre comment les accompagner dans leurs phases de deuil. La transparence doit être le maitre mot de ces campagnes de communication. Un mensonge sur l’état du projet peut entraîner des conflits et développer une résistance encore plus forte au changement. 5 – Une mise en œuvre pragmatique L’accompagnement doit s’ajuster à la réalité de l’entreprise, une méthodologie reste théorique. Les outils d’accompagnement doivent être opérationnels et faciles d’utilisation. La formation doit être adaptée aux besoins des collaborateurs par entité. Il est nécessaire d’utiliser des supports de formation et de communications avec des messages claires et précis des tâches à mener. Pour mener à bien la conduite du changement, les entreprises ne possèdent pas forcément le niveau de maturité nécessaire pour la mise en place des projets de transformation. La stratégie d’accompagnement au changement doit être adaptée à la réalité opérationnelle de l’entreprise, c’est-à-dire, par rapport à la phase de deuil dans laquelle l’entreprise se trouve. Cela permettra et facilitera l’appropriation des changements. De plus, les dirigeants et les managers opérationnels jouent un rôle clé lors de l’accompagnement avant et après le déploiement du projet. Les outils mis à disposition des collaborateurs doivent être pragmatiques et faciles d’utilisation afin de favoriser la capitalisation des connaissances. Enfin, tout au long du projet, les dirigeants et les managers opérationnels doivent être à l’écoute des collaborateurs afin d’adapter le dispositif d’accompagnement si nécessaire. Par Amelia Malige, Consultant Senior du cabinet VERTUO Conseil
106	http://www.vertuoconseil.com/portfolio/devoir-de-vigilance-exces-de-regulation-en-vue/	 Devoir de vigilance : excès de régulation en vue VERTUO 2017-02-12T13:22:38+00:00 Project Description Les Echos – 24 février 2016 : La proposition de loi sur le devoir de vigilance des entreprises donneuses d’ordre projette de créer une obligation juridique de vigilance. Il serait alors question de responsabilité civile, de faute et de procès. Cela change radicalement la prévention du risque et la gestion des crises. Une mauvaise gestion des risques peut coûter cher si l’on se trouve sous la juridiction des États-Unis. Les procureurs américains n’ont épargné ni British Petroleum ni BNP Paribas : ces entreprises ont été sensibilisées à la gestion de leurs risques à coup d’amendes records. En 2013, l’effondrement du Rana Plaza au Bangladesh a fait plus de mille victimes et endommagé la réputation de grandes marques de la confection. Ni procès ni amendes n’ont suivi. La différence de traitement est frappante. C’est ce problème que le législateur français s’efforce de résoudre. Obligation, le mot qui fait mal La proposition de loi vise à créer une obligation juridique de vigilance pour les sociétés donneuses d’ordres concernant la gestion des risques d’atteintes aux droits de l’Homme, aux libertés fondamentales, à l’environnement et à la santé. Avec une obligation juridique, il devient question de responsabilité civile, de faute et de procès : cela change radicalement la prévention du risque et la gestion des crises. La menace sera lourde. En s’alignant sur le droit commun, la proposition de loi va générer un potentiel de contentieux sans limites. Le légicentrisme français est coriace et ne rate pas sa cible : syndicats, ONG, associations et toutes personnes ayant un intérêt à agir pourra donc déclencher un procès et rechercher la responsabilité civile (et financière) de l’entreprise. Le risque d’un trou noir judiciaire L’esprit de la future loi est limpide : judiciariser pour forcer l’adoption de meilleurs standards. Que va gagner l’économie française avec cette nouvelle contrainte ? Certes la compétitivité hors coûts gagnera en noblesse, mais on s’imagine également que les entreprises concernées réajusteront rapidement leurs politiques d’investissement et de production, pour éviter d’être enferré dans un feuilleton juridico médiatique au dénouement aléatoire. Il est envisagé que le futur plan de vigilance puisse être examiné par le juge dans le cadre d’un procès : ce dernier va donc auditer son contenu et qualifier juridiquement sa qualité au regard des risques qui menacent l’entreprise. Il s’agit d’une revue complète de la gestion des risques. Certains secteurs étant particulièrement techniques et internationalisés, il faudra envisager de recourir à des expertises et de solliciter une coopération judiciaire internationale. Ces phases judiciaires s’annoncent donc particulièrement longues et sources de contentieux complémentaires. En l’état, la proposition de loi charrie beaucoup d’incertitudes et fait naître un risque judiciaire inconsidéré. Un nouveau risque à maîtriser Au plan opérationnel, les gestionnaires des risques seront sollicités : la proposition de loi prévoit pour les sociétés concernées l’identification de tous les risques dans tous les environnements où opère l’entreprise. La proposition de loi dépasse une simple analyse des risques et impose juridiquement que le plan de vigilance couvre en détail les risques pays, la contractualisation et l’audit des enjeux « RSE », les mesures d’alertes et la formation des salariés. Le texte s’inscrit dans une exigence d’exhaustivité, qui va faire peser une pression plus forte sur les directions des risques et des audits internes. Cette mise en conformité impliquera une évolution des outils de gestion du risque et un travail de granularité dans l’identification, la mesure et la documentation des risques. Cela va exiger davantage de compétences en diagnostic opérationnel pour analyser en détail les processus et leurs risques, tout en conservant une vision globale de la chaîne d’approvisionnement. Il faudra également envisager une collaboration plus étroite avec les directions juridiques, afin d’organiser une gestion transversale des risques judiciaires. Cette proposition de loi est néfaste pour la compétitivité française. Au regard des enjeux, le sujet nécessiterait d’être réglé au niveau international ou européen. Il faut espérer que le Parlement réévalue la pertinence économique de cette proposition de loi. On peut s’étonner que le législateur n’ait pas envisagé de traiter la question sous l’angle assurantiel, en posant une obligation de couverture des risques « RSE » dans les chaînes d’approvisionnement. Cette option aurait l’avantage d’obliger les entreprises à documenter et couvrir leurs risques, sans les soumettre à l’épée de Damoclès du juge. Par Pierre Theobald, Consultant Senior chez Vertuo Conseil
107	http://www.vertuoconseil.com/portfolio/pourquoi-en-2015-la-gestion-des-donnees-risque-est-encore-un-sujet/	 Pourquoi, en 2015, la gestion des données Risque est encore un sujet ? VERTUO 2017-01-24T20:01:38+00:00 Project Description L’exercice combiné d’évaluation des actifs et de tests de résistance mené l’année dernière par la BCE, le fameux AQR, avait ceci de nouveau – au-delà du fait qu’il s’agissait d’un exercice d’une ampleur jamais connue – qu’il imposait une phase d’assurance qualité. En quoi a consisté cette phase ? A vérifier la qualité des données utilisées et des résultats produits, notamment via des contrôles de cohérence entre les résultats des différents sous-chantiers de l’exercice. Les exigences de cette phase, décrites et spécifiées par la BCE, ont entrainé une charge non négligeable estimée (Source : intervention EY du Club Banque du 20 janvier 2015) à 20% du coût total contre environ 2% dans les exercices habituels de reporting. Parallèlement au lancement de l’audit AQR, le comité de Bâle publiait en novembre 2013 le BCBS 239 relatif à la gestion des données risque. Entre BCBS 239 et les « Remediation plans », les grands établissements français se sont rapidement lancés dans des chantiers d’envergure pour renforcer et accélérer : La fiabilisation de leurs données, La mutualisation des travaux de reportings, La refonte de leurs systèmes d’information avec notamment un rapprochement des systèmes ’information Comptabilité/Finance et Risque. Comment expliquer, plus de 10 ans après le passage à Bâle 2, ce constat d’une donnée Risque encore insuffisamment fiabilisée voire insuffisamment maîtrisée ? Une première explication est à trouver dans la segmentation des organisations. Par exemple, pour les établissements en approche interne, la mise en place de modèles internes nécessite de faire appel à des profils pointus, qui ont les compétences pour exploiter les données et en déduire des modèles adaptés. Mais de quel postulat partent-ils ? Celui que la donnée utilisée est une donnée de qualité. Avec quelle validation ? Pas la leur, puisque leur responsabilité porte sur la réalisation de modèles quantitatifs ? Pas celle, non plus, des équipes de communication financière, celles chargées des reportings réglementaires ou de pilotage interne. Tous sont amenés à exploiter la donnée, et non pas à la valider. Quant aux équipes IT, elles sont en charge d’implémenter les systèmes et de permettre à la donnée d’y transiter correctement. Au final, qui, pour se porter garant de la qualité de la donnée, en ayant une vision complète, de sa création à ses multiples utilisations ? Se porter garant et responsable, c’est finalement là que le bât blesse : les organisations actuelles ont créé trop de silos et chacun a pris l’habitude de ne faire que la tâche qui lui incombe. On se retrouve ainsi dans un monde paradoxal, qui produit de la donnée en masse et génère le phénomène Big Data, où chacun veut exploiter les données sans restriction, mais où personne ne souhaite en porter la responsabilité. L’exploitation des données sans restriction, c’est un peu ce dont rêvent les régulateurs, avec des demandes de reportings toujours plus nombreux, toujours plus fréquents et à remettre toujours plus rapidement. Force est de constater aujourd’hui, que malgré la demande des établissements pour une harmonisation des reportings, ces derniers se multiplient, portent sur des notions similaires mais pas strictement identiques, complexifiant les travaux de reportings et imposant des travaux de rapprochement. Si, lors de la première consultation du FSB en octobre 2011, les banques ont été plutôt réticentes à l’idée de produire de l’information à la demande (car cela aurait nécessité une revue complète de leur système d’information), et ont voulu maintenir la production de reportings à fréquence régulière, elles se retrouvent aujourd’hui à crouler sous les demandes de reportings variés et à produire de la donnée en quantité considérable. De ce constat, on peut s’interroger : les banques n’auraient-elles pas eu, en définitive, intérêt à investir massivement dans une évolution globale de l’architecture de leurs systèmes d’information et se mettre en capacité de produire des reportings à la demande. Etre capable de produire de l’information intelligente à la demande, peut-être plus fine, plus fiable et de manière plus flexible, afin, au final, d’enrayer ce système actuel de surproduction mensuelle, hebdomadaire et quotidienne, de données, dont l’exploitation reste à démontrer. Car l’énergie et les ressources employées à produire, contrôler, rapprocher et fiabiliser les reportings est aujourd’hui considérable. Toujours dans cette optique de production de reportings, la tendance organisationnelle ces dernières années a consisté en l’émergence, dans les grands établissements français, des cellules spécialisées rapprochant équipes Risque, équipes Finance, équipes métier pour créer des pôles spécialisés. Ces organisations qui permettent, il est vrai, une plus grande mutualisation, sont également, d’une certaine manière, l’aboutissement de ces silos où l’activité est fractionnée et les équipes dédiées à la production, séparées des autres activités de la banque. Si ces organisations viennent soutenir les travaux de production, elles démontrent que la question de la validation de la donnée exploitée ne s’est pas posée, car une telle organisation peut-elle réellement permettre la fiabilisation des données? Il faut malheureusement passer par une consultation du régulateur (BCBS239), annonciateur d’un prochain règlement, pour replacer la donnée au centre des réflexions. Les chantiers « Refonte des SI » ou « Mutualisation » ou encore « Qualité des données » existent depuis des années dans les grandes banques françaises mais, c’est depuis peu seulement que la question de la qualité des données qui transitent dans les systèmes devient centrale. Certains évoquent la possibilité de créer un poste de directeur qualité. En mettant le sujet au même niveau que les grandes fonctions du groupe, ce serait un message fort du Management sur l’importance donnée à cette problématique. Mais ensuite comment mettre en musique cette grande direction de la qualité ? Doit-on réellement nommer des personnes en particulier pour porter la responsabilité de la qualité des données ? Ne devrait-on pas, au contraire, diluer cette responsabilité, et la (faire) partager ? Ne devrait-on pas se dire que toute personne utilisant une donnée doit s’assurer de sa qualité, que chaque utilisateur a un devoir de validation, comme tout journaliste a obligation de vérifier ses sources ? Ces interrogations qui se posent encore aujourd’hui peuvent en partie expliquer pourquoi un travail sur les systèmes et sur les données est encore nécessaire aujourd’hui. Car si à première vue, travailler sur les données parait moins complexe que réaliser des modèles, il s’agit en réalité d’un travail plus difficile car il oblige à travailler autrement. Toute la difficulté de BCBS 239 réside dans cette nécessité de modifier en profondeur la gestion de la donnée et donc l’ensemble des travaux de la banque, nécessité qui va, finalement, au-delà de la seule contrainte réglementaire. La gestion des risques entre aujourd’hui dans un nouveau paradigme. Après une dizaine d’années dédiées à une gestion des risques avant tout quantitative, basée sur la mise en place de modèles et le respect de ratios financiers, arrive l’ère du qualitatif. Et pour passer le cap, les établissements n’auront d’autres choix que de revoir en profondeur organisation, gouvernance, systèmes d’informations et processus. Par Aude Couderc, Project Manager du cabinet VERTUO Conseil
108	http://www.vertuoconseil.com/portfolio/comptes-courants-la-facturation-en-question/	 Comptes courants : la facturation en question VERTUO 2017-02-12T13:28:48+00:00 Project Description Les Echos – 15 janvier 2016 : La généralisation de la facturation des comptes courants en France en 2016 suscite des interrogations. Pourquoi ces frais ? Sont-ils légitimes ? Malgré une fin d’année 2015 et un début d’année 2016 largement et malheureusement marqués par des sujets de société, un sujet économique a refait surface une nouvelle fois : celui des frais bancaires, encore et toujours. En effet, différentes annonces de banques françaises autour de la facturation des comptes courants ont suscité interrogations et réactions, certaines associations comme l’AFUB (Association Française des Usagers des Banques) se positionnant même en faveur d’une intervention du ministre des Finances sur le sujet. Une facturation qui se généralise Cette facturation des comptes courants au sein des établissements bancaires français, certes amorcée il y a quelques années, prend dernièrement un nouveau virage. Après les sept fédérations du Crédit Mutuel en octobre dernier, BNP Paribas et la Société Générale facturent la tenue des comptes courants depuis le 1er janvier. LCL suivra, courant 2016, ainsi que certaines caisses régionales des différents groupes mutualistes : à titre d’exemple la Caisse d’Épargne Île-de-France, dernière Caisse d’Épargne ne facturant pas la tenue de compte, fera désormais payer ce service. Il s’agit d’un mouvement massif et visible qui entraîne par conséquent des réactions. Il est vrai que l’on peut se demander pourquoi cela intervient après que les différents gouvernements qui se sont succédé depuis des années ont par ailleurs œuvré pour la réduction des frais bancaires imposés aux Français, car, rappelons-le, la détention d’un compte courant est nécessaire pour percevoir son salaire. Pourquoi une telle tendance aujourd’hui, à contre-courant et de manière relativement concomitante des établissements bancaires français : clients et associations de consommateurs s’interrogent. Une décision mal expliquée Si la décision de beaucoup d’établissements bancaires français de facturer la tenue des comptes courants apparaît comme une tendance relativement uniforme, on ne peut pas en dire autant des tarifs appliqués, ni même des raisons évoquées par chacune d’elles. Certaines parlent de la gestion quotidienne de ces comptes et de la qualité de service, d’autres d’investissements dans la modernisation du réseau et dans le digital, voire de la lutte contre la fraude ou encore de la sauvegarde du réseau et des emplois : de quoi laisser les clients perplexes. Certains établissements mettent en avant le fait que cette facturation, pour la tenue du compte courant, ne s’appliquera pas aux jeunes, ni aux clients dits fragiles, ni même aux clients titulaires d’une offre groupée de services (package). Concernant les packages, le sujet reste sensible, car certaines banques l’ont presque imposé à l’ouverture de compte avec un certain nombre de produits et services, peu voire pas utilisés par les clients. Des sources de revenus de plus en plus faibles pour la banque de détail En réalité, beaucoup de raisons ont conduit à la mise en place de ces frais et principalement la baisse du PNB sur deux types d’opérations. Tout d’abord, le crédit immobilier avec la faiblesse persistante des taux d’intérêt, voire leur renégociation, qui a entamé les marges. Ensuite, la facturation des incidents liés au fonctionnement du compte, qui est de plus en plus encadrée et de moins en moins rémunératrice. En effet, après la gratuité instaurée pour les clients dits fragiles, la gratuité des virements et prélèvements, le plafonnement des commissions d’intervention suite à la loi bancaire de 2013, c’est récemment le plafonnement des frais pour compte inactif qui a été introduit par la loi Eckert. Les banques se voient quasi contraintes de trouver d’autres sources de revenus récurrents. Enfin, il ne faut pas perdre de vue une autre réalité économique : le compte courant français est un compte chèque. Les banques fournissent donc un moyen de paiement gratuit à leurs clients et ce moyen de paiement entraîne un coût de traitement élevé et surtout, il résiste. On annonce sa disparition imminente depuis plus d’une décennie, mais le chèque demeure bel et bien là et ses coûts de traitement doivent être couverts, d’une manière ou d’une autre. Rappelons d’ailleurs que, la facturation du chèque, sujet un moment mis sur la table en contrepartie d’une rémunération du compte courant, ne verra sans doute jamais le jour. Un écart tarifaire qui se creuse Les banques en ligne ne facturent pas la tenue du compte. La différence de facturation va donc encore s’accroître entre banques traditionnelles facturant désormais massivement ce service et banques en ligne ne le facturant pas et multipliant offres de bienvenue et gratuité. Mais il ne faut pas s’y tromper : ces dernières sélectionnent en fait leurs clients qui doivent justifier d’un certain niveau de revenus et les domicilier afin de pouvoir ouvrir un compte dans une banque en ligne à des conditions avantageuses. Idem pour la carte bancaire qui est gratuite sous conditions de revenus ou d’épargne confiée. Aujourd’hui, les revenus perçus ou les avoirs détenus par les clients déterminent donc le prix payé pour disposer d’un compte et d’une carte bancaire, certains clients restant « captifs » des banques traditionnelles. Dans un écosystème mouvant, où beaucoup d’acteurs aux modèles disruptifs ont pour objectif de concurrencer les banques sur certaines de leurs activités, il est fort à parier que le « bruit ambiant » autour de ce sujet n’aura pas aidé nos banquiers à redorer leur blason. Par ailleurs, au moment où Orange officialise son projet Orange Banque et annonce par la voix de son PDG, Stéphane Richard, vouloir devenir le « Free de la banque », le sujet des frais bancaires a encore de beaux jours devant lui… Karim Terbeche, Project Manager du cabinet VERTUO Conseil
109	http://www.vertuoconseil.com/portfolio/volkswagen-symptome-de-letouffement-reglementaire/	 Volkswagen : symptôme de l’étouffement règlementaire ? VERTUO 2017-02-12T13:33:40+00:00 Project Description La Tribune – 29 septembre 2015 : L’hypocrisie du cycle d’homologation, qui amène à valider des consommations impossibles à reproduire dans la circulation courante, conduit les constructeurs à prendre des mesures extrêmes parfois au détriment de toute forme de légalité et de conscience citoyenne. Depuis plusieurs années, la prise de conscience de l’effet des activités humaines sur le réchauffement climatique a amené les politiques à lutter toutes voiles dehors contre les émissions de CO2. De façon assez classique, des normes et des incitations fiscales ont bouleversé l’offre et par extension la demande. Ainsi, dans l’industrie des transports privés et l’industrie automobile en particulier, le marché européen a atteint une Diésélisation sans précédent, souvent sans une analyse économique suffisante pour en démontrer la rentabilité. Or, les progrès technologiques qui ont mis ce type de propulsion sur le devant de la scène ont eu d’autres conséquences sur l’émission de particules directement néfastes pour la santé. Des dispositifs toujours plus lourds Jamais à court de progrès technologique, les industries allemande et française ont partiellement contourné le problème avec des dispositifs toujours plus lourds (et donc énergivores !), plus coûteux et complexes, répercutés directement en hausse sur les coûts d’achats et de maintenance (pièces détachées) pour les particuliers. Ce phénomène exclut d’office les populations les moins favorisées de la course environnementale, ce qui alimente une certaine fracture ou déresponsabilisation dans l’approche citoyenne. La seconde zone d’ombre provient de la méthode d’homologation des consommations. Elle spécifie, entre autres exemples, que le banc de mesure (et non une piste dédiée avec de vraies conditions atmosphériques et de circulation) simule une accélération de 0 à 70km/h en 41 secondes. Pas sûr que quiconque ait la patience de tenir ce rythme. Logiquement, la mesure est sous-estimée et au final le client est pénalisé car il reçoit une information biaisée, bien plus favorable que ce qu’il sera capable de reproduire, et ce sans la moindre mauvaise volonté des constructeurs qui ne font que se plier à une norme ! Les investissements se focalisent sur la seule norme d’homologation La troisième zone d’ombre provient de l’accélération du renforcement des normes et exigences. Avec notamment la mise en place du système de bonus / malus (et de son durcissement annuel), le progrès ne peut plus suivre la cadence et, peut-être encore plus grave, tous les investissements se focalisent sur la norme d’homologation (qui demeure une vitrine commerciale) parfois au détriment de problématiques tout aussi essentielles : sécurité active, sécurité passive, protection des piétons et des 2 roues, etc. Une nouvelle fois, les exigences règlementaires, pourtant parties d’une bonne intention fondamentalement paternaliste, induisent une inflation non maîtrisée : des coûts, de la complexité du cadre professionnel (un frein à l’embauche ?) et des inégalités sociales. Quand les exigences politiques pénalisent le consommateur final et amènent des géants industriels à contourner la loi, n’est-il pas urgent de repenser nos dispositions règlementaires pour les rendre enfin acceptables et acceptées ? Par Adrien Aubert, Senior Manager du cabinet VERTUO conseil
110	http://www.vertuoconseil.com/portfolio/square-8eme-evenement-culturel-musee-jacquemart-andre/	
111	http://www.vertuoconseil.com/portfolio/les-4-axes-de-formation/	
112	http://www.vertuoconseil.com/portfolio/devenir-faussaire-les-autorites-nous-guident-pas-a-pas/	 Devenir faussaire ? Les autorités nous guident pas à pas! VERTUO 2017-01-24T20:11:49+00:00 Project Description A la veille de la sortie du nouveau billet de 20€, prévue pour le 25 Novembre, les chiffres de la BCE sont plus parlants que jamais : 838 000 fausses coupures ont été retirées de la circulation en 2014, soit 25% de plus qu’en 2013, le billet de 20 euros étant le plus contrefait. Cette même BCE qui, sur son site internet, nous précise les signes de sécurité de ce nouveau billet de 20€, dont le graphisme devrait être moins imitable. Nous y retrouvons donc les signes de sécurité du billet, selon 3 techniques : le « toucher », le « regarder » et le « incliner » (TRI). En bonus, nous avons même droit à des « signes supplémentaires », tels que les propriétés à retrouver sous une lampe UV ou à infrarouge! Dans ce contexte, peut-on, dans un souci de transparence permanent, publier officiellement ces informations, accessibles à tous, et donc aux plus malintentionnés ? Par ailleurs, dans son bulletin officiel du 29 août 2014, le ministère de la justice présente son plan de coordination pénale afin de lutter efficacement contre la contrefaçon de monnaie : améliorer la circulation de l’information, choisir des qualifications pénales pertinentes, mieux gérer les scellés. Ainsi, dans son désir de mieux coordonner les différents éléments, il met en exergue les failles de la justice sur le sujet et surtout, nous livre en annexe, la liste des produits pouvant servir à contrefaire les billets de banque ! Le ministère de la justice énumère donc les produits, les classe par matériau et par signe de sécurité : Spray coiffant pour cheveux, Gouache blanche conditionnée en tube Stylo réservoir avec de l’encre invisible fluorescente sous UV, Vernis à ongle à effet optique de différentes couleurs, et la liste est longue. Plus de 60 produits listés ! Et il nous informe également que seulement certains de ces produits sont déjà utilisés par les contrefacteurs. Cela signifie-t-il que, s’ils sont à court d’idées, ils peuvent se sourcer dans le Bulletin Officiel ? Autant nous fournir une Edition illustrée « Pour les nuls » ! Peut-on, dans un désir de communication continu, dresser une liste de produits accessibles dans le commerce, et dans le même temps, se déresponsabiliser du phénomène ? L’information aujourd’hui est accessible de tous, partout, sur de nombreux supports. Internet, cet « open space virtuel », est une mine d’informations et surtout, une plateforme d’échanges sur le sujet. Les faux billets se commande sur internet, les transactions sont payées anonymement (carte bancaires prépayées, Bitcoins…) et même le cachet de la poste prouvant la provenance est trafiqué ! Autant dire que la tâche est rude et qu’elle n’est nullement facilitée par Internet. Caractérisé par une accessibilité importante des informations à bas coût, Internet donne les moyens à l’individu et aux groupes d’individus de penser que «tout voir» et « tout savoir » est la norme. De ce fait, un sentiment de puissance s’est installé auprès des internautes, laissant croire que la transparence est obligatoire dans un monde perçu comme plus complexe et surtout incertain. La transparence devient opérante si elle est utilisée pour maintenir et entretenir la confiance, et on doit pouvoir identifier quels savoirs partager. Les pouvoirs publics ne peuvent pas tout dévoiler, mais ils ont comme mission de reconquérir la confiance des individus et surtout, de l’entretenir. Cela ne doit pas passer nécessairement par la diffusion d’informations sensibles, prioritaires ou qui peuvent porter préjudices à leurs travaux et enquêtes. Car une fois que le processus de transparence est enclenché, il faut savoir en éviter les pièges, vis-à-vis du public. En effet, la transparence permanente dévoile une demande accrue du public, qu’il faut informer et avec qui il faut entretenir une relation de confiance qui induit un engagement permanent. Par ailleurs, en atteignant ce niveau de détail, on crée d’autres interrogations et attentes chez le public, qui peuvent rester sans réponses. La transparence dans la communication doit poser des limites. C’est comme la chambre des parents: il n’est pas bon de laisser tout voir. Par Hind Aqallal, Consultante Senior du cabinet VERTUO conseil
113	http://www.vertuoconseil.com/portfolio/reseaux-sociaux-dentreprise-conduite-changement/	 Réseaux sociaux d’entreprise et conduite du changement Mathilde. Taillez 2017-04-24T06:11:08+00:00 Project Description Le réseau social d’entreprise : l’outil collaboratif par excellence doit encore convaincre Le travail en équipe et la collaboration apparaissent clairement de nos jours comme un facteur clé de succès dans des entreprises qui prônent désormais le « décloisonnement », le « désilotage » et la « transversalité ». Les Réseaux Sociaux d’Entreprises (RSE) se présentent comme les nouveaux outils de cette collaboration en entreprise et tendent à se démocratiser depuis quelques années, que ce soit grâce à l’arrivée progressive sur le marché de la génération Y ou l’essor des réseaux sociaux dans la vie privée. Les trois quarts des entreprises du CAC 40 ont un RSE et 47 % des grandes entreprises en auraient déjà ou projetteraient d’en avoir un. Toutefois, la France semble être en retard, puisque seulement 5% des salariés français auraient accès à un RSE dans leur entreprise. Et 11% utiliseraient des outils sociaux de collaboration (contre 18% en moyenne en Europe), avec l’intranet en fer de lance, loin d’un RSE abouti avec des fonctionnalités plus développées. Nous sommes donc encore loin de la généralisation massive de cet outil dans les entreprises et de l’adoption unanime par les salariés de ce nouvel outil qui, en même temps, peut faire peur pour plusieurs raisons. Les principales craintes liées aux RSE sont du domaine de la sécurité, de la perte de contrôle et de temps des salariés. Effectivement, la mise en place d’un tel outil, pourtant en plein essor et porteur de nombreuses promesses, n’est pas sans risque. Parmi eux on retrouve le coût avec un ROI parfois difficile à calculer, la déstabilisation de la culture de l’entreprise et de la hiérarchie (remise en cause de la ligne managériale), la perte de repères organisationnels, la perte de temps et donc de productivité, mais également des problématiques RH, de santé au travail et de sécurité comme la fuite de documents confidentiels par exemple. Toutes ces peurs doivent être prises très sérieusement en considération, d’autant plus qu’elles ne sont pas formulées uniquement par des technophobes avérés mais bien relayées par des professionnels des domaines de la santé, de l’informatique, des RH et autres managers sur le terrain. Cependant, il reste bon de rappeler que toutes ces craintes sont sensiblement les même que celles perçues à l’avènement de nouvelles technologies (téléphone, internet, etc.). Le but est ici de s’aider de ces craintes, pour déterminer, dans un premier temps, si elles sont justifiées ou non, et dans un deuxième temps, les lever ou anticiper celles qui peuvent l’être.   La conduite du changement au service du réseau social d’entreprise Le projet de mise en place d’un RSE au sein d’une entreprise ne diffère pas des autres projets en ce sens qu’il doit avoir une conduite du changement à la hauteur de ses ambitions. La qualité de l’outil ne fait bien évidemment pas tout… Même si un mauvais outil ne marchera certainement jamais, un bon outil n’est pas pour autant une garantie de succès. Ainsi, la conduite du changement sur des projets stratégiques de mise en place d’un RSE doit permettre de bien préparer le terrain pour que le RSE n’apparaisse pas comme une contrainte supplémentaire, mais au contraire comme un vecteur de la transformation en cours. Et cela passe notamment par une évolution progressive de la culture d’entreprise et des modes de management. Toutefois, on remarque que si l’ensemble des acteurs s’accorde à dire que l’accompagnement du changement est d’une importance primordiale, les premières coupes budgétaires se font à ce niveau, répétant l’erreur classique de trop vouloir se focaliser sur l’outil alors que ce sont les salariés qui vont produire l’intelligence collective. On observe très souvent, dans les entreprises, que 80% de l’investissement est fait sur l’outil et 20% sur l’accompagnement alors que ce ratio devrait être quasiment l’inverse. Qui mettrait un ouvrier sur une machine sans formation préalable ? C’est pourtant ce qui est constaté avec les RSE, et même la fameuse génération Y, adepte des réseaux sociaux, a besoin de repères sur des outils professionnels (et non plus personnels grand public dont ils ont l’habitude). Quand bien même les fonctionnalités principales sont rarement compliquées à comprendre. La phase de cadrage des actions de conduite du changement est primordiale en ce sens qu’il faut tout d’abord se poser les bonnes questions : Quels sont les objectifs de l’entreprise, pourquoi se lance-t-elle dans la mise en place d’un réseau social d’entreprise ? Quels sont les freins observés ? Et comment les dépasser ? Ces questions se veulent d’autant plus importantes que l’arrivée d’un RSE en entreprise va bien au-delà de l’outil et requiert l’adoption de nouvelles pratiques et d’usages particuliers : aplanissements de l’organigramme, fluidification de la communication, transversalité… Pour ce faire il existe de nombreux leviers bien connus de la conduite du changement qui permettent de faciliter ces changements de culture et ces profonds bouleversements. Parmi ces leviers, on peut citer la formation évoquée précédemment ou encore la conception d’une charte d’utilisation, mais un des plus fondamentaux de mon point de vue est la création d’un réseau d’ « ambassadeurs », véritables promoteurs du projet et du RSE au sein de l’entreprise. Ils sont les relais du projet auprès des différentes populations de l’entreprise et sont de ce fait en première ligne face aux utilisateurs, qu’ils soient volontaires ou réfractaires au changement. C’est pourquoi cette population d’ambassadeurs se doit d’être la plus diversifiée possible en comptant des personnes de toutes fonctions et tout rang hiérarchique confondus, afin de montrer l’implication de l’ensemble de l’entreprise et la portée du changement en cours. Un des facteurs clés de succès de ce réseau d’« ambassadeurs », et même plus généralement de l’appropriation du RSE au sein de l’entreprise, est l’implication de la direction dans ce projet. Elle doit être la première à montrer l’exemple et à utiliser les fonctionnalités, créer son profil pour donner l’impulsion nécessaire au RSE. Le sponsoring doit être visible et fort tout au long du projet. Par la suite, pendant la phase de « run », les lancements, même s’ils peuvent paraître parfois un peu « forcés », doivent faire l’objet d’évènements forts sous le lead d’une direction volontaire. Le réseau social d’entreprise au service de la conduite du changement Lorsque l’on parle de RSE et de conduite du changement, on pense tout d’abord à la nécessité d’accompagner cette révolution d’entreprise, comme présenté plus tôt, mais pas encore, ou très peu, aux possibilités qu’un tel outil peut apporter à la conduite du changement elle-même. Le RSE est un outil qui est fondamentalement centré sur l’humain, or la conduite du changement cherche à favoriser l’appropriation par les acteurs de l’entreprise de nouveaux dispositifs, que ce soit des outils, des organisations, des processus… Il s’intéresse donc de très près à la dimension humaine de tous ces projets. Partant de cette réflexion il semble naturel de penser qu’à son tour le RSE peut servir la conduite du changement. Il est là pour favoriser le liant entre les collaborateurs et permettre un échange plus facile et plus constructif. Dès les premières phases d’un projet, lors du cadrage des besoins, l’échange est nécessaire et primordial entre toutes les parties concernées par le projet, et c’est là qu’intervient le RSE afin de favoriser ces échanges. Les collaborateurs prennent plus facilement connaissance des avis et des remarques des autres et le leur est considéré et pris en compte au même titre. Des fonctionnalités d’organigramme dynamique permettent d’identifier rapidement les bons acteurs qui détiennent l’information et de ce fait les solliciter pour répondre au mieux au besoin. Les systèmes de communauté offrent la possibilité de transcender les frontières hiérarchiques en mobilisant un groupe de collaborateurs de niveau différent sur une même problématique, permettant de connaitre les freins et la façon de les lever mais aussi de repérer et gérer plus facilement les salariés réfractaires. Grâce à toutes ces fonctionnalités les collaborateurs se sentent entourés et moins seuls face aux problématiques rencontrées, l’intelligence collective est mise au service du projet et la sensation d’être seul face à une montagne de difficultés s’atténue, favorisant l’appropriation des acteurs du projet. En résumé si le RSE est bien conçu et bien né, souplesse et transversalité seront au rendez-vous. Il doit faciliter les échanges, améliorer la résolution des points bloquants et favoriser le sentiment d’appartenance à l’entreprise et au projet. Finalement, le RSE devient un vecteur de la transformation et du changement de culture de l’entreprise, il n’est « que » le relais de cette nouvelle dynamique et s’inscrit dans un projet plus global. Il ne peut se suffire à lui-même et a besoin d’être en phase avec une évolution générale des mentalités et des usages dans l’entreprise. Il peut devenir, à cette condition clé, un outil incontournable de la collaboration et du bien-être en entreprise, deux enjeux majeurs de notre époque. Par Guillaume BOISSON, Consultant Senior du cabinet VERTUO Conseil
114	http://www.vertuoconseil.com/portfolio/soiree-annuelle-au-jamel-comedy-club/	
115	http://www.vertuoconseil.com/portfolio/le-paiement-etat-de-lart-dun-marche-innovant/	 Le paiement : état de l’art d’un marché innovant VERTUO 2017-02-12T13:41:20+00:00 Project Description Les Echos – 13 février 2015 : Aujourd’hui, dans le secteur bancaire, le domaine des moyens de paiement est l’un des plus prometteurs, grâce notamment aux avancées technologiques et à la démocratisation du Smartphone. En effet, si aujourd’hui nous payons globalement comme hier, il ne fait aucun doute que nous payerons différemment demain. Pour l’heure, nous sommes face à un certain nombre d’évolutions, prémices d’une révolution. Une révolution du paiement mobile qui se fait attendre Le paiement mobile sans contact NFC (Near Field Communication) suscite de nombreux espoirs dans le secteur bancaire français, particulièrement en termes de nouveaux usages et de services à valeur ajoutée : fidélité, couponing, offres dédiées géolocalisées, etc. Pourtant, celui-ci tarde à s’imposer, car il se heurte encore à un certain nombre de freins. En premier lieu, des problématiques d’ergonomie du parcours client lors de l’enrôlement, ainsi que des problématiques d’interopérabilité ont vu le jour, notamment concernant le TSM (Trusted Services Manager), « fournisseur de services de confiance » pour le paiement mobile. Ensuite, un écosystème peu lisible et un positionnement flou de certains acteurs sur la chaîne de valeur qui ont mis en lumière un business model complexe et encore peu rémunérateur. Côté technique, on observe un environnement qui se cherche, avec une multitude de technologies qui coexistent : microSD, SIMcentric, Secure Element, QR Code, BLE (Bluetooth Low Energy), HCE (Host Card Emulation), etc. À ce jour, aucune d’entre elles n’a émergé en tant que standard de marché. Enfin, on constate qu’un certain nombre de pilotes et d’expérimentations ne revêt pas un caractère suffisamment universel pour permettre le succès et l’adoption à grande échelle d’un nouveau mode de paiement. L’espoir d’un décollage du NFC sous l’impulsion d’Apple Apple, numéro 2 mondial derrière Samsung sur le marché des Smartphones, mais dont la notoriété et l’influence sur ce marché sont extrêmement fortes avait, jusqu’à l’iPhone 5, toujours refusé d’inclure la technologie NFC dans ses mobiles. L’arrivée de l’iPhone 6, doté de cette technologie, laisse penser que le virage NFC est en passe d’être pris. Néanmoins, il convient de rester prudent et de noter que des points d’interrogation subsistent à propos d’Apple Pay et de son Touch ID. Lancé d’abord aux États-Unis, il ne fait aucun doute qu’Apple Pay arrivera dès que possible sur le marché français. Mais cela pourrait être plus compliqué au regard des différences notables entre les marchés du paiement français et américain, en particulier concernant l’aspect sécuritaire, la France ayant des standards plus exigeants (norme EMV). L’iPhone 6 se heurtera également à une problématique de taux d’équipement due au coût de l’appareil. Enfin se pose la question des alliances et partenariats. Outre-Atlantique, Apple a déjà conclu des partenariats avec des schemes (Visa, MasterCard et American Express), des banques (Bank of America, Citigroup, JPMorgan Chase, Wells Fargo, etc.) et des enseignes de premier plan (Subway, McDonald’s, Disney, Nike, etc.). En France, la tâche risque d’être plus complexe et on peut penser que les négociations avec les banques ne seront pas aisées, particulièrement sur les commissions, dans un contexte de forte pression règlementaire Européenne entraînant une érosion des interchanges. Quoi qu’il en soit, le nouveau choix stratégique d’Apple concernant le NFC va dynamiser le marché du paiement mobile en forçant les autres acteurs à proposer des solutions concurrentes rapidement. En outre, ces acteurs vont devoir se montrer créatifs face à Apple qui a les moyens de devenir un acteur majeur sur la chaîne de valeur du paiement. Grâce à ses 800 millions de comptes iTunes et données associées, Apple a une longueur d’avance dans la course aux données clients, clé des services à valeur ajoutée. Une croissance lente des paiements sans contact Pour l’heure, c’est le paiement sans contact par carte qui doit « ouvrir la voie » au mobile et entrer dans les habitudes des consommateurs : la récente campagne de publicité du GIE Cartes bancaires « Poser, c’est payé » va dans ce sens. Ce marché du sans contact est en croissance, mais reste aujourd’hui encore un marché de niche, comme en attestent les derniers chiffres du GIE CB (à fin décembre 2014) : 45,9 % des cartes CB sont sans contact, 19,7 % des terminaux le sont, mais ils ne totalisent que 1,52 % des paiements (en volume)… Notons que la barre symbolique des 1 % de paiements sans contact avait été franchie pour la première fois en octobre dernier. Les paiements par carte sans contact s’établissent en moyenne aux alentours de 11 € et sont, rappelons-le, limités unitairement à 20 €. Activité soutenue sur les marchés du wallet et du mPOS Dans l’attente de la révolution du paiement mobile, les banques françaises se positionnent sur deux marchés. Premièrement, le wallet (ou portefeuille numérique), dont tout le monde connaît le plus célèbre : PayPal. Destiné à ce jour au paiement en ligne, le wallet va pouvoir bénéficier du dynamisme du marché du e-commerce français, 6e marché mondial, avec un Chiffre d’Affaires de 56,8 milliards d’euros en 2014, en hausse de 11 % par rapport à l’année précédente (source Fevad). Les banques françaises ont donc lancé le « wallet français », Paylib, via le consortium BNP Paribas, La Banque Postale, Société Générale, qui sera rejoint prochainement par le Crédit Mutuel Arkéa et le Crédit Agricole. Même si ce produit est voué à évoluer en termes de services à valeur ajoutée, il a aujourd’hui le mérite de reconstruire l’interbancarité française et d’éviter la « guerre des boutons » de paiement sur les sites de e-commerce… Deuxième marché très actif, le marché du mPOS (mobile Point Of Sale), où un Smartphone est utilisé comme support d’acceptation du paiement par carte. Cette solution qui permet de transformer le Smartphone ou la tablette du commerçant en un terminal de paiement s’adresse principalement aux petits marchands et marchands ambulants (artisans, médecins, taxis, livreurs, etc.) non équipés de TPE, principalement pour des raisons de coût et de durée d’engagement. Les solutions de mPOS peuvent également adresser des commerçants de plus grande taille, déjà équipés de matériel classique, afin de proposer des points d’acceptation supplémentaires tout en véhiculant une image innovante. Ce marché est sans doute le plus innovant aujourd’hui et, contre toute attente, c’est du côté des commerçants que le mobile est en train de faire évoluer les usages en matière de paiement. En 2014, Dilizi de BPCE, Mobo de BNP Paribas ou encore la solution de w-HA, filiale d’Orange, ont vu le jour sur le marché français. Les solutions Smart TPE du Crédit Agricole, Monem Mobile de LCL et Monetico Mobile du Crédit Mutuel-CIC arrivent sur le marché à leur tour. Vers une mutation de l’acte de paiement Il devient aujourd’hui difficile de recenser l’ensemble des initiatives innovantes liées au paiement tant elles sont nombreuses : utilisation des réseaux sociaux (paiement par Twitter de BPCE), authentification vocale (Talk to Pay de La Banque Postale), biométrie (Natural Security Alliance), etc. Il apparaît pourtant de plus en plus clairement que l’on peut résumer l’enjeu de l’innovation sur le marché des paiements à travers l’ambition exprimée par PayPal : « Faire disparaître complètement l’acte de paiement »… Karim Terbeche, Project Manager, VERTUO Conseil
116	http://www.vertuoconseil.com/portfolio/ce-que-va-changer-le-paiement-instantane/	
117	http://www.vertuoconseil.com/portfolio/industrie-auto-innovation-cacher/	 Industrie auto : une innovation peut en cacher une autre Mathilde. Taillez 2017-06-12T05:26:08+00:00 Project Description L’innovation n’est pas toujours celle que l’on croit. Alors qu’Elon Musk, la superstar mégalo qui prétend révolutionner le secteur automobile, peine à masquer sous une communication extravagante les déconvenues (au premier rang desquelles figurent la sécurité et la qualité) que rencontre tout constructeur cherchant à grossir vite, des innovations majeures et à moindre coût en provenance de la vieille Europe pourraient faire taire les cassandres du bon vieux moteur thermique. Innovant, Tesla l’est sûrement, et dans plusieurs domaines, depuis la conception des châssis et des habitacles jusqu’à ses techniques de commercialisation ou encore sa contribution sur le développement du réseau de bornes de recharges. Mais ses concurrents ne se tournent pas les pouces. En particulier l’industrie du vieux continent qui ne souhaite pas rater le prochain virage énergétique. Parmi eux, des constructeurs, des équipementiers et des chercheurs. Volkswagen, en quête de vertu depuis un an, finance un projet d’électrification de ses gammes à hauteur de 3,5Md€ et emboîte ainsi le pas à l’Alliance Renault-Nissan qui se veut être le leader du marché du véhicule électrique à l’aube de la prochaine décennie, notamment en le rendant plus accessible avec un projet de véhicule low-cost destinés aux marchés asiatiques. Sous ce déluge d’annonces vertes, les esprits cyniques ne manqueront pas, à raison, de s’interroger sur la viabilité économique et environnementale de l’électricité. A l’heure où l’extraction de lithium interpelle sur les coûts écologiques et amène à relativiser le génie de la gigafactory Tesla, censé devenir en 2017 le plus grand centre de production de batteries au monde et ainsi un levier de réduction des coûts, il semblerait que le moteur à combustion ait encore de beaux jours devant lui. Car ses progrès en termes de rendement ne sont pas encore terminés, comme en témoigne la dernière innovation de la société FreeValve AB issue d’un partenariat international porté par Koenigsegg. L’idée : supprimer les arbres à came séculaires par un système pneumatique bien plus flexible et efficace. Avec à la clef des gains estimés jusqu’à 10% en poids et 40% en rendement, donc en performance, en sobriété et en maîtrise des émissions polluantes. D’ailleurs Christian von Koenigsegg n’en était à pas son coup d’essai côté progrès technique : il avait démontré sur ses voitures ultra-élitistes qu’utiliser le carburant E85 permettait d’optimiser le fonctionnement de ses moteurs pour encore plus de rendement. Problème : l’E85 actuel est souvent « mal » produit. Alors que ce carburant aurait pu permettre de relancer certaines filières agricoles et entrer dans un cercle vertueux de diversification, il est en général extrapolé à partir de produits issus de l’agriculture alimentaire (maïs), entraînant avec lui une spéculation sur les cours et un renchérissement de ressources vitales pour les pays émergents. D’où l’idée de plancher sur la nouvelle génération de biocarburants. Et cette fois c’est de France que pourrait venir la solution, plus que jamais fidèle à sa culture du « on n’a pas de pétrole mais on a des idées ». Plusieurs équipes de chercheurs sont parvenues à exploiter les propriétés de certaines algues pour en extrapoler des carburants au rendement énergétique bien supérieur à celui du traditionnel sans-plomb. Non fossile, disponible en quantité quasi-infinie, aisé à synthétiser, plus calorifique que les standards actuels, directement utilisable dans les moteurs actuels, cette solution semblerait ne proposer que des avantages sur le papier. Au point que plusieurs PME se sont lancées dans sa culture : elles seraient aujourd’hui près d’une centaine en France à tenter leur chance dans cette filière. Il ne reste plus qu’à espérer qu’un grand groupe de la filière pétrolière intègre cette solution pour lever la barrière du réseau de distribution, qui demeure aujourd’hui le principal frein à l’expansion des nouveaux modes de propulsion (électrique, hydrogène, etc.). D’autant plus que le timing politique semble parfait : Bruxelles vient d’annoncer une réflexion sur la composition de l’E10 qui viserait à limiter la part du bioéthanol issus de cultures céréalières ou de betteraves. Car tant que l’électricité ne sera pas massivement issue d’une source renouvelable et bridée en termes d’infrastructures, elle restera une forme de mirage écologique qui ne doit pas faire oublier les solutions de bon sens qui s’offrent sous nos yeux. Encore faut-il que les bonnes personnes en saisissent l’enjeu et se portent sponsor de ce projet. Par Adrien AUBERT, Senior Manager du Cabinet Vertuo Conseil
118	http://www.vertuoconseil.com/portfolio/rd-le-lab-vertuo-organise-le-22-mars-prochain-son-premier-seminaire-de-restitution/	
119	http://www.vertuoconseil.com/portfolio/nouvelles-menaces-nouvelles-technologies-la-fin-annoncee-du-pca-traditionnel/	 Nouvelles menaces, nouvelles technologies : la fin annoncée du PCA traditionnel ? VERTUO 2017-02-12T13:04:57+00:00 Project Description Les Echos – 13 juillet 2016 : Motivées par des réglementations internationales, recommandations nationales ou initiatives propres à la suite d’une crise, de nombreuses organisations sont aujourd’hui dotées d’un Plan de Continuité d’Activité (PCA). C’est notamment le cas des opérateurs d’importance vitale, grands groupes et banques, qui dédient à cette tâche des ressources humaines et financières spécifiques. Ces entités sont-elles pour autant parées à affronter les crises – raisonnablement probables – qui les menacent ? Peuvent-elles affirmer être organisées de façon à assurer leur résilience ? Qu’elles le sachent ou se le cachent, la réponse est presque toujours négative. Un corpus documentaire qui a perdu son objectif Car en effet le PCA tel qu’on le retrouve aujourd’hui dans la plupart des organisations souffre de multiples défauts accentués avec le temps. Le plus grave d’entre tous ? Sa dénomination, qui neutralise le concept d’urgence de la crise. La notion de « Plan » rassure : tout est prêt, les choses sont écrites, anticipées. Il n’y a guère plus qu’à le déployer pour « continuer l’activité » : la question n’est pas « si » elle reprendra, mais « comment » ? Il s’agit d’une vision grossièrement trompeuse de ce qui attendra les décideurs le jour J : un évènement inattendu, aux multiples imprévus, nécessitant des décisions rapides face à une activité qui s’est arrêtée dans des conditions brusques, et dont le redémarrage est fortement compromis. Le plan de continuité traditionnel, son scénario d’indisponibilité du site principal suite à un incendie ou d’indisponibilité du personnel suite à un épisode neigeux, avec comme solution un repli des personnels-clés sur un site secondaire, fait presque office de madeleine de Proust du responsable de PCA. Ainsi, avec la multiplication des risques et des changements organisationnels, a-t-on (parfois) décidé d’aller plus loin, d’écrire de plus nombreux scénarios, de lister plus en détail les processus, les personnels, les délais de reprise attendus, les jours critiques, les fournisseurs, les interfaces, les dépendances et les compétences croisées. Au final, la rigidité d’une documentation prolifique l’a emporté sur la seule et unique qualité requise en temps de crise : la flexibilité. Car la seule chose certaine lorsque le jour J se présentera, c’est que rien ne se passera comme prévu : La cellule de crise mettra plus de temps à se réunir que le délai théorique de redémarrage des activités les plus critiques ; La cellule de crise ne sera jamais au complet, et les rôles ne seront pas définis ; Une partie des collaborateurs ne recevra pas l’alerte pour des raisons diverses ; Sur les collaborateurs essentiels (voire les managers), 20 à 40 % seront en congés ou en déplacement (dem pour les remplaçants identifiés) ; Beaucoup n’auront pas la capacité de rejoindre le site de repli (en raison de la situation elle-même ou d’un simple évènement de la vie : enfant à garder, voiture chez le garagiste, etc.) ; C’est évidement à ce moment qu’une infrastructure, un outil, un maillon logistique va dysfonctionner. De préférence la plate-forme de conférence téléphonique ou de visioconférence, qui semble toujours guetter ces moments de crise pour se découvrir des plantages improbables ou le besoin soudain et impérieux de procéder à une mise à jour. Tout ce que l’on peut imaginer, et surtout ce que l’on n’aura pas imaginé, se produira. Paradoxalement, le plan extrêmement détaillé et précis deviendra alors un obstacle à la gestion de la crise, égarant les décideurs et les détournant des urgences propres à la situation à laquelle ils sont confrontés. En situation de crise, le détail est une niche confortable dans laquelle l’esprit est tenté de se réfugier. La résilience ne peut pas passer par un ajout de complexité Le PCA doit donc se recentrer sur la capacité de la cellule de crise à prendre des décisions rapides sur un nombre limité de schémas, s’appuyant sur des solutions « légères », tout en déléguant la responsabilité de l’organisation opérationnelle au management de proximité. Il s’agit, toute proportion gardée, du modèle du plan ORSEC régissant en France les actions des secours en cas de catastrophe et qui a fait ses preuves. Cet objectif ne pourra jamais être atteint en ajoutant des procédures supplémentaires et spécifiques à la gestion de crise dans des organisations qui n’y sont que peu confrontées : le moment de la crise n’est pas celui où l’on doit s’essayer à quelque chose que l’on ne maîtrise pas. C’est au contraire sur les réflexes conditionnés qu’il faut s’appuyer. Et avec en moyenne un exercice par an, les managers et équipes seront plus proches de l’oubli total que de la maîtrise instinctive de ces processus. Le développement au cours des quinze dernières années d’équipes dédiées au PCA est une avancée majeure et louable, mais qui peine à arriver à maturité. On peut raisonnablement considérer que cette perpétuelle adolescence est une conséquence logique de la contre-productivité inhérente à ce mode d’organisation. Car une équipe dont le travail est de faire du PCA fera du PCA. De façon de plus en plus indépendante. En s’ajoutant aux processus métiers existants. En entrant en concurrence avec d’autres équipes. Pour justifier et mettre en valeur sa position dans l’entreprise, chaque service doit être prolifique et démontrer ainsi son importance. Or la résilience est l’ennemi du prolifique : elle a besoin de simplicité et de processus intégrés. Responsable PCA : un rôle de conseil plutôt que d’exécution La phase initiale d’élaboration et mise à jour du PCA, l’analyse d’impact business, est le premier maillon défectueux de la chaîne. Le niveau de détails et la complexité de lecture aboutissent à des analyses biaisées selon le manager interrogé et son interlocuteur. Seules les questions les plus sensibles et les plus objectives doivent demeurer : Quelles activités reprendre ? Dans quel ordre ? Avez-vous sur vous les coordonnées de vos collaborateurs pour organiser la reprise de vos activités ? L’objectif est double et ramène au rôle essentiel du responsable de la continuité d’activité. En premier lieu, implanter les messages-clés dans l’esprit des managers de proximité par le biais de questions claires et peu nombreuses. Dans un second temps, compiler dans le PCA uniquement le volume d’information assimilable par la cellule de crise et suffisant pour éclairer la prise de décision. Par ailleurs, la stratégie de reprise de l’activité doit se libérer au maximum de l’attachement à des sites de repli. Tout responsable PCA ayant organisé des exercices de repli a nécessairement pensé, au moins une fois dans la journée : « ça ne marchera jamais en situation réelle ». À raison. Cette solution doit être restreinte aux seules activités pour lesquelles un matériel spécifique non déportable est obligatoire en toute situation. Il s’agit par ailleurs d’un gouffre financier qui porte préjudice à l’image renvoyée par le dispositif de préparation à la crise. Le travail à distance et l’usage d’outils collaboratifs numériques doivent être la brique fondamentale du PCA moderne. Il s’agit non seulement de la solution permettant au plus grand nombre de reprendre leur activité avec le plus de facilité, mais qui a par ailleurs l’avantage de capitaliser sur des transformations numériques déjà engagées dans de nombreuses structures. Là encore, le rôle du responsable PCA est d’éclairer la prise de décision sur les orientations stratégiques en termes d’infrastructure informatique dans cette perspective de résilience. Il sera plus efficace, efficient et économique de concevoir conjointement avec la DSI une cible de dispositif permettant le homeworking, plutôt qu’intervenir a posteriori pour démontrer l’absence de résilience du SI et investir sur des dispositifs spécifiques complémentaires. Toute solution de résilience doit s’appuyer sur un dispositif usité et maîtrisé par les collaborateurs et le homeworking n’y fait pas exception. Là encore, le rôle du PCA est d’amener les fonctions managériales à en comprendre l’intérêt et les bénéfices. Non seulement pour la sécurisation à moindre coût de la continuité, mais également pour l’activité quotidienne. Par exemple, en cas d’enfant malade ou d’indisponibilité des transports, où le travail à distance permet de ne pas avoir à se passer totalement d’une ressource pour toute la journée. Et aux réticents qui craignent la chute de productivité au domicile, le responsable PCA éclairé posera la question suivante : est-il plus facile de ne pas travailler assis à son bureau, ou à distance quand la seule preuve de son activité est de produire emails et documents de façon régulière ? En somme, le positionnement de l’équipe en charge de la continuité ne peut pas être celui d’une entité proposant ses solutions propres. C’est au contraire celui d’un agitateur qui, posant les questions importantes à chaque occasion, permet aux processus en place d’évoluer progressivement vers une résilience native et intrinsèque. Et maintenant ? Il ne s’agit pas de défaire toutes les organisations en charge du PCA qui ont le mérite d’apporter aujourd’hui un premier niveau de réponse. Mais il est temps d’accepter l’échec imminent de cette méthode, et construire une transition douce vers un modèle plus actuel et plus réaliste. Remettre les équipes de continuité d’activité dans une situation de transversalité intégrée, plutôt qu’un métier à part avec ses propres objectifs et ambitions. Favoriser l’évolution des outils et processus de sorte qu’ils puissent fonctionner de façon dégradée, plutôt que tenter de les remplacer par des processus spécifiques à la crise. Donner les clés simples et pratiques pour permettre aux décideurs de se consacrer aux priorités en cellule de crise, plutôt que de les noyer sous des documentations prolifiques. Accentuer le sentiment de responsabilité des managers quant à la résilience de l’activité dont ils sont responsables, plutôt que leur donner l’illusion que quelqu’un s’en charge. Tous ces changements, et ceux qu’ils induisent sont évidemment complexes et comportent plusieurs dimensions techniques et organisationnelles. Il ne s’agit pas de le sous-évaluer. Toutefois, il est temps de replacer le PCA dans son véritable rôle : assurer une capacité opérationnelle à gérer la crise. Non pas « si » elle survient, mais « quand » elle surviendra ? Par Selim Miled consultant Senior du cabinet VERTUO Conseil
120	http://www.vertuoconseil.com/portfolio/mais-pourquoi-tous-ces-projets-qui-derapent/	 Mais pourquoi tous ces projets qui dérapent ? VERTUO 2017-01-12T11:33:51+00:00 Project Description LA RÉUSSITE D’UN PROJET N’EST PAS LA RÈGLE Force est de constater que le dérapage des projets, en termes de coûts, de délais ou d’atteinte des objectifs, fait partie du quotidien des entreprises. Il est même devenu la norme, accepté tant par le top management que par les chefs de projets. Au-delà d’une impression souvent partagée par les acteurs des projets, ce constat est confirmé par différentes études menées par des organismes indépendants. Le plus connu d’entre eux, le Standish Group, a réalisé depuis 1994 des études annuelles sur le déroulement des projets ayant une composante SI (Systèmes d’informations) et concernant donc la majorité des projets du secteur de la banque-assurance. Ces études permettent de quantifier et de qualifier le dérapage des projets sur une période d’une vingtaine d’années : une forte amélioration peut être constatée (de 16% de succès en 1994 à 29% en 2015), mais la réussite des projets n’est pas encore la règle (en 2015, 29 % des projets réussissent, 52% dérapent, 19% sont en échec). Source: Chaos Report 2015 (Standish Group) A noter que le Standish Group a fait évoluer la notion de projet « réussi » en 2015. Il a en effet considéré que le critère de respect du périmètre («on target» en anglais) n’était pas un critère satisfaisant car il ne rendait pas compte de la satisfaction des clients de leur SI. Ce critère a donc été remplacé par une analyse de la valeur cliente perçue, qui vient compléter les deux autres critères initiaux que sont le respect des délais et le respect du budget. Parallèlement, ces vingt dernières années sont également apparus divers modèles standardisés proposant des socles méthodologiques de gestion de projets, associés à des dispositifs de certification payants. Nous pouvons notamment citer le PMP (Project Management Professional) du PMI (Project Management Institute), Prince2 (PRojects IN Controlled Environments) de l’OGC (Office of Government Commerce) ou d’autres standards plus adaptées à l’univers du logiciel comme le CMMI (Capability Model Maturity Integration) ou Scrum dans la mouvance des méthodes agiles. Les établissements du secteur de la banque-assurance se sont naturellement inscrits dans cette tendance en s’outillant avec des socles méthodologiques de gestion de projets personnalisés, inspirés des standards du marché. Malgré ces avancées, la réussite d’un projet reste aujourd’hui la situation la moins probable. DES STANDARDS NÉCESSAIRES MAIS PAS SUFFISANTS Tous ces socles méthodologiques s’articulent autour d’une méthode et d’un ensemble d’outils associés. Ils apportent une réponse opérationnelle au niveau des compétences techniques (« hard skills ») requises pour gérer un projet, en fournissant une description structurée d’un ensemble de bonnes pratiques (les étapes d’un projet) et un outillage (des modèles de livrable projet) pour les mettre en œuvre. Le consensus est général en ce qui concerne ces bonnes pratiques : elles décrivent des principes évidents (un projet doit être cadré, planifié, piloté, etc.) et simples à formuler, qui ne peuvent pas être mis en doute. Si bien la description de ces bonnes pratiques est aisée, leur mise en œuvre dans la vie réelle ne l’est pas pour autant. L’environnement dans lequel évoluent les projets étant chargé de contraintes, organisationnelles et humaines notamment, une application stricte de ces bonnes pratiques est en général difficile car les conditions pour le faire ne sont qu’exceptionnellement remplies. La mise en œuvre des bonnes pratiques doit ainsi se faire en prenant en compte les particularités de l’environnement de chaque projet, car les socles méthodologiques n’en tiennent pas compte. Par exemple, les socles méthodologiques des établissements du secteur de la banque-assurance sont souvent conçus par et pour les équipes IT, et construits en général en ayant comme cible la gestion de grands projets de développements spécifiques de solutions logicielles. Une application stricte des préceptes de tels socles peut être contreproductive lorsqu’il s’agit de mener à bien un petit projet, ou des projets ayant une dominante métier plutôt que SI (lancement d’un nouveau produit/offre de services, par exemple). La situation est analogue si l’on s’appuie sur un socle méthodologique standard : soit il est générique et il nécessite des adaptations pour pouvoir s’en servir dans des situations spécifiques (cas des socles PMP et Prince2), soit il est spécifique et n’est donc utilisable que dans certaines situations (cas des socles CMMI et Scrum). Toute la subtilité du rôle de chef de projet réside donc dans cette capacité à faire un usage raisonné des compétences techniques (« hard skills») décrites dans ces socles en naviguant entre un suivi formel et un détachement mesuré des préceptes du socle méthodologique sur lequel il s’appuie. Au-delà de sa technicité méthodologique, le chef de projet doit aussi développer une palette de compétences non techniques («soft skills ») qui faciliteront l’adaptation à un environnement donné, y compris un environnement hostile à une gestion sereine de projets. Un état d’esprit orienté solution, la capacité à donner du sens à un projet, à négocier, à mobiliser toutes les parties prenantes, à entrer dans un niveau de détail élevé puis à s’en soustraire pour repartir vers une vision macroscopique, sont autant d’atouts nécessaires à la réussite du projet. Ces aptitudes permettent à un chef de projet de rendre un projet gérable, en modifiant le projet ou l’environnement dans lequel il se déroule, ce qui est un prérequis dans presque toutes les situations de projet. En effet, le point de départ d’un projet résulte souvent d’une situation de déséquilibre car les ambitions affichées par le sponsor du projet ne sont pas en cohérence avec les délais annoncés pour le réaliser et les moyens mis en œuvre. La capacité à négocier avec le sponsor et à mobiliser toutes les parties prenantes sont respectivement nécessaires pour établir cet équilibre et pour être en mesure de garantir sa stabilité pendant toute la durée du projet. Les moyens pour y parvenir peuvent consister à réviser les objectifs du projet, ou à restructurer un projet initialement grand, en un ensemble de petits projets plus maîtrisables. Les compétences non techniques (« soft skills ») sont difficiles à modéliser sous la forme d’étapes ou de processus, et ne font donc malheureusement pas partie des méthodes et outils qui sont mis à disposition des chefs de projet. Pourtant, la réussite d’un projet est fortement dépendante des compétences techniques (« hard skills ») et non techniques (« soft skills ») que doit avoir un chef de projet. Les socles méthodologiques standard et ceux qui sont déployés au sein des entreprises ne décrivant que partiellement l’ensemble de ces compétences, à eux seuls ils ne peuvent garantir la réussite d’un projet. Ceci explique que les effets sur le taux de réussite des projets de l’arrivée de socles méthodologiques standardisés ou cours des 15 dernières années, soient finalement faibles. QUE FAUT-IL FAIRE POUR AUGMENTER LA PROBABILITÉ DE RÉUSSITE DES PROJETS ? Le Rapport 2011 réalisé par l’Observatoire des Projets stratégiques a mis en évidence que même si des socles méthodologiques existent, leur utilisation réelle n’est pas systématique : ils n’existent que dans 50% des entreprises et lorsqu’ils existent, l’adaptation de leur utilisation en fonction du contexte est en général laissée à l’appréciation du chef de projet. Les éventuelles modalités d’adaptation aux besoins des projets ne sont décrites que dans 13% des cas ! La réussite d’un projet semble ainsi reposer davantage sur le choix de la bonne personne, le profil idéal étant un chef de projet qui cumule en même temps des compétences techniques (« hard skills ») et non techniques (« soft skills »). Ce profil est plutôt rare, et plusieurs raisons expliquent cela : La première raison est historique et culturelle. Pour beaucoup d’entreprises la compétence principale à prendre en considération pour choisir le bon chef de projet est l’expertise métier. Les compétences en gestion de projets (techniques et non techniques) sont considérées comme une option éventuellement intéressante, mais pas forcément comme une qualité nécessaire. Les acteurs concernés ne sont donc pas formés à la gestion de projets, aucun processus d’acquisition des « hard skills » et des «soft skills » n’est engagé. La deuxième, est la couverture même des socles méthodologiques existants. Même dans le cas où la compétence méthodologique est jugée nécessaire, la formation des chefs de projet s’articule autour des socles existants, qui comme nous l’avons déjà expliqué, ne couvrent qu’une partie des compétences techniques (« hard skills ») requises pour gérer un projet. Ces formations exposent une méthode et présentent éventuellement des outils associés, mais ne fournissent que peu de clés pour savoir comment adapter leur mise en œuvre à un environnement spécifique. Les « soft skills » peuvent être abordées dans le cadre de formations comportementales complémentaires, mais ces dernières sont souvent génériques et ne ciblent pas en particulier les situations de gestion de projets en entreprise. La troisième, est la nature même des actions qui sont engagées pour assurer la montée en compétences des chefs de projet. Elles se réduisent en général à la participation à des actions de formation, ce qui est insuffisant. De même qu’un ingénieur ne peut pas être considéré comme 100% opérationnel sur la base de ses seuls acquis théoriques, la formation n’est pas suffisante pour permettre une montée en compétences effective d’un chef de projet. QUELLES ACTIONS PEUT ENGAGER UNE ENTREPRISE POUR LIMITER CETTE DÉPENDANCE A RESSOURCE FINALEMENT RARE, ET AINSI ACCROÎTRE LA PROBABILITÉ DE RÉUSSITE DE SES PROJETS ? La première approche consiste à envisager une montée en compétences adéquate des chefs de projet. Il s’agit alors de mettre en place un plan visant à garantir l’acquisition de compétences méthodologiques (pour bien connaître un socle méthodologique de référence), l’appropriation (pour savoir faire un usage adapté de ce socle dans différentes situations) et la pérennisation de ces acquis (pour inscrire ces acquis dans la durée). L’acquisition de compétences peut se faire au travers d’une formation adaptée, qui aborde aussi bien les « hard skills » que les « soft skills » nécessaires pour mener à bien un projet et qui donne des clés pour savoir s’adapter à des situations différentes de projet. L’appropriation doit se faire en s’appuyant sur un dispositif de mentoring, complémentaire de la formation, où un chef de projet très expérimenté- le mentor – accompagne le chef de projet formé dans la mise en œuvre des acquis théoriques. Le mentor suit de près le pilotage du projet et donne des conseils concrets au chef de projet pour adapter l’usage du socle méthodologique à chaque situation. La pérennisation se fera en mettant en place un dispositif de capitalisation sur la connaissance qui centralise les leçons apprises des expériences de projets passées. Les études montrent que le taux de réussite des projets est plus élevé dans les organisations où des moyens de cette nature sont mis en place. Cette approche centrée sur l’investissement dans une montée en compétences complète et pérenne favorise l’émergence de chefs de projet « tout-terrain ». Elle est particulièrement adaptée dans les organisations où il existe un département qui centralise les compétences de pilotage de projets (Bureau des projets), et qui fournit des chefs de projet aux départements qui en ont besoin. Une autre approche, complémentaire de la précédente, consiste à améliorer les socles méthodologiques existants pour renforcer leur adéquation avec la culture, l’organisation et les besoins de l’entreprise, afin de réduire la dépendance d’un profil rare de chef de projet. Cette amélioration peut prendre plusieurs formes allant de la simplification (abandonner un socle méthodologique de type «artillerie lourde » si la majorité des projets réalisés sont petits), à l’adaptation (construire des socles méthodologiques sur mesure qui prennent en compte l’organisation, la culture et les besoins de l’entreprise, ou différentes typologies de projets). Ainsi, un socle méthodologique conçu par et pour les équipes IT peut être étendu pour impliquer davantage les acteurs des métiers dans la vie du projet. Cet ajustement pourra se traduire par l’ajout d’étapes, d’outils et de conseils pratiques, correspondant à la phase dite d’avant-projet (cadrage du projet, spécification générale du besoin). En réintégrant ces éléments dans le champ de responsabilités du chef de projet, on l’incite à impliquer les acteurs du métier dès la formulation de la première idée, à adopter un point de vue qui privilégie le résultat attendu par le métier plutôt que la solution technique à mettre en œuvre (une manière implicite de confronter le chef de projet aux compétences non techniques comme la négociation avec les parties prenantes). Dans tous les cas de figure, la réussite d’un projet reposant davantage sur les compétences du chef de projet que sur le choix d’un socle méthodologique en particulier, l’investissement à minima dans un plan de montée en compétences est nécessaire pour accroître la probabilité de réussite des projets au sein d’une entreprise. CONCLUSION La marge de progression pour basculer dans des projets réussis reste encore importante. Si bien des socles méthodologiques standards (PMI, Prince2, CMMI, Scrum) sont à l’origine d’une amélioration, ils ne sont clairement pas suffisants, car les conditions requises pour en profiter pleinement ne sont que rarement remplies. Tant que les efforts nécessaires ne seront pas engagés, le retour sur investissement des projets réalisés dans les entreprises du secteur de la banque-assurance sera faible. Ces efforts doivent se concentrer essentiellement, sur une mise en adéquation des compétences techniques (« hard skills ») afin de développer les capacités d’adaptation des chefs de projet à différentes situations. Ils doivent aussi se concentrer sur les compétences non techniques (« soft skills ») de ces derniers, qui sont indispensables pour sécuriser le déroulement d’un projet sur toute sa durée. Grâce à ce couple de compétences, les chefs de projet seront en mesure de mettre en place les conditions nécessaires la réussite du projet, pour amplifier les taux de réussite au sein des entreprises. Par Mathilde Degremont, Project manager et Claudio Maldonado, Senior Manager du cabinet VERTUO Conseil
